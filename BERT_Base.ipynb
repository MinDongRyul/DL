{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "a7814163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchsummary as ts\n",
    "import torchinfo as ti\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfd8043",
   "metadata": {},
   "source": [
    "# BERT 구현\n",
    "### 학습 목표\n",
    "1. BERT의 Embeddings모듈 동작을 이해하고 구현할 수 있다.\n",
    "2. BERT의 Self-Attention을 활용한 Transformer 부분인 BertLayer모듈의 동작을 이해하고 구현할 수 있다.\n",
    "3. BERT의 Pooler모듈의 동작을 이해하고 구현할 수 있다.\n",
    "### 참고\n",
    "- https://eatchu.tistory.com/entry/BERT-Architecture-Transformer-Encoder#google_vignette"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaff8569",
   "metadata": {},
   "source": [
    "## 8.2.2 BERT_Base의 네트워크 설정 파일 읽기\n",
    "- 먼저 BERT_Base에서 Transformer가 12단인 것과 특징량 벡터가 768차원인 것 등을 적은 weights폴더의 네트워크 설정 파일 bert_config.json을 읽어들입니다.\n",
    "- 읽어들인 JSON파일의 사전형 변수에서 key 'hidden'값을 취하려면 config['hidden_size']로 적어야합니다. 이를 config.hidden_size로 기술하면 깔끔합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c0ec71b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'architectures': ['BertForMaskedLM'],\n",
       " 'attention_probs_dropout_prob': 0.1,\n",
       " 'gradient_checkpointing': False,\n",
       " 'hidden_act': 'gelu',\n",
       " 'hidden_dropout_prob': 0.1,\n",
       " 'hidden_size': 768,\n",
       " 'initializer_range': 0.02,\n",
       " 'intermediate_size': 3072,\n",
       " 'layer_norm_eps': 1e-12,\n",
       " 'max_position_embeddings': 512,\n",
       " 'model_type': 'bert',\n",
       " 'num_attention_heads': 12,\n",
       " 'num_hidden_layers': 12,\n",
       " 'pad_token_id': 0,\n",
       " 'position_embedding_type': 'absolute',\n",
       " 'transformers_version': '4.6.0.dev0',\n",
       " 'type_vocab_size': 2,\n",
       " 'use_cache': True,\n",
       " 'vocab_size': 30522}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# config.json에서 설정을 읽어들여 JSON 사전 변수를 오브젝트 변수로 변환\n",
    "import json\n",
    "\n",
    "config_file = './weights/bert_config.json'\n",
    "\n",
    "# 파일을 열어 JSON으로 읽는다.\n",
    "with open(config_file, 'r') as f:\n",
    "    config = json.load(f)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fbb4a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 사전 변수를 오브젝트 변수로\n",
    "from attrdict import AttrDict\n",
    "\n",
    "config = AttrDict(config)\n",
    "config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "027dda2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8063c1c",
   "metadata": {},
   "source": [
    "## 8.2.3 BERT에 레이어 정규화 층 정의\n",
    "- BERT 모델 구축의 사전 준비로 레이어 정규화 층의 클래스를 정의합니다. 7장에서 사용한 것 처럼 파이토치에도 레이어 정규화가 있습니다.\n",
    "- 텐서플로와 파이토치에서는 레이어 정규화의 구현 방법이 약간 다릅니다. 텐서의 마지막 채널(즉 단어의 특징량 벡터 768차원)에 평균 0, 표준편차 1이 되도록 레이어 정규화를 수행합니다. 0으로 나누지 않도록 보조 항 엡실론을 넣는 방법은 파이토치와 텐서플로가 서로 다릅니다.\n",
    "- 이번에 사용할 학습된 모델은 구글이 공개한 텐서플로의 학습 결과에 기반하여 텐서플로 버전의 레이어 정규화 층을 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b22ec377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT용으로 레이어 정규화 층 정의\n",
    "# 세부 구현을 텐서플로에 맞춘다.\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class BertLayerNorm(nn.Module):\n",
    "    '''레이어 정규화 층'''\n",
    "    \n",
    "    def __init__(self, hidden_size, eps=1e-12):\n",
    "        super(BertLayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(hidden_size)) # weight에 대한 것\n",
    "        self.beta = nn.Parameter(torch.zeros(hidden_size)) # 바이어스에 대한 것\n",
    "        self.variance_epsilon = eps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 평균\n",
    "        u = x.mean(-1, keepdim=True)\n",
    "        # 분산\n",
    "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
    "        # 일반 정규화 적용 variance_epsilon은 0으로 나누지 않도록 보조 항 삽입\n",
    "        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
    "        # 최종적으로 gamma 와 x를 곱하고 beta를 더해서 return\n",
    "        return self.gamma * x + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82ac7f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "BertLayerNorm                            1,536\n",
       "=================================================================\n",
       "Total params: 1,536\n",
       "Trainable params: 1,536\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
    "ti.summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7479886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ts.summary(model, input_size=(10, 768))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13acf22d",
   "metadata": {},
   "source": [
    "## 8.2.4 Embedding 구현\n",
    "### Transformer의 Embeddings 모듈과 두 가지 큰 차이점 존재\n",
    "- 첫째, Positional Embedding(위치 정보를 벡터로 변환)의 표현 기법을 Transformer는 sin, cos으로 계산하지만 BERT는 표현 방법도 학습시킵니다. 학습 시키는 것은 단어의 위치 정보뿐이며 단어 벡터의 차원 정보는 부여하지 않습니다. 즉 첫 번째 단어의 768차원은 동일한 position_embeddings값이 저장 되고 두 번째 단어는 첫 번째 단어와는 다르지만 768차원 방향에 같은 position_embeddings값이 저장됩니다.\n",
    "- 둘째, Sentence Embedding의 존재입니다. BERT는 두 문장을 입력합니다. 첫 번째 문장과 두 번째 문장을 구분하기 위한 Embedding를 준비합니다. Embeddings 모듈에서는 Token Embedding, Positional Embedding, Sentence Embedding에서 각각 구할 세 개의 텐서를 Transformer처럼 더하여 Embeddings 모듈의 출력으로 합니다. Embeddings모듈에 대한 입력 텐서는 (batch_size, seq_len)크기로 이루어진 문장의 단어 ID 나열인 변수 input_ids와 (batch_size, seq_len)의 각 단어가 첫 번째 문장인지 두 번째 문장인지 나타내는 문장 id인 변수 token_type_ids가 됩니다. 출력은 (batch_size, seq_len, hidden_size)의 텐서입니다. seq_len은 512이고 hidden_size는 768입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e1c4886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT의 Embeddings 모듈\n",
    "class BertEmbeddings(nn.Module):\n",
    "    '''문장의 단어 ID열과 첫 번째인지 두 번째 문장인지 정보를 내장 벡터로 변환'''\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(BertEmbeddings, self).__init__()\n",
    "        \n",
    "        # 세 개의 벡터 표현 내장\n",
    "        \n",
    "        # Token Embedding: 단어 ID를 단어 벡터로 변환\n",
    "        # vocab_size = 30522로 BERT의 학습된 모델에 사용된 vocabulary 양\n",
    "        # hidden_size = 768로 특징량 벡터의 길이는 768\n",
    "        self.word_embeddings = nn.Embedding(\n",
    "            config.vocab_size, config.hidden_size, padding_idx=0)\n",
    "        \n",
    "        # padding_idx = 0의 idx = 0 단어 벡터는 0으로 한다. BERT의 vocabulary의 idx=0은 [PAD]이다\n",
    "        \n",
    "        # Transformer Positional Embedding: 위치 정보 텐서를 벡터로 변환\n",
    "        # Transformer의 경우는 sin, cos로 이루어진 고정 값이지만 BERT는 학습시킨다.\n",
    "        # max_position_embeddings = 512로 문장 길이는 512단어\n",
    "        self.position_embeddings = nn.Embedding(\n",
    "            config.max_position_embeddings, config.hidden_size)\n",
    "        \n",
    "        # Sentence Embedding: 첫 번째, 두 번째 문장을 벡터로 변환\n",
    "        # type_vocab_size = 2\n",
    "        self.token_type_embeddings = nn.Embedding(\n",
    "            config.type_vocab_size, config.hidden_size)\n",
    "        \n",
    "        # 작성한 레이어 정규화 층\n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
    "        \n",
    "        # 드롭아웃 'hidden_dropout_prob':0.1\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        \n",
    "    def forward(self, input_ids, token_type_ids=None):\n",
    "        '''\n",
    "        input_ids: [batch_size, seq_len] 문장의 단어 ID 나열\n",
    "        token_type_ids: [batch_size, seq_len] 각 단어가 첫 번째 문장인지 두 번째 문장인지 나타내는 id\n",
    "        '''\n",
    "        \n",
    "        # 1. Token Embeddings\n",
    "        # 단어 ID를 단어 벡터로 변환\n",
    "        words_embeddings = self.word_embeddings(input_ids)\n",
    "        \n",
    "        # 2. Sentence Embeddings\n",
    "        # token_type_ids가 없는 경우는 문장의 모든 단어를 첫 번째 문장으로 하여 0으로 설정\n",
    "        # input_ids와 같은 크기로 제로 텐서 작성\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "        \n",
    "        # 3. Transformer Positional Embedding:\n",
    "        # [0, 1, 2, ...]로 문장의 길이 만큼 숫자가 하나씩 올라간다.\n",
    "        # [batch_size, seq_len]의 텐서 positional_ids 작성\n",
    "        # positional_ids를 입력하여 position_embeddings 층에서 768차원의 텐서를 꺼낸다\n",
    "        seq_length = input_ids.size(1) # 문장 길이\n",
    "        position_ids = torch.arange(\n",
    "            seq_length, dtype=torch.long, device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        \n",
    "        # 세 개의 내장 텐서를 더한다. [batch_size, seq_len, hidden_size]\n",
    "        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n",
    "        \n",
    "        # 레이어 정규화와 드롭아웃 실행\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        \n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "58e2fda4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "BertEmbeddings                           --\n",
       "├─Embedding: 1-1                         23,440,896\n",
       "├─Embedding: 1-2                         393,216\n",
       "├─Embedding: 1-3                         1,536\n",
       "├─BertLayerNorm: 1-4                     1,536\n",
       "├─Dropout: 1-5                           --\n",
       "=================================================================\n",
       "Total params: 23,837,184\n",
       "Trainable params: 23,837,184\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_model = BertEmbeddings(config)\n",
    "ti.summary(emb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0106330e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = torch.LongTensor([[31, 51, 12, 23, 99], [15, 5, 1, 0, 0]])\n",
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "be6d5258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_type_ids = torch.LongTensor([[0, 0, 1, 1, 1], [0, 1, 1, 1, 1]])\n",
    "token_type_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "bba1977d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 7632, 2026, 2171, 2003, 4080,  102]])"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "0484cb66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 7, 768]),\n",
       " tensor([[[ 1.0441, -1.0624, -0.0035,  ...,  0.4908,  0.4285,  0.1971],\n",
       "          [ 0.1484, -0.3204, -1.8787,  ...,  0.3521, -0.4409,  0.8620],\n",
       "          [ 1.1590, -1.1851, -0.6478,  ...,  0.0799, -0.9569, -0.3837],\n",
       "          ...,\n",
       "          [ 1.7973, -0.0000, -2.3744,  ...,  0.0000,  1.3792,  1.4229],\n",
       "          [-0.1955, -1.1131, -0.6789,  ...,  0.7324, -0.2080, -0.1860],\n",
       "          [ 2.6219, -0.9074, -0.3120,  ...,  1.7529,  0.3887,  0.8767]]],\n",
       "        grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_to_emb = emb_model(input_ids)\n",
    "token_to_emb.shape, token_to_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "4fb9cbdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-5.2473e-01,  1.1588e-01,  9.3648e-01,  1.6489e+00,  5.4864e-02,\n",
       "        -4.6306e-01, -1.7095e-01,  1.4399e+00, -1.2468e+00,  9.1688e-01,\n",
       "        -3.5326e-01, -9.2228e-01, -6.0763e-01, -1.3202e+00, -2.9825e-01,\n",
       "        -5.5747e-01,  5.2000e-02, -1.5810e-01,  3.4517e-01, -1.7875e-01,\n",
       "        -5.1647e-01, -9.3223e-01,  1.4936e+00, -9.9227e-01, -9.5211e-01,\n",
       "         5.8768e-01,  5.9252e-01,  6.8447e-02, -1.3360e-01, -1.3938e+00,\n",
       "        -1.0904e+00,  8.8862e-01, -2.5226e+00, -1.7354e+00, -7.6498e-01,\n",
       "        -9.8466e-03, -9.0285e-01, -1.5628e+00, -4.8471e-01, -4.4143e-01,\n",
       "         4.6817e-01,  1.4616e+00,  6.2225e-01,  2.5519e+00,  4.9818e-01,\n",
       "        -1.9967e+00, -1.6518e+00, -1.5871e+00, -7.3923e-01,  9.3160e-01,\n",
       "        -7.0247e-03, -9.7281e-01,  3.4608e-01, -1.2703e-01,  1.9975e+00,\n",
       "        -2.6407e-02,  7.2559e-01,  2.7518e-01, -1.8591e+00,  6.9145e-01,\n",
       "        -1.7654e+00,  6.2779e-01,  1.3450e+00, -6.2132e-01, -3.3217e-02,\n",
       "        -9.0692e-01,  1.5312e-01, -3.7733e-01, -4.7325e-01, -6.5366e-01,\n",
       "        -1.2425e-01,  2.2521e+00,  5.9061e-01,  1.8320e+00, -1.1270e+00,\n",
       "         2.5148e-01,  1.0706e+00,  3.0299e-01,  7.3789e-02,  1.7850e-01,\n",
       "         1.1533e+00, -1.4647e-01, -2.9854e-01, -1.0748e+00, -2.3557e+00,\n",
       "         1.6958e-01, -1.6493e+00, -2.6533e-01, -2.1007e+00,  6.5550e-01,\n",
       "        -8.7316e-01, -1.1465e+00,  1.4410e-01, -3.2928e-01, -9.2248e-02,\n",
       "         1.0246e+00,  7.9574e-01, -1.6367e+00,  7.0467e-01, -3.5615e-01,\n",
       "        -9.0086e-01,  4.5651e-01,  2.2647e+00, -2.3119e-01,  3.5663e-01,\n",
       "         4.7745e-02,  2.1005e+00,  8.1235e-01, -3.1158e-01,  1.0979e+00,\n",
       "         3.2444e-01,  7.5538e-01,  6.2549e-01, -5.9365e-01, -6.7249e-01,\n",
       "        -9.6360e-01,  4.2011e-01, -4.3999e-01,  6.0737e-01,  2.4403e-02,\n",
       "        -1.3960e+00, -1.4797e+00, -8.0132e-01, -1.9291e-01, -4.7938e-01,\n",
       "         1.3350e+00, -7.4682e-01, -7.4739e-01, -3.1267e-02,  4.2594e-01,\n",
       "         1.1863e+00, -1.4124e+00, -1.2820e+00,  4.8170e-01, -3.9602e-01,\n",
       "         1.8945e-02, -7.9278e-01,  4.4396e-02, -5.9688e-03,  5.1621e-02,\n",
       "         1.7404e+00, -1.1028e+00, -2.4473e+00,  2.3669e+00, -4.1653e-01,\n",
       "        -2.7647e-01, -4.0546e-01, -2.0745e+00, -9.7295e-01,  3.3538e-01,\n",
       "        -2.8535e-02,  3.0390e-01,  3.1201e-01, -1.9995e+00,  8.4294e-01,\n",
       "         1.8040e+00, -7.2554e-01,  2.1860e+00, -7.0259e-01,  1.1897e+00,\n",
       "        -7.4175e-01,  7.5258e-01,  2.9793e-01,  2.0622e-01, -3.2737e-01,\n",
       "         3.9769e-01, -1.6386e+00,  3.7948e-01,  2.8321e-01,  1.9281e+00,\n",
       "        -2.0728e+00,  1.9705e+00, -1.7975e-01,  1.9483e+00, -5.5371e-01,\n",
       "        -4.4678e-01,  1.3530e-01, -3.9710e-02,  1.7969e+00, -1.1804e+00,\n",
       "         7.0013e-02,  3.5222e-02,  1.1998e+00,  1.9555e-01, -2.1209e-01,\n",
       "        -4.4129e-01,  3.0858e-01, -1.7120e+00, -7.4367e-01, -4.5197e-01,\n",
       "        -6.3937e-01, -1.3638e+00,  9.5497e-01,  1.4475e+00,  1.2718e-01,\n",
       "         9.1248e-01, -1.2219e+00,  1.5991e-01,  1.9476e-01,  4.9215e-01,\n",
       "        -6.0501e-01,  6.8434e-01, -6.7944e-01,  4.4552e-01, -1.1566e+00,\n",
       "        -5.7060e-01, -3.7494e-01,  1.0279e-01, -1.0889e+00,  2.4045e+00,\n",
       "        -1.1647e+00, -7.4553e-01,  2.1858e-01,  5.1468e-01,  9.5274e-01,\n",
       "         6.7977e-01,  2.5583e-01,  5.5104e-01,  1.9157e-01,  2.7044e-01,\n",
       "        -5.3547e-01,  5.0524e-01,  1.9113e+00,  3.0502e-01, -9.4962e-01,\n",
       "        -9.7152e-01,  6.0179e-01, -3.3768e-01,  1.3119e+00, -1.2176e+00,\n",
       "         9.3066e-01,  3.8749e-01,  1.3782e+00,  2.0629e+00, -7.3328e-01,\n",
       "        -1.4539e-03,  1.3506e+00, -1.3789e+00, -1.2438e+00,  8.2135e-01,\n",
       "        -9.3656e-01, -2.2911e-01,  9.4131e-01,  3.2114e-01, -1.0985e+00,\n",
       "        -1.2459e+00, -1.1902e-01,  8.1054e-01,  1.9615e+00, -1.9219e-01,\n",
       "        -1.1677e+00, -5.0393e-01,  3.5272e-02,  3.6979e-01,  9.8733e-01,\n",
       "         3.2761e-01, -6.2120e-01,  5.6390e-01,  6.2163e-01, -1.4292e+00,\n",
       "        -7.7103e-01,  1.0212e+00,  2.3787e-02, -6.3711e-01,  1.6232e-01,\n",
       "         9.5024e-01,  5.2533e-02,  1.0382e+00, -4.0038e-02,  8.3279e-01,\n",
       "         9.0630e-01, -4.3393e-01, -2.0616e+00, -1.0450e+00, -3.2202e-01,\n",
       "        -6.8510e-01,  1.0043e+00, -1.7637e+00, -1.5940e+00, -1.4893e+00,\n",
       "        -3.5221e+00, -1.5994e+00, -1.1209e+00, -9.9651e-01,  6.0995e-01,\n",
       "         1.1737e+00, -1.4856e+00, -6.3666e-01, -4.8704e-01,  3.2151e-01,\n",
       "         5.3999e-01,  5.9929e-01,  5.2548e-01,  5.4255e-01,  5.3232e-02,\n",
       "         4.7559e-01,  1.0805e+00,  7.9024e-01,  7.7449e-01,  9.2044e-01,\n",
       "         7.1503e-01,  2.1038e+00, -1.3687e+00, -2.0968e+00, -3.1228e-01,\n",
       "        -4.7635e-01, -1.1990e+00, -1.0571e+00, -4.2240e-01,  2.1098e+00,\n",
       "        -1.2646e-01, -7.0462e-02, -1.1815e+00,  6.4741e-01,  1.2705e+00,\n",
       "         2.2485e-01,  4.2333e-02, -1.1056e+00, -8.5400e-01, -1.3873e+00,\n",
       "        -4.9600e-01,  3.6102e-01, -7.3650e-01, -7.0218e-01, -2.1074e+00,\n",
       "        -3.7924e-01, -2.8850e-01, -4.5891e-01, -1.4211e+00, -1.4014e-01,\n",
       "        -4.3989e-01, -5.8724e-02,  2.1569e+00,  8.8537e-01,  1.7874e+00,\n",
       "         3.6943e-01,  1.6280e+00, -1.0073e+00, -6.7446e-01,  2.2232e-01,\n",
       "         6.0120e-01,  1.4967e+00,  2.1106e+00,  2.9095e-01,  4.1686e-01,\n",
       "        -3.8662e-01,  9.6138e-01, -1.0616e-01,  1.8926e-01,  1.2898e-01,\n",
       "        -7.9569e-01,  8.3564e-01,  5.4753e-01,  1.2220e+00, -5.6376e-01,\n",
       "         6.9770e-01, -5.5822e-01, -1.7434e+00,  9.6391e-01, -1.3866e+00,\n",
       "         1.0574e+00,  7.5350e-01,  3.3936e-01,  2.3197e+00, -8.2411e-01,\n",
       "        -5.4330e-01,  5.7605e-01, -5.4293e-01,  1.0181e+00, -5.2961e-02,\n",
       "         9.0136e-01,  7.7772e-02, -3.8303e-01,  2.2802e-01,  3.4522e-01,\n",
       "        -5.8396e-01,  1.3346e+00, -1.1029e+00, -7.3429e-01, -1.1770e+00,\n",
       "        -2.1263e+00, -9.9490e-01, -1.4909e-01,  3.5163e-01, -9.0720e-01,\n",
       "         1.4410e+00, -9.4937e-01, -4.6826e-01, -1.3483e+00, -8.4931e-01,\n",
       "         1.2705e+00,  1.0785e-01, -1.8880e-01, -5.0598e-01,  2.0129e-01,\n",
       "         1.0472e+00, -2.3079e+00,  7.7730e-02,  8.5254e-01,  1.1248e+00,\n",
       "         1.3472e+00,  3.6461e-01, -4.8398e-01, -1.6283e+00, -1.1922e+00,\n",
       "        -1.0272e+00,  1.5020e-01, -1.2371e+00,  3.2285e-01, -3.1192e-01,\n",
       "        -6.3774e-01, -1.2033e+00,  1.1963e+00, -4.4477e-01, -1.9399e+00,\n",
       "        -4.2973e-02,  5.1891e-01, -1.5007e+00,  8.8546e-01,  1.6136e+00,\n",
       "        -1.5274e+00,  1.6793e+00, -7.7742e-01,  2.6893e-01,  9.9938e-01,\n",
       "         1.1141e-01,  9.6090e-02,  9.6252e-01,  1.2326e-01, -1.9971e-01,\n",
       "        -3.8660e-01, -5.6936e-01,  1.6489e+00, -3.0723e-01,  1.0735e+00,\n",
       "         2.5006e-01, -1.2974e+00, -5.8956e-01,  6.2026e-03,  1.8024e-01,\n",
       "        -1.4482e+00,  5.3607e-01,  7.1887e-01,  1.4044e-01,  3.1792e-01,\n",
       "         1.2122e+00,  2.2342e+00,  2.5546e+00, -9.9315e-01,  1.7264e+00,\n",
       "        -5.1605e-01, -4.3188e-01,  1.4363e-01,  5.4367e-01, -6.9247e-01,\n",
       "        -2.1469e+00, -7.5426e-01,  1.0884e-01,  8.8251e-01,  1.1475e+00,\n",
       "         1.8677e+00, -1.7304e+00, -1.0109e+00, -1.0625e+00, -4.7871e-01,\n",
       "        -8.2731e-01,  9.0825e-01,  3.9823e-01, -4.4271e-01,  1.5989e+00,\n",
       "         1.5920e-02, -9.9818e-01, -5.1303e-01, -8.2401e-01, -1.1493e+00,\n",
       "         1.6713e+00, -1.6742e+00,  8.2292e-01, -2.1505e+00,  1.1607e+00,\n",
       "        -3.7909e-01,  3.5782e-02, -3.3976e-01,  8.3782e-01,  1.0115e+00,\n",
       "         4.5694e-01,  6.8455e-01,  7.5236e-01, -8.1638e-01,  6.4215e-01,\n",
       "         2.3587e-01,  4.2200e-02,  4.3344e-01, -8.7341e-01,  1.5926e+00,\n",
       "         3.4531e-01,  1.5254e+00, -1.4914e+00, -1.7548e-01,  5.2829e-01,\n",
       "         8.4815e-01,  1.1797e+00, -9.9017e-01, -7.4520e-01, -5.4848e-01,\n",
       "        -5.9483e-01, -6.2456e-01,  8.8206e-01,  1.0313e-01, -1.9324e-01,\n",
       "         2.5040e-01, -6.4073e-01, -1.0710e-01,  1.0720e+00, -4.7089e-01,\n",
       "         4.7256e-02, -1.3645e+00,  6.7773e-01,  4.8647e-01,  4.9086e-01,\n",
       "         4.1207e-01,  7.8486e-01, -1.0953e-01,  1.4802e+00, -3.4185e-01,\n",
       "         5.5661e-01, -6.7258e-01,  1.7243e+00, -1.3022e+00,  9.7786e-01,\n",
       "        -4.5697e-01,  5.9886e-01, -7.1227e-01, -7.7220e-01,  1.6124e-01,\n",
       "         2.0665e-01, -1.2252e+00,  1.0635e+00, -9.5268e-01, -8.1797e-01,\n",
       "         5.8136e-01,  1.3615e+00, -1.9794e-01, -3.1123e-01,  7.2927e-01,\n",
       "         6.9810e-01,  4.1332e-02,  1.8027e+00,  1.0419e+00,  1.4403e+00,\n",
       "         4.9702e-01, -1.6172e+00,  6.6582e-02, -1.0574e+00, -2.5174e-01,\n",
       "        -1.7720e+00, -1.5908e+00, -2.1229e+00,  4.3753e-01,  5.6065e-01,\n",
       "        -2.0625e+00, -1.9358e-01, -1.4683e-01, -1.0280e+00, -9.1165e-01,\n",
       "        -4.2371e-01, -1.6894e+00,  1.4492e+00, -3.1091e-01,  2.4479e-01,\n",
       "         3.5963e-01,  7.1477e-01, -1.1041e+00, -1.0235e+00,  2.0446e+00,\n",
       "        -4.8292e-01,  7.4377e-01, -1.2920e+00, -7.8003e-01, -6.7284e-01,\n",
       "         1.1400e+00, -7.2149e-01, -1.3006e+00,  1.2047e+00,  9.3721e-02,\n",
       "         1.1166e+00, -2.5158e+00,  9.4084e-01,  5.7094e-01, -5.2351e-01,\n",
       "         1.4452e+00, -1.9280e+00,  3.8525e-01, -5.1837e-01, -1.0496e+00,\n",
       "         7.6837e-01, -3.4731e-01,  1.4291e-01,  1.4662e+00,  1.4934e+00,\n",
       "         7.7607e-02,  5.9160e-01, -1.1547e+00,  1.7223e+00,  3.8764e-01,\n",
       "         3.1441e-01,  4.6771e-01, -9.8762e-01, -3.0647e-01, -4.2256e-01,\n",
       "         2.2125e+00, -5.6591e-02, -9.3524e-01, -6.0319e-01, -6.3727e-01,\n",
       "         4.9496e-01, -4.3252e-01, -3.7675e-01, -1.7241e+00,  4.9289e-01,\n",
       "        -1.8486e-01,  3.0106e-01, -1.2219e+00, -4.7638e-01,  1.5889e+00,\n",
       "         7.3825e-01,  4.2778e-01,  2.0941e+00,  7.6266e-01, -3.1509e-01,\n",
       "        -1.5202e+00,  4.4585e-01,  1.0269e+00,  1.2083e+00, -3.6185e-01,\n",
       "        -4.3806e-01, -7.0708e-01,  5.5328e-01,  6.6532e-01, -4.6342e-01,\n",
       "        -1.0066e+00,  7.2349e-01,  6.7864e-01,  2.7394e-01, -1.9498e-01,\n",
       "         6.0047e-01,  5.6663e-01, -2.8239e+00,  5.9191e-01, -9.8753e-01,\n",
       "        -1.2231e+00, -7.6314e-01, -6.5095e-02, -4.3196e-01,  7.9841e-01,\n",
       "         4.1356e-01,  4.2206e-01,  1.6607e+00, -1.6133e+00, -1.4584e-01,\n",
       "         1.5527e+00, -4.4005e-01,  2.5839e-01, -2.7972e-01,  2.6359e-02,\n",
       "        -1.0526e+00, -6.3489e-01, -2.7228e-02,  2.5674e-03,  7.9338e-01,\n",
       "         1.1550e-01, -1.4442e+00, -6.2519e-01, -5.5710e-01,  4.9705e-01,\n",
       "         6.1694e-01,  9.1893e-01,  9.9493e-01, -7.8262e-01,  2.1021e+00,\n",
       "        -1.0415e+00,  1.9466e+00,  9.8566e-01,  1.1851e+00,  3.6332e-01,\n",
       "        -7.3700e-02,  1.3113e-01, -4.8082e-02, -4.4022e-01,  1.7205e-01,\n",
       "         6.1786e-01,  3.2600e-01, -7.0288e-02,  1.7233e+00,  9.5321e-01,\n",
       "         9.9008e-01, -1.0148e+00, -1.2218e+00, -1.6344e+00,  1.4756e+00,\n",
       "         4.4621e-01, -9.1810e-01,  4.6610e-01, -1.7740e+00,  2.6708e-01,\n",
       "         1.1214e+00,  1.9502e+00, -2.4614e+00, -7.1847e-03, -2.1367e-02,\n",
       "         1.5872e+00, -4.5813e-01,  3.3795e-02, -1.0573e+00,  7.4366e-01,\n",
       "         4.9285e-01,  4.0213e-01,  2.2774e-01,  8.6437e-01, -8.1799e-01,\n",
       "         6.5116e-01,  3.3331e-01,  6.9694e-01,  3.9635e-01, -1.8460e+00,\n",
       "        -7.9314e-01,  1.2248e-01, -5.7664e-01,  6.0360e-01,  4.6910e-01,\n",
       "        -1.7790e+00, -7.8136e-01,  6.1264e-02,  7.6985e-01, -5.4907e-01,\n",
       "        -6.0505e-02,  1.4500e-01, -4.1360e-01, -3.2709e-01,  1.6905e+00,\n",
       "        -7.2586e-01,  5.1594e-01, -2.0225e-01, -2.4872e-01,  9.4430e-01,\n",
       "         5.6167e-01,  1.0251e+00,  2.3209e+00,  2.5240e+00, -2.5066e-01,\n",
       "        -2.0721e-02,  2.4765e-01, -3.5852e-01,  2.5957e+00,  1.0966e+00,\n",
       "         9.8474e-01,  8.8359e-01,  8.6967e-02, -2.4319e+00, -2.6606e-01,\n",
       "        -1.1415e-01,  6.1799e-01,  1.3616e+00, -9.2137e-01, -5.2399e-01,\n",
       "         7.8081e-01, -3.7398e-01,  2.1997e-01], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_model.word_embeddings.weight[101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "7511ea26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-5.2473e-01,  1.1588e-01,  9.3648e-01,  1.6489e+00,  5.4864e-02,\n",
       "        -4.6306e-01, -1.7095e-01,  1.4399e+00, -1.2468e+00,  9.1688e-01,\n",
       "        -3.5326e-01, -9.2228e-01, -6.0763e-01, -1.3202e+00, -2.9825e-01,\n",
       "        -5.5747e-01,  5.2000e-02, -1.5810e-01,  3.4517e-01, -1.7875e-01,\n",
       "        -5.1647e-01, -9.3223e-01,  1.4936e+00, -9.9227e-01, -9.5211e-01,\n",
       "         5.8768e-01,  5.9252e-01,  6.8447e-02, -1.3360e-01, -1.3938e+00,\n",
       "        -1.0904e+00,  8.8862e-01, -2.5226e+00, -1.7354e+00, -7.6498e-01,\n",
       "        -9.8466e-03, -9.0285e-01, -1.5628e+00, -4.8471e-01, -4.4143e-01,\n",
       "         4.6817e-01,  1.4616e+00,  6.2225e-01,  2.5519e+00,  4.9818e-01,\n",
       "        -1.9967e+00, -1.6518e+00, -1.5871e+00, -7.3923e-01,  9.3160e-01,\n",
       "        -7.0247e-03, -9.7281e-01,  3.4608e-01, -1.2703e-01,  1.9975e+00,\n",
       "        -2.6407e-02,  7.2559e-01,  2.7518e-01, -1.8591e+00,  6.9145e-01,\n",
       "        -1.7654e+00,  6.2779e-01,  1.3450e+00, -6.2132e-01, -3.3217e-02,\n",
       "        -9.0692e-01,  1.5312e-01, -3.7733e-01, -4.7325e-01, -6.5366e-01,\n",
       "        -1.2425e-01,  2.2521e+00,  5.9061e-01,  1.8320e+00, -1.1270e+00,\n",
       "         2.5148e-01,  1.0706e+00,  3.0299e-01,  7.3789e-02,  1.7850e-01,\n",
       "         1.1533e+00, -1.4647e-01, -2.9854e-01, -1.0748e+00, -2.3557e+00,\n",
       "         1.6958e-01, -1.6493e+00, -2.6533e-01, -2.1007e+00,  6.5550e-01,\n",
       "        -8.7316e-01, -1.1465e+00,  1.4410e-01, -3.2928e-01, -9.2248e-02,\n",
       "         1.0246e+00,  7.9574e-01, -1.6367e+00,  7.0467e-01, -3.5615e-01,\n",
       "        -9.0086e-01,  4.5651e-01,  2.2647e+00, -2.3119e-01,  3.5663e-01,\n",
       "         4.7745e-02,  2.1005e+00,  8.1235e-01, -3.1158e-01,  1.0979e+00,\n",
       "         3.2444e-01,  7.5538e-01,  6.2549e-01, -5.9365e-01, -6.7249e-01,\n",
       "        -9.6360e-01,  4.2011e-01, -4.3999e-01,  6.0737e-01,  2.4403e-02,\n",
       "        -1.3960e+00, -1.4797e+00, -8.0132e-01, -1.9291e-01, -4.7938e-01,\n",
       "         1.3350e+00, -7.4682e-01, -7.4739e-01, -3.1267e-02,  4.2594e-01,\n",
       "         1.1863e+00, -1.4124e+00, -1.2820e+00,  4.8170e-01, -3.9602e-01,\n",
       "         1.8945e-02, -7.9278e-01,  4.4396e-02, -5.9688e-03,  5.1621e-02,\n",
       "         1.7404e+00, -1.1028e+00, -2.4473e+00,  2.3669e+00, -4.1653e-01,\n",
       "        -2.7647e-01, -4.0546e-01, -2.0745e+00, -9.7295e-01,  3.3538e-01,\n",
       "        -2.8535e-02,  3.0390e-01,  3.1201e-01, -1.9995e+00,  8.4294e-01,\n",
       "         1.8040e+00, -7.2554e-01,  2.1860e+00, -7.0259e-01,  1.1897e+00,\n",
       "        -7.4175e-01,  7.5258e-01,  2.9793e-01,  2.0622e-01, -3.2737e-01,\n",
       "         3.9769e-01, -1.6386e+00,  3.7948e-01,  2.8321e-01,  1.9281e+00,\n",
       "        -2.0728e+00,  1.9705e+00, -1.7975e-01,  1.9483e+00, -5.5371e-01,\n",
       "        -4.4678e-01,  1.3530e-01, -3.9710e-02,  1.7969e+00, -1.1804e+00,\n",
       "         7.0013e-02,  3.5222e-02,  1.1998e+00,  1.9555e-01, -2.1209e-01,\n",
       "        -4.4129e-01,  3.0858e-01, -1.7120e+00, -7.4367e-01, -4.5197e-01,\n",
       "        -6.3937e-01, -1.3638e+00,  9.5497e-01,  1.4475e+00,  1.2718e-01,\n",
       "         9.1248e-01, -1.2219e+00,  1.5991e-01,  1.9476e-01,  4.9215e-01,\n",
       "        -6.0501e-01,  6.8434e-01, -6.7944e-01,  4.4552e-01, -1.1566e+00,\n",
       "        -5.7060e-01, -3.7494e-01,  1.0279e-01, -1.0889e+00,  2.4045e+00,\n",
       "        -1.1647e+00, -7.4553e-01,  2.1858e-01,  5.1468e-01,  9.5274e-01,\n",
       "         6.7977e-01,  2.5583e-01,  5.5104e-01,  1.9157e-01,  2.7044e-01,\n",
       "        -5.3547e-01,  5.0524e-01,  1.9113e+00,  3.0502e-01, -9.4962e-01,\n",
       "        -9.7152e-01,  6.0179e-01, -3.3768e-01,  1.3119e+00, -1.2176e+00,\n",
       "         9.3066e-01,  3.8749e-01,  1.3782e+00,  2.0629e+00, -7.3328e-01,\n",
       "        -1.4539e-03,  1.3506e+00, -1.3789e+00, -1.2438e+00,  8.2135e-01,\n",
       "        -9.3656e-01, -2.2911e-01,  9.4131e-01,  3.2114e-01, -1.0985e+00,\n",
       "        -1.2459e+00, -1.1902e-01,  8.1054e-01,  1.9615e+00, -1.9219e-01,\n",
       "        -1.1677e+00, -5.0393e-01,  3.5272e-02,  3.6979e-01,  9.8733e-01,\n",
       "         3.2761e-01, -6.2120e-01,  5.6390e-01,  6.2163e-01, -1.4292e+00,\n",
       "        -7.7103e-01,  1.0212e+00,  2.3787e-02, -6.3711e-01,  1.6232e-01,\n",
       "         9.5024e-01,  5.2533e-02,  1.0382e+00, -4.0038e-02,  8.3279e-01,\n",
       "         9.0630e-01, -4.3393e-01, -2.0616e+00, -1.0450e+00, -3.2202e-01,\n",
       "        -6.8510e-01,  1.0043e+00, -1.7637e+00, -1.5940e+00, -1.4893e+00,\n",
       "        -3.5221e+00, -1.5994e+00, -1.1209e+00, -9.9651e-01,  6.0995e-01,\n",
       "         1.1737e+00, -1.4856e+00, -6.3666e-01, -4.8704e-01,  3.2151e-01,\n",
       "         5.3999e-01,  5.9929e-01,  5.2548e-01,  5.4255e-01,  5.3232e-02,\n",
       "         4.7559e-01,  1.0805e+00,  7.9024e-01,  7.7449e-01,  9.2044e-01,\n",
       "         7.1503e-01,  2.1038e+00, -1.3687e+00, -2.0968e+00, -3.1228e-01,\n",
       "        -4.7635e-01, -1.1990e+00, -1.0571e+00, -4.2240e-01,  2.1098e+00,\n",
       "        -1.2646e-01, -7.0462e-02, -1.1815e+00,  6.4741e-01,  1.2705e+00,\n",
       "         2.2485e-01,  4.2333e-02, -1.1056e+00, -8.5400e-01, -1.3873e+00,\n",
       "        -4.9600e-01,  3.6102e-01, -7.3650e-01, -7.0218e-01, -2.1074e+00,\n",
       "        -3.7924e-01, -2.8850e-01, -4.5891e-01, -1.4211e+00, -1.4014e-01,\n",
       "        -4.3989e-01, -5.8724e-02,  2.1569e+00,  8.8537e-01,  1.7874e+00,\n",
       "         3.6943e-01,  1.6280e+00, -1.0073e+00, -6.7446e-01,  2.2232e-01,\n",
       "         6.0120e-01,  1.4967e+00,  2.1106e+00,  2.9095e-01,  4.1686e-01,\n",
       "        -3.8662e-01,  9.6138e-01, -1.0616e-01,  1.8926e-01,  1.2898e-01,\n",
       "        -7.9569e-01,  8.3564e-01,  5.4753e-01,  1.2220e+00, -5.6376e-01,\n",
       "         6.9770e-01, -5.5822e-01, -1.7434e+00,  9.6391e-01, -1.3866e+00,\n",
       "         1.0574e+00,  7.5350e-01,  3.3936e-01,  2.3197e+00, -8.2411e-01,\n",
       "        -5.4330e-01,  5.7605e-01, -5.4293e-01,  1.0181e+00, -5.2961e-02,\n",
       "         9.0136e-01,  7.7772e-02, -3.8303e-01,  2.2802e-01,  3.4522e-01,\n",
       "        -5.8396e-01,  1.3346e+00, -1.1029e+00, -7.3429e-01, -1.1770e+00,\n",
       "        -2.1263e+00, -9.9490e-01, -1.4909e-01,  3.5163e-01, -9.0720e-01,\n",
       "         1.4410e+00, -9.4937e-01, -4.6826e-01, -1.3483e+00, -8.4931e-01,\n",
       "         1.2705e+00,  1.0785e-01, -1.8880e-01, -5.0598e-01,  2.0129e-01,\n",
       "         1.0472e+00, -2.3079e+00,  7.7730e-02,  8.5254e-01,  1.1248e+00,\n",
       "         1.3472e+00,  3.6461e-01, -4.8398e-01, -1.6283e+00, -1.1922e+00,\n",
       "        -1.0272e+00,  1.5020e-01, -1.2371e+00,  3.2285e-01, -3.1192e-01,\n",
       "        -6.3774e-01, -1.2033e+00,  1.1963e+00, -4.4477e-01, -1.9399e+00,\n",
       "        -4.2973e-02,  5.1891e-01, -1.5007e+00,  8.8546e-01,  1.6136e+00,\n",
       "        -1.5274e+00,  1.6793e+00, -7.7742e-01,  2.6893e-01,  9.9938e-01,\n",
       "         1.1141e-01,  9.6090e-02,  9.6252e-01,  1.2326e-01, -1.9971e-01,\n",
       "        -3.8660e-01, -5.6936e-01,  1.6489e+00, -3.0723e-01,  1.0735e+00,\n",
       "         2.5006e-01, -1.2974e+00, -5.8956e-01,  6.2026e-03,  1.8024e-01,\n",
       "        -1.4482e+00,  5.3607e-01,  7.1887e-01,  1.4044e-01,  3.1792e-01,\n",
       "         1.2122e+00,  2.2342e+00,  2.5546e+00, -9.9315e-01,  1.7264e+00,\n",
       "        -5.1605e-01, -4.3188e-01,  1.4363e-01,  5.4367e-01, -6.9247e-01,\n",
       "        -2.1469e+00, -7.5426e-01,  1.0884e-01,  8.8251e-01,  1.1475e+00,\n",
       "         1.8677e+00, -1.7304e+00, -1.0109e+00, -1.0625e+00, -4.7871e-01,\n",
       "        -8.2731e-01,  9.0825e-01,  3.9823e-01, -4.4271e-01,  1.5989e+00,\n",
       "         1.5920e-02, -9.9818e-01, -5.1303e-01, -8.2401e-01, -1.1493e+00,\n",
       "         1.6713e+00, -1.6742e+00,  8.2292e-01, -2.1505e+00,  1.1607e+00,\n",
       "        -3.7909e-01,  3.5782e-02, -3.3976e-01,  8.3782e-01,  1.0115e+00,\n",
       "         4.5694e-01,  6.8455e-01,  7.5236e-01, -8.1638e-01,  6.4215e-01,\n",
       "         2.3587e-01,  4.2200e-02,  4.3344e-01, -8.7341e-01,  1.5926e+00,\n",
       "         3.4531e-01,  1.5254e+00, -1.4914e+00, -1.7548e-01,  5.2829e-01,\n",
       "         8.4815e-01,  1.1797e+00, -9.9017e-01, -7.4520e-01, -5.4848e-01,\n",
       "        -5.9483e-01, -6.2456e-01,  8.8206e-01,  1.0313e-01, -1.9324e-01,\n",
       "         2.5040e-01, -6.4073e-01, -1.0710e-01,  1.0720e+00, -4.7089e-01,\n",
       "         4.7256e-02, -1.3645e+00,  6.7773e-01,  4.8647e-01,  4.9086e-01,\n",
       "         4.1207e-01,  7.8486e-01, -1.0953e-01,  1.4802e+00, -3.4185e-01,\n",
       "         5.5661e-01, -6.7258e-01,  1.7243e+00, -1.3022e+00,  9.7786e-01,\n",
       "        -4.5697e-01,  5.9886e-01, -7.1227e-01, -7.7220e-01,  1.6124e-01,\n",
       "         2.0665e-01, -1.2252e+00,  1.0635e+00, -9.5268e-01, -8.1797e-01,\n",
       "         5.8136e-01,  1.3615e+00, -1.9794e-01, -3.1123e-01,  7.2927e-01,\n",
       "         6.9810e-01,  4.1332e-02,  1.8027e+00,  1.0419e+00,  1.4403e+00,\n",
       "         4.9702e-01, -1.6172e+00,  6.6582e-02, -1.0574e+00, -2.5174e-01,\n",
       "        -1.7720e+00, -1.5908e+00, -2.1229e+00,  4.3753e-01,  5.6065e-01,\n",
       "        -2.0625e+00, -1.9358e-01, -1.4683e-01, -1.0280e+00, -9.1165e-01,\n",
       "        -4.2371e-01, -1.6894e+00,  1.4492e+00, -3.1091e-01,  2.4479e-01,\n",
       "         3.5963e-01,  7.1477e-01, -1.1041e+00, -1.0235e+00,  2.0446e+00,\n",
       "        -4.8292e-01,  7.4377e-01, -1.2920e+00, -7.8003e-01, -6.7284e-01,\n",
       "         1.1400e+00, -7.2149e-01, -1.3006e+00,  1.2047e+00,  9.3721e-02,\n",
       "         1.1166e+00, -2.5158e+00,  9.4084e-01,  5.7094e-01, -5.2351e-01,\n",
       "         1.4452e+00, -1.9280e+00,  3.8525e-01, -5.1837e-01, -1.0496e+00,\n",
       "         7.6837e-01, -3.4731e-01,  1.4291e-01,  1.4662e+00,  1.4934e+00,\n",
       "         7.7607e-02,  5.9160e-01, -1.1547e+00,  1.7223e+00,  3.8764e-01,\n",
       "         3.1441e-01,  4.6771e-01, -9.8762e-01, -3.0647e-01, -4.2256e-01,\n",
       "         2.2125e+00, -5.6591e-02, -9.3524e-01, -6.0319e-01, -6.3727e-01,\n",
       "         4.9496e-01, -4.3252e-01, -3.7675e-01, -1.7241e+00,  4.9289e-01,\n",
       "        -1.8486e-01,  3.0106e-01, -1.2219e+00, -4.7638e-01,  1.5889e+00,\n",
       "         7.3825e-01,  4.2778e-01,  2.0941e+00,  7.6266e-01, -3.1509e-01,\n",
       "        -1.5202e+00,  4.4585e-01,  1.0269e+00,  1.2083e+00, -3.6185e-01,\n",
       "        -4.3806e-01, -7.0708e-01,  5.5328e-01,  6.6532e-01, -4.6342e-01,\n",
       "        -1.0066e+00,  7.2349e-01,  6.7864e-01,  2.7394e-01, -1.9498e-01,\n",
       "         6.0047e-01,  5.6663e-01, -2.8239e+00,  5.9191e-01, -9.8753e-01,\n",
       "        -1.2231e+00, -7.6314e-01, -6.5095e-02, -4.3196e-01,  7.9841e-01,\n",
       "         4.1356e-01,  4.2206e-01,  1.6607e+00, -1.6133e+00, -1.4584e-01,\n",
       "         1.5527e+00, -4.4005e-01,  2.5839e-01, -2.7972e-01,  2.6359e-02,\n",
       "        -1.0526e+00, -6.3489e-01, -2.7228e-02,  2.5674e-03,  7.9338e-01,\n",
       "         1.1550e-01, -1.4442e+00, -6.2519e-01, -5.5710e-01,  4.9705e-01,\n",
       "         6.1694e-01,  9.1893e-01,  9.9493e-01, -7.8262e-01,  2.1021e+00,\n",
       "        -1.0415e+00,  1.9466e+00,  9.8566e-01,  1.1851e+00,  3.6332e-01,\n",
       "        -7.3700e-02,  1.3113e-01, -4.8082e-02, -4.4022e-01,  1.7205e-01,\n",
       "         6.1786e-01,  3.2600e-01, -7.0288e-02,  1.7233e+00,  9.5321e-01,\n",
       "         9.9008e-01, -1.0148e+00, -1.2218e+00, -1.6344e+00,  1.4756e+00,\n",
       "         4.4621e-01, -9.1810e-01,  4.6610e-01, -1.7740e+00,  2.6708e-01,\n",
       "         1.1214e+00,  1.9502e+00, -2.4614e+00, -7.1847e-03, -2.1367e-02,\n",
       "         1.5872e+00, -4.5813e-01,  3.3795e-02, -1.0573e+00,  7.4366e-01,\n",
       "         4.9285e-01,  4.0213e-01,  2.2774e-01,  8.6437e-01, -8.1799e-01,\n",
       "         6.5116e-01,  3.3331e-01,  6.9694e-01,  3.9635e-01, -1.8460e+00,\n",
       "        -7.9314e-01,  1.2248e-01, -5.7664e-01,  6.0360e-01,  4.6910e-01,\n",
       "        -1.7790e+00, -7.8136e-01,  6.1264e-02,  7.6985e-01, -5.4907e-01,\n",
       "        -6.0505e-02,  1.4500e-01, -4.1360e-01, -3.2709e-01,  1.6905e+00,\n",
       "        -7.2586e-01,  5.1594e-01, -2.0225e-01, -2.4872e-01,  9.4430e-01,\n",
       "         5.6167e-01,  1.0251e+00,  2.3209e+00,  2.5240e+00, -2.5066e-01,\n",
       "        -2.0721e-02,  2.4765e-01, -3.5852e-01,  2.5957e+00,  1.0966e+00,\n",
       "         9.8474e-01,  8.8359e-01,  8.6967e-02, -2.4319e+00, -2.6606e-01,\n",
       "        -1.1415e-01,  6.1799e-01,  1.3616e+00, -9.2137e-01, -5.2399e-01,\n",
       "         7.8081e-01, -3.7398e-01,  2.1997e-01], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = emb_model.word_embeddings(input_ids)\n",
    "input_ids.shape, emb.shape\n",
    "emb[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "c1d9c51a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 101, 7632, 2026, 2171, 2003, 4080,  102])"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "1ab4a35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_length : 7\n",
      "position_ids : tensor([0, 1, 2, 3, 4, 5, 6])\n",
      "after unsqueeze position_ids : tensor([[0, 1, 2, 3, 4, 5, 6]])\n",
      "after embedding position_embeddings : tensor([[[ 0.6665, -0.3817,  0.2419,  ..., -0.7578,  0.2447, -0.6742],\n",
      "         [-1.0725,  0.2605, -0.8837,  ...,  0.3608, -1.1377, -0.4868],\n",
      "         [ 1.5611, -1.0221,  0.5453,  ..., -1.2333, -2.0115, -0.5676],\n",
      "         ...,\n",
      "         [ 0.8230, -0.9710,  1.7839,  ..., -0.2120,  1.5923,  1.1770],\n",
      "         [-0.5308, -0.5556,  0.3099,  ...,  0.2551, -1.6586, -1.1233],\n",
      "         [ 1.1312, -0.5512,  1.2568,  ...,  1.6600, -0.5874,  0.4305]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "position_embeddings shape : torch.Size([1, 7, 768])\n"
     ]
    }
   ],
   "source": [
    "seq_length = input_ids.size(1) # 문장 길이\n",
    "print('seq_length : {}'.format(seq_length))\n",
    "position_ids = torch.arange(\n",
    "    seq_length, dtype=torch.long, device=input_ids.device)\n",
    "print('position_ids : {}'.format(position_ids))\n",
    "position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "print('after unsqueeze position_ids : {}'.format(position_ids))\n",
    "position_embeddings = emb_model.position_embeddings(position_ids)\n",
    "print('after embedding position_embeddings : {}'.format(position_embeddings))\n",
    "print('position_embeddings shape : {}'.format(position_embeddings.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "1b7edada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.5162, -1.3922, -1.1696,  ...,  0.7639,  0.8182,  0.7789],\n",
       "         [ 1.5162, -1.3922, -1.1696,  ...,  0.7639,  0.8182,  0.7789],\n",
       "         [ 1.5162, -1.3922, -1.1696,  ...,  0.7639,  0.8182,  0.7789],\n",
       "         ...,\n",
       "         [ 1.5162, -1.3922, -1.1696,  ...,  0.7639,  0.8182,  0.7789],\n",
       "         [ 1.5162, -1.3922, -1.1696,  ...,  0.7639,  0.8182,  0.7789],\n",
       "         [ 1.5162, -1.3922, -1.1696,  ...,  0.7639,  0.8182,  0.7789]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_emb = emb_model.token_type_embeddings(torch.zeros_like(input_ids))\n",
    "tok_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "544fb62f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.5162, -1.3922, -1.1696,  ...,  0.7639,  0.8182,  0.7789],\n",
       "         [ 1.5162, -1.3922, -1.1696,  ...,  0.7639,  0.8182,  0.7789],\n",
       "         [ 1.5162, -1.3922, -1.1696,  ...,  0.7639,  0.8182,  0.7789],\n",
       "         ...,\n",
       "         [ 1.5162, -1.3922, -1.1696,  ...,  0.7639,  0.8182,  0.7789],\n",
       "         [ 1.5162, -1.3922, -1.1696,  ...,  0.7639,  0.8182,  0.7789],\n",
       "         [ 1.5162, -1.3922, -1.1696,  ...,  0.7639,  0.8182,  0.7789]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if token_type_ids is None:\n",
    "    token_type_ids = torch.zeros_like(input_ids)\n",
    "token_type_embeddings = emb_model.token_type_embeddings(token_type_ids)\n",
    "token_type_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "583757c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.6579, -1.6581,  0.0088,  ...,  0.7870,  0.6889,  0.3247],\n",
       "         [ 0.2656, -0.4530, -2.8417,  ...,  0.5779, -0.6376,  1.3595],\n",
       "         [ 1.7510, -1.7458, -0.9442,  ...,  0.1413, -1.4053, -0.5503],\n",
       "         ...,\n",
       "         [ 2.6611, -2.3332, -3.5140,  ...,  1.1806,  2.0422,  2.1068],\n",
       "         [-0.2973, -1.6818, -1.0267,  ...,  1.1027, -0.3161, -0.2829],\n",
       "         [ 3.9636, -1.3088, -0.4194,  ...,  2.6655,  0.6275,  1.3564]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb + position_embeddings + tok_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "fc81f309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.9397, -0.9562, -0.0032,  ...,  0.4417,  0.3857,  0.1774],\n",
       "         [ 0.1335, -0.2884, -1.6908,  ...,  0.3169, -0.3968,  0.7758],\n",
       "         [ 1.0431, -1.0666, -0.5830,  ...,  0.0719, -0.8612, -0.3453],\n",
       "         ...,\n",
       "         [ 1.6176, -1.4190, -2.1370,  ...,  0.7174,  1.2413,  1.2806],\n",
       "         [-0.1760, -1.0018, -0.6111,  ...,  0.6591, -0.1872, -0.1674],\n",
       "         [ 2.3597, -0.8167, -0.2808,  ...,  1.5777,  0.3498,  0.7890]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddig = emb_model.LayerNorm(emb + position_embeddings + tok_emb)\n",
    "embeddig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "15697f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0441, -1.0624, -0.0035,  ...,  0.4908,  0.4285,  0.1971],\n",
       "         [ 0.1484, -0.3204, -0.0000,  ...,  0.3521, -0.4409,  0.0000],\n",
       "         [ 1.1590, -0.0000, -0.6478,  ...,  0.0000, -0.9569, -0.3837],\n",
       "         ...,\n",
       "         [ 1.7973, -1.5767, -2.3744,  ...,  0.7972,  1.3792,  1.4229],\n",
       "         [-0.1955, -1.1131, -0.6789,  ...,  0.7324, -0.2080, -0.1860],\n",
       "         [ 2.6219, -0.9074, -0.3120,  ...,  1.7529,  0.3887,  0.8767]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddig = emb_model.dropout(embeddig)\n",
    "embeddig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "d668e340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0441, -1.0624, -0.0035,  ...,  0.0000,  0.4285,  0.1971],\n",
       "         [ 0.1484, -0.0000, -1.8787,  ...,  0.3521, -0.4409,  0.8620],\n",
       "         [ 1.1590, -1.1851, -0.6478,  ...,  0.0799, -0.0000, -0.3837],\n",
       "         ...,\n",
       "         [ 1.7973, -1.5767, -2.3744,  ...,  0.7972,  1.3792,  1.4229],\n",
       "         [-0.1955, -1.1131, -0.6789,  ...,  0.7324, -0.2080, -0.1860],\n",
       "         [ 2.6219, -0.9074, -0.3120,  ...,  1.7529,  0.3887,  0.0000]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_to_emb = emb_model(input_ids)\n",
    "token_to_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e04bb60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b41045",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "507c17a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8bbe557c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length = input_ids.size(1)\n",
    "seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "18689fd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, tensor([0, 1, 2, 3, 4]))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length = input_ids.size(1) # 문장 길이\n",
    "position_ids = torch.arange(\n",
    "    seq_length, dtype=torch.long, device=input_ids.device)\n",
    "seq_length, position_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d31222fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
    "position_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4036f348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5]),\n",
       " tensor([[0, 1, 2, 3, 4],\n",
       "         [0, 1, 2, 3, 4]]))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "position_ids.shape, position_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f2716750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1813,  0.0983, -0.8991,  ..., -0.1996,  0.3542,  1.7562],\n",
       "         [-0.2457, -0.2471, -0.0510,  ..., -2.1774,  0.8956, -0.6549],\n",
       "         [-0.5985,  1.6512, -0.8037,  ...,  1.5534,  0.1627,  2.2195],\n",
       "         [-1.6036,  0.4944,  0.6576,  ...,  1.2255,  0.8074,  2.3766],\n",
       "         [ 0.2873, -0.4889, -0.5152,  ..., -0.4286,  1.9095, -0.5513]],\n",
       "\n",
       "        [[-0.1813,  0.0983, -0.8991,  ..., -0.1996,  0.3542,  1.7562],\n",
       "         [-0.2457, -0.2471, -0.0510,  ..., -2.1774,  0.8956, -0.6549],\n",
       "         [-0.5985,  1.6512, -0.8037,  ...,  1.5534,  0.1627,  2.2195],\n",
       "         [-1.6036,  0.4944,  0.6576,  ...,  1.2255,  0.8074,  2.3766],\n",
       "         [ 0.2873, -0.4889, -0.5152,  ..., -0.4286,  1.9095, -0.5513]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_embeddings = model.position_embeddings(position_ids)\n",
    "position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9f07e675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1813,  0.0983, -0.8991,  ..., -0.1996,  0.3542,  1.7562],\n",
       "         [-0.2457, -0.2471, -0.0510,  ..., -2.1774,  0.8956, -0.6549],\n",
       "         [-0.5985,  1.6512, -0.8037,  ...,  1.5534,  0.1627,  2.2195],\n",
       "         [-1.6036,  0.4944,  0.6576,  ...,  1.2255,  0.8074,  2.3766],\n",
       "         [ 0.2873, -0.4889, -0.5152,  ..., -0.4286,  1.9095, -0.5513]],\n",
       "\n",
       "        [[-0.1813,  0.0983, -0.8991,  ..., -0.1996,  0.3542,  1.7562],\n",
       "         [-0.2457, -0.2471, -0.0510,  ..., -2.1774,  0.8956, -0.6549],\n",
       "         [-0.5985,  1.6512, -0.8037,  ...,  1.5534,  0.1627,  2.2195],\n",
       "         [-1.6036,  0.4944,  0.6576,  ...,  1.2255,  0.8074,  2.3766],\n",
       "         [ 0.2873, -0.4889, -0.5152,  ..., -0.4286,  1.9095, -0.5513]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_emb = model.position_embeddings(position_ids)\n",
    "pos_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d944b9c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9225,  1.1107, -0.0111,  ..., -2.3113,  2.5776,  1.0327],\n",
       "         [ 0.4528,  1.4399,  0.8410,  ..., -4.3547,  3.6022, -1.0994],\n",
       "         [ 0.6467,  0.2667, -0.0129,  ...,  0.8187,  2.8086,  3.1922],\n",
       "         [ 1.5365, -0.7661,  1.9692,  ..., -0.5671,  2.0402,  2.0562],\n",
       "         [ 0.8985, -0.8304,  1.7427,  ..., -0.9196,  3.9093,  0.1902]],\n",
       "\n",
       "        [[ 0.6833,  0.4515,  0.7226,  ..., -2.7106,  2.2928,  3.3426],\n",
       "         [ 0.4065, -0.2599, -0.2544,  ..., -4.7195,  4.0781, -0.2462],\n",
       "         [ 0.0629,  1.6372, -0.0887,  ...,  1.0221,  1.7888,  4.6532],\n",
       "         [-0.8642,  0.3574,  2.1328,  ..., -0.3988,  2.6559,  2.8398],\n",
       "         [ 1.0267, -0.6259,  0.9600,  ..., -2.0530,  3.7580, -0.0881]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb + pos_emb + tok_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0c6accb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Embedding?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4aa2d4d",
   "metadata": {},
   "source": [
    "## 8.2.5 BertLayer 모듈\n",
    "- BertLayer는 Transformer부분에 해당.\n",
    "- 서브 네트워크로서 Self-Attention을 계산하는 BertAttention과 Self-Attention의 출력을 처리하는 전결합 층인 BertIntermediate, 그리고 Self-Attention 출력과 BertIntermediate에서 처리한 특징량을 더하는 BertOutput 세 가지로 구성됩니다.\n",
    "- BertLayer에 대한 입력은 Embedding 모듈의 출력 또는 앞단의 BertLayer에서의 출력이며 크기는 (batch_size, seq_len, hidden_size)입니다. - BertLayer구현에서 7장 Transformer와 두 가지 다른 점이 있습니다.\n",
    "- 첫째, BertIntermediate 전결합 층 뒤의 활성화 함수에 GELU함수를 사용하는 점입니다. GELU는 기본적으로 RELU와 같은 형태의 함수입니다. 입력이 0이지만 ReLU출력이 거친(매끄러운 변화가 아니라 급격환 변화) 반면 GELU는 입력 0 근처의 출력이 매끄러운 형태입니다.\n",
    "- 둘쩨, Attention이 Multi-Headed Self-Attention입니다. Transformer도 Multi-Headed Self-Attention이지만 7장에서는 이해를 돕기 위하여 단일 Self-Attention으로 구현하였습니다. Multi-Headed Self-Attention은 단순히 Self-Attention이 여러 개 있는것 뿐입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "0b7d1982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 64, 768)"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_attention_heads = config.num_attention_heads\n",
    "attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "all_head_size = num_attention_heads * attention_head_size\n",
    "num_attention_heads, attention_head_size, all_head_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4ad37569",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSelfAttention(nn.Module):\n",
    "    '''BertAttention의 Self-Attention이다'''\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(BertSelfAttention, self).__init__()\n",
    "        \n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        # num_attention_heads = 12\n",
    "        \n",
    "        self.attention_head_size = int(\n",
    "            config.hidden_size / config.num_attention_heads) # 768 / 12 = 64\n",
    "        self.all_head_size = self.num_attention_heads * \\\n",
    "            self.attention_head_size # = 'hidden_size' : 768\n",
    "        \n",
    "        # Self-Attention의 특징량을 작성하는 전결합 층\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        \n",
    "        # drop out\n",
    "        self.drop_out = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "        \n",
    "    def transpose_for_scores(self, x):\n",
    "        '''Multi-Headed Attention용으로 텐서의 형태 변환\n",
    "        [batch_size, seq_len, hidden] -> [batch_size, 12, seq_len, hidden/12]\n",
    "        '''\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "    \n",
    "    def forward(self, hidden_states, attention_mask, attention_show_fig=False):\n",
    "        '''\n",
    "        hidden_states : Embeddings 모듈 또는 앞단의 BertLayer에서의 출력\n",
    "        attention_mask : Transformer의 마스크와 같은 기능의 마스킹\n",
    "        attention_show_fig : Self-Attention의 가중치를 반환할지 플래그\n",
    "        '''\n",
    "        \n",
    "        # 입력의 전결합 층에서 특징량 변환(Multi-headed Attention 전부 한꺼번에 변환)\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "        \n",
    "        # Multi-Headed Attention용으로 텐서 형태 변환\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "        \n",
    "        # 특징량끼리 곱하여 비슷한 정도를 Attention_scores로 구한다.\n",
    "        attention_scores = torch.matmul(\n",
    "            query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / \\\n",
    "            math.sqrt(self.attention_head_size)\n",
    "        \n",
    "        # 마스크가 있는 부분에 마스크 적용\n",
    "        attention_scores = attention_scores + attention_mask\n",
    "        # 마스크는 곰셈이 아니라 덧셈이 직관적이지만 그 후에 소프트맥스로 정규화하므로\n",
    "        # 마스크된 부분은 -inf로 한다. attention_mask에는 원래 0이나 -inf가 있으므로 덧셈으로 한다.\n",
    "        \n",
    "        # Attention 정규화\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "        \n",
    "        # 드롭아웃\n",
    "        attention_probs = self.drop_out(attention_probs)\n",
    "        \n",
    "        # Attention Map을 곱한다.\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        \n",
    "        # Multi-Headed Attention의 텐서 형태를 원래대로 되돌린다.\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "        \n",
    "        # attention_show일 경우 attention_probs도 반환\n",
    "        if attention_show_fig == True:\n",
    "            return context_layer, attention_probs\n",
    "        elif attention_show_fig == False:\n",
    "            return context_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "e1f6d905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertSelfAttention(\n",
       "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (drop_out): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_self_attn = BertSelfAttention(config)\n",
    "bert_self_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "a75ba4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding shape : torch.Size([1, 7, 768])\n",
      "embedding to query shape : torch.Size([1, 7, 768])\n",
      "transpose for score shape : torch.Size([1, 12, 7, 64])\n"
     ]
    }
   ],
   "source": [
    "embedding = emb_model(input_ids)\n",
    "print('embedding shape : {}'.format(embedding.shape))\n",
    "query = bert_self_attn.query(embedding)\n",
    "print('embedding to query shape : {}'.format(query.shape))\n",
    "query_layer = bert_self_attn.transpose_for_scores(query)\n",
    "print('transpose for score shape : {}'.format(query_layer.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "8e05e7ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0441, -1.0624, -0.0035,  ...,  0.4908,  0.4285,  0.1971],\n",
       "         [ 0.1484, -0.3204, -1.8787,  ...,  0.3521, -0.4409,  0.8620],\n",
       "         [ 1.1590, -1.1851, -0.0000,  ...,  0.0799, -0.9569, -0.3837],\n",
       "         ...,\n",
       "         [ 0.0000, -1.5767, -2.3744,  ...,  0.7972,  1.3792,  1.4229],\n",
       "         [-0.1955, -1.1131, -0.6789,  ...,  0.7324, -0.0000, -0.1860],\n",
       "         [ 2.6219, -0.9074, -0.3120,  ...,  1.7529,  0.0000,  0.8767]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "b8fd5e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x size : torch.Size([1, 7, 768]), x size [:-1] : torch.Size([1, 7])\n",
      "new_x_shape : torch.Size([1, 7, 12, 64])\n",
      "*new_x_shape : 1 7 12 64\n",
      "before view x shape : torch.Size([1, 7, 768])\n",
      "after view x shape : torch.Size([1, 7, 12, 64])\n",
      "after view * permute(0, 2, 1, 3) x shape : torch.Size([1, 12, 7, 64])\n"
     ]
    }
   ],
   "source": [
    "x = embedding\n",
    "print('x size : {}, x size [:-1] : {}'.format(x.size(), x.size()[:-1]))\n",
    "new_x_shape = x.size()[:-1] + (num_attention_heads, attention_head_size)\n",
    "print('new_x_shape : {}'.format(new_x_shape))\n",
    "print('*new_x_shape :', *new_x_shape)\n",
    "print('before view x shape : {}'.format(x.shape))\n",
    "x = x.view(*new_x_shape)\n",
    "print('after view x shape : {}'.format(x.shape))\n",
    "x = x.permute(0, 2, 1, 3)\n",
    "print('after view * permute(0, 2, 1, 3) x shape : {}'.format(x.shape))\n",
    "# torch.Size([1, 12, 7, 64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "2a593663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2688.0"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5376 / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "bc21f64d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 768])"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "82a7bd52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5376"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7 * 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "7fee11f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2688])"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = embedding\n",
    "x.view(2, 2688).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "d80cea47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 12, 64])"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = embedding\n",
    "new_x_shape = x.size()[:-1] + (num_attention_heads, attention_head_size)\n",
    "x.view(*new_x_shape).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "cb174c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 7 12 64\n"
     ]
    }
   ],
   "source": [
    "print(*new_x_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "43c66c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 12])"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "f6ad0257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 12, 12, 64])"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()[:-1] + (num_attention_heads, attention_head_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "d0d61197",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 7, 64])"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed_key_layer = bert_self_attn.key(embedding)\n",
    "key_layer = bert_self_attn.transpose_for_scores(mixed_key_layer)\n",
    "key_layer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "f4121677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 7, 64])"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed_value_layer = bert_self_attn.key(embedding)\n",
    "value_layer = bert_self_attn.transpose_for_scores(mixed_value_layer)\n",
    "value_layer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "1b3e93a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attenscore shape : torch.Size([1, 12, 7, 7])\n",
      "attenscore shape : torch.Size([1, 12, 7, 7])\n",
      "attenscore shape : torch.Size([1, 12, 7, 7])\n",
      "attention_probs shape : torch.Size([1, 12, 7, 7])\n",
      "attention_probs shape : torch.Size([1, 12, 7, 7])\n",
      "context_layer shape : torch.Size([1, 12, 7, 64])\n",
      "after permute & cntignous context_layer shape : torch.Size([1, 7, 12, 64])\n",
      "new_context_layer shape : torch.Size([1, 7, 768])\n",
      "before context_layer shape : torch.Size([1, 7, 12, 64])\n",
      "after view(new_context_layer_shape) context_layer shape : torch.Size([1, 7, 768])\n"
     ]
    }
   ],
   "source": [
    "attention_scores = torch.matmul(\n",
    "            query_layer, key_layer.transpose(-1, -2))\n",
    "print('attenscore shape : {}'.format(attention_scores.shape))\n",
    "attention_scores = attention_scores / \\\n",
    "            math.sqrt(attention_head_size)\n",
    "print('attenscore shape : {}'.format(attention_scores.shape))\n",
    "attention_scores = attention_scores + extended_attention_mask\n",
    "print('attenscore shape : {}'.format(attention_scores.shape))\n",
    "attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "print('attention_probs shape : {}'.format(attention_probs.shape))\n",
    "attention_probs = bert_self_attn.drop_out(attention_probs)\n",
    "print('attention_probs shape : {}'.format(attention_probs.shape))\n",
    "context_layer = torch.matmul(attention_probs, value_layer)\n",
    "print('context_layer shape : {}'.format(context_layer.shape))\n",
    "context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "# .contiguous()는 텐서를 메모리에서 연속적인 구조로 변환해, 특정 연산에서 발생할 수 있는 문제를 방지하고 효율성을 확보하는 데 사용됨. \n",
    "# 비연속적 텐서를 다룰 때 자주 사용.\n",
    "print('after permute & cntignous context_layer shape : {}'.format(context_layer.shape))\n",
    "new_context_layer_shape = context_layer.size()[:-2] + (all_head_size,)\n",
    "print('new_context_layer shape : {}'.format(new_context_layer_shape))\n",
    "print('before context_layer shape : {}'.format(context_layer.shape))\n",
    "context_layer = context_layer.view(*new_context_layer_shape)\n",
    "print('after view(new_context_layer_shape) context_layer shape : {}'.format(context_layer.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb31291e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c830cbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSelfOutput(nn.Module):\n",
    "    '''BertSelfAttention의 출력을 처리하는 전결합 층이다'''\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(BertSelfOutput, self).__init__()\n",
    "        \n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        # 'hidden_dropout_prob': 0.1\n",
    "        \n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        '''\n",
    "        hidden_stats : BertSelfAttention의 출력 텐서\n",
    "        input_tensor : Embeddings 모듈 또는 앞단의 BertLayer에서의 출력\n",
    "        '''\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "a43abd2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertSelfOutput(\n",
       "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (LayerNorm): BertLayerNorm()\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_self_output = BertSelfOutput(config)\n",
    "bert_self_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "9bff95a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('attention_probs shape : {}'.format(attention_probs.shape))\n",
    "context_layer = torch.matmul(attention_probs, value_layer)\n",
    "# print('context_layer shape : {}'.format(context_layer.shape))\n",
    "context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "# .contiguous()는 텐서를 메모리에서 연속적인 구조로 변환해, 특정 연산에서 발생할 수 있는 문제를 방지하고 효율성을 확보하는 데 사용됨. \n",
    "# 비연속적 텐서를 다룰 때 자주 사용.\n",
    "# print('after permute & cntignous context_layer shape : {}'.format(context_layer.shape))\n",
    "new_context_layer_shape = context_layer.size()[:-2] + (all_head_size,)\n",
    "# print('new_context_layer shape : {}'.format(new_context_layer_shape))\n",
    "# print('before context_layer shape : {}'.format(context_layer.shape))\n",
    "context_layer = context_layer.view(*new_context_layer_shape)\n",
    "\n",
    "hiddenstates = bert_self_output(context_layer, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "2d5440a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_layer shape : torch.Size([1, 7, 768])\n",
      "context_layer shape : torch.Size([1, 7, 768])\n",
      "context_layer shape : torch.Size([1, 7, 768])\n",
      "context_layer shape : torch.Size([1, 7, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0043, -0.7704, -0.2196,  ...,  0.3540,  0.6049, -0.1678],\n",
       "         [ 0.0584, -0.1665, -2.0488,  ...,  0.2063, -0.2671,  0.4716],\n",
       "         [ 1.0142, -0.9420, -0.2485,  ...,  0.0330, -0.6917, -0.6882],\n",
       "         ...,\n",
       "         [-0.0578, -1.4090, -2.4367,  ...,  0.6246,  1.3533,  0.9206],\n",
       "         [-0.2972, -0.8860, -0.8465,  ...,  0.5522,  0.2657, -0.4653],\n",
       "         [ 2.3881, -0.6177, -0.6395,  ...,  1.5153,  0.2489,  0.4453]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_layer = torch.matmul(attention_probs, value_layer)\n",
    "# print('context_layer shape : {}'.format(context_layer.shape))\n",
    "context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "# .contiguous()는 텐서를 메모리에서 연속적인 구조로 변환해, 특정 연산에서 발생할 수 있는 문제를 방지하고 효율성을 확보하는 데 사용됨. \n",
    "# 비연속적 텐서를 다룰 때 자주 사용.\n",
    "# print('after permute & cntignous context_layer shape : {}'.format(context_layer.shape))\n",
    "new_context_layer_shape = context_layer.size()[:-2] + (all_head_size,)\n",
    "# print('new_context_layer shape : {}'.format(new_context_layer_shape))\n",
    "# print('before context_layer shape : {}'.format(context_layer.shape))\n",
    "context_layer = context_layer.view(*new_context_layer_shape)\n",
    "# print('after view(new_context_layer_shape) context_layer shape : {}'.format(context_layer.shape))\n",
    "print('context_layer shape : {}'.format(context_layer.shape))\n",
    "context_layer = bert_self_output.dense(context_layer)\n",
    "print('context_layer shape : {}'.format(context_layer.shape))\n",
    "context_layer = bert_self_output.dropout(context_layer)\n",
    "print('context_layer shape : {}'.format(context_layer.shape))\n",
    "context_layer = bert_self_output.LayerNorm(context_layer + embedding)\n",
    "print('context_layer shape : {}'.format(context_layer.shape))\n",
    "context_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "a60252d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0043, -0.7704, -0.2196,  ...,  0.3540,  0.6049, -0.1678],\n",
       "         [ 0.0584, -0.1665, -2.0488,  ...,  0.2063, -0.2671,  0.4716],\n",
       "         [ 1.0142, -0.9420, -0.2485,  ...,  0.0330, -0.6917, -0.6882],\n",
       "         ...,\n",
       "         [-0.0578, -1.4090, -2.4367,  ...,  0.6246,  1.3533,  0.9206],\n",
       "         [-0.2972, -0.8860, -0.8465,  ...,  0.5522,  0.2657, -0.4653],\n",
       "         [ 2.3881, -0.6177, -0.6395,  ...,  1.5153,  0.2489,  0.4453]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "1a0143cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertAttention(nn.Module):\n",
    "    '''BertLayer 모듈의 Self-Attention 부분'''\n",
    "    def __init__(self, config):\n",
    "        super(BertAttention, self).__init__()\n",
    "        self.selfattn = BertSelfAttention(config)\n",
    "        self.output = BertSelfOutput(config)\n",
    "        \n",
    "    def forward(self, input_tensor, attention_mask, attention_show_fig=False):\n",
    "        '''\n",
    "        input_tensor : Embeddings 모듈 또는 앞단의 BertLayer에서의 출력\n",
    "        attention_mask : Transformer의 마스크와 같은 기능의 마스킹\n",
    "        attention_show_fig : Self-Attention의 가중치를 반환할지의 플래그\n",
    "        '''\n",
    "        if attention_show_fig == True:\n",
    "            '''attention_show일 경우 attention_probs도 반환한다.'''\n",
    "            self_output, attention_probs = self.selfattn(input_tensor, attention_mask, attention_show_fig)\n",
    "            attention_output = self.output(self_output, input_tensor)\n",
    "            return attention_output, attention_probs\n",
    "        \n",
    "        elif attention_show_fig == False:\n",
    "            self_output = self.selfattn(input_tensor, attention_mask, attention_show_fig)\n",
    "            attention_output = self.output(self_output, input_tensor)\n",
    "            return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "a2f44cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    '''Gaussian Error Linear Unit라는 활성화 함수이다\n",
    "    ReLU가 0으로 거칠고 불연속적이므로 연속적으로 매끄럽게 한 셩태의 ReLU이다\n",
    "    '''\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
    "\n",
    "class BertIntermediate(nn.Module):\n",
    "    '''BERT의 TransformerBlock 모듈 FeedForward'''\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(BertIntermediate, self).__init__()\n",
    "        \n",
    "        # 전결합 층 : 'hidden_size': 768, 'intermediate_size': 3072\n",
    "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        # 활성화 함수\n",
    "        self.intermediate_act_fn = gelu\n",
    "        \n",
    "    def forward(self, hidden_states):\n",
    "        '''\n",
    "        hidden_states : BertAttention의 출력\n",
    "        '''\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.intermediate_act_fn(hidden_states) # GELU에 의한 활성화\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "9e2bbeb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertIntermediate(\n",
       "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 558,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_intermediate = BertIntermediate(config)\n",
    "bert_intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "74b1826d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1538, -0.1689,  0.5047,  ...,  0.0127, -0.1653, -0.0041],\n",
       "         [-0.1561,  0.2039, -0.1468,  ..., -0.1289,  0.0439, -0.0933],\n",
       "         [-0.0907, -0.0419,  0.5353,  ..., -0.1095,  0.3296, -0.1376],\n",
       "         ...,\n",
       "         [-0.0516, -0.1687,  0.0517,  ..., -0.1412,  0.0033,  0.1908],\n",
       "         [-0.1659, -0.0535,  0.4830,  ..., -0.1675,  0.7587,  0.0525],\n",
       "         [-0.1671, -0.1440,  0.2304,  ..., -0.1108, -0.0572,  0.3275]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_intermediate(hiddenstates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "8285e0e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 3072])"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hiddenstates = bert_self_output(context_layer, embedding)\n",
    "hiddenstates = bert_intermediate.dense(hiddenstates)\n",
    "hiddenstates = bert_intermediate.intermediate_act_fn(hiddenstates)\n",
    "hiddenstates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "2435917c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertOutput(nn.Module):\n",
    "    '''BERT의 TransformerBlock 모듈 FeedForward'''\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(BertOutput, self).__init__()\n",
    "        \n",
    "        # 전결합 층 : 'intermediate_size': 3072, 'hidden_size': 768\n",
    "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        \n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
    "        \n",
    "        # 'hidden_dropout_prob': 0.1\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        \n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        '''\n",
    "        hidden_states : BertIntermediate 출력 텐서\n",
    "        input_tensor : BertAttention 출력 텐서\n",
    "        '''\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "8739a96a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertOutput(\n",
       "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "  (LayerNorm): BertLayerNorm()\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_output = BertOutput(config)\n",
    "bert_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "c8b4b7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_states shape : torch.Size([1, 7, 3072])\n",
      "after BertOutput hidden_states shape : torch.Size([1, 7, 768])\n"
     ]
    }
   ],
   "source": [
    "hiddenstates = bert_self_output(context_layer, embedding)\n",
    "hiddenstates = bert_intermediate.dense(hiddenstates)\n",
    "hiddenstates = bert_intermediate.intermediate_act_fn(hiddenstates)\n",
    "\n",
    "print('hidden_states shape : {}'.format(hiddenstates.shape))\n",
    "hiddenstates = bert_output(hiddenstates, context_layer)\n",
    "print('after BertOutput hidden_states shape : {}'.format(hiddenstates.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "b89d6380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before dense layer hiddenstates shape : torch.Size([1, 7, 3072])\n",
      "after dense layer hiddenstates shape : torch.Size([1, 7, 768])\n",
      "after dropout layer hiddenstates shape : torch.Size([1, 7, 768])\n",
      "after LayerNorm layer hiddenstates shape : torch.Size([1, 7, 768])\n"
     ]
    }
   ],
   "source": [
    "hiddenstates = bert_self_output(context_layer, embedding)\n",
    "hiddenstates = bert_intermediate.dense(hiddenstates)\n",
    "hiddenstates = bert_intermediate.intermediate_act_fn(hiddenstates)\n",
    "print('before dense layer hiddenstates shape : {}'.format(hiddenstates.shape))\n",
    "hiddenstates = bert_output.dense(hiddenstates)\n",
    "print('after dense layer hiddenstates shape : {}'.format(hiddenstates.shape))\n",
    "hiddenstates = bert_output.dropout(hiddenstates)\n",
    "print('after dropout layer hiddenstates shape : {}'.format(hiddenstates.shape))\n",
    "hiddenstates = bert_output.LayerNorm(hiddenstates + embedding)\n",
    "print('after LayerNorm layer hiddenstates shape : {}'.format(hiddenstates.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "b43cc274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 768])"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_encoder_layers = [hiddenstates]\n",
    "all_encoder_layers[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5021966",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d845708c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(nn.Module):\n",
    "    '''BERT의 BertLayer모듈이다. Transformer가 된다.'''\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(BertLayer, self).__init__()\n",
    "        \n",
    "        # Self-Attention 부분\n",
    "        self.attention = BertAttention(config)\n",
    "        \n",
    "        # Self-Attention의 출력을 처리하는 전결합 층\n",
    "        self.intermediate = BertIntermediate(config)\n",
    "        \n",
    "        # Self-Attention에 의한 특징량과 BertLayer에 원래의 입력을 더하는 층\n",
    "        self.output = BertOutput(config)\n",
    "        \n",
    "    def forward(self, hidden_states, attention_mask, attention_show_fig=False):\n",
    "        '''\n",
    "        hidden_states : Embedder 모듈의 출력 텐서 [batch_size, seq_len, hidden_size]\n",
    "        attention_mask : Transformer의 마스크와 같은 기능의 마스킹\n",
    "        attention_show_fig : Self-Attention의 가중치를 반환할지의 플래그\n",
    "        '''\n",
    "        if attention_show_fig == True:\n",
    "            '''attention_show일 경우 attention_probs도 반환한다.'''\n",
    "            attention_output, attention_probs = self.attention(hidden_states, \n",
    "                                                               attention_mask, \n",
    "                                                               attention_show_fig)\n",
    "            \n",
    "            intermediate_output = self.intermediate(attention_output)\n",
    "            \n",
    "            layer_output = self.output(intermediate_output, attention_output)\n",
    "            \n",
    "            return layer_output, attention_probs\n",
    "        \n",
    "        elif attention_show_fig == False:\n",
    "            attention_output = self.attention(hidden_states, \n",
    "                                              attention_mask, \n",
    "                                              attention_show_fig)\n",
    "            \n",
    "            intermediate_output = self.intermediate(attention_output)\n",
    "            \n",
    "            layer_output = self.output(intermediate_output, attention_output)\n",
    "            \n",
    "            return layer_output # [batch_size, seq_length, hidden_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f976ed8",
   "metadata": {},
   "source": [
    "## 8.2.6 BertLayer 모듈의 반복 부분\n",
    "- BERT_Base에서는 BertLayer 모듈(Transformer)을 12회 반복.\n",
    "- 이들을 묶어서 BertEncoder클래스로 만듭니다.\n",
    "- 단순히 BertLayer 12개를 nn.ModuleList에 기재하여 순전파.\n",
    "- 순전파 함수 forward의 인수\n",
    "    - output_all_encoded_layer인수는 반환 값으로 BertLayer에서 출력된 특징량을 12단만큼 모두 반환할지 아니면 12단 최종 층의 특징량만 반환할지 여부를 지정하는 변수.\n",
    "    - 12단의 Transformer 중간에 단어 벡터가 어떻게 변해가는지 확인하고 싶을 때 output_all_encoded_layers인수를 True로 하여 12단 만큼의 단어 벡터를 꺼낼 수 있습니다.\n",
    "    - 단순히 12단 출력만을 사용하여 자연어 처리를 작업하는 경우 False로 하여 최종 BertLayer 모듈 출력만 BertEncoder에서 출력시킨 후 사용\n",
    "    - attention_show_fig인수는 BertLayer 모듈에서 사용했던 변수와 동일\n",
    "    - Self-Attention의 가중치를 출력할지 여부를 지정합니다. BERT_Base의 Attention은 각 층이 12개인 Multi-headed Self-Attention입니다.\n",
    "    - BertEncoder에서 attention_show_fig인수를 True로 한 경우에는 BertLayer 모듈 중 12단 끝에 있는 BertLayer 모듈에서 12개의 Multi-Headed Self-Attention가중치를 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1ea6fcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BertLayer 모듈의 반복 부분이다.\n",
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        '''BertLayer 모듈의 반복 부분'''\n",
    "        super(BertEncoder, self).__init__()\n",
    "        \n",
    "        # config.num_hidden_layers의 값, 즉 12개의 BertLayer 모듈을 만든다.\n",
    "        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        \n",
    "    def forward(self, hidden_states, attention_mask, output_all_encoded_layers=True, attention_show_fig=False):\n",
    "        '''\n",
    "        hidden_states : Embeddings 모듈 출력\n",
    "        attention_mask : Transformer의 마스크와 동일한 기능의 마스킹\n",
    "        output_all_encoded_layers : 반환 값을 전체 TransformerBlock 모듈의 출력으로 할지 마지막 층만으로 한정할지의 플래그\n",
    "        attention_show_fig : Self-Attention의 가중치를 반환할지의 플래그\n",
    "        '''\n",
    "        \n",
    "        # 반환 값으로 사용할 리스트\n",
    "        all_encoder_layers = []\n",
    "        \n",
    "        # BertLayer 모듈의 처리 반복\n",
    "        for layer_module in self.layer:\n",
    "            \n",
    "            if attention_show_fig == True:\n",
    "                '''attention_show의 경우 attention_probs도 반환'''\n",
    "                hidden_states, attention_probs = layer_module(\n",
    "                    hidden_states, attention_mask, attention_show_fig)\n",
    "            elif attention_show_fig == False:\n",
    "                hidden_states = layer_module(\n",
    "                    hidden_states, attention_mask, attention_show_fig)\n",
    "                \n",
    "            # 반환 값으로 BertLayer에서 출력된 특징량만을 사용할 경우의 처리\n",
    "            if output_all_encoded_layers:\n",
    "                all_encoder_layers.append(hidden_states)\n",
    "                    \n",
    "        # 반환 값으로 마지막 BertLayer에서 출력된 특징량만을 사용할 경우의 처리\n",
    "        if not output_all_encoded_layers:\n",
    "            all_encoder_layers.append(hidden_states)\n",
    "            \n",
    "        # attention_show의 경우 attention_probs(마지막 12단)도 반환한다.\n",
    "        if attention_show_fig == True:\n",
    "            return all_encoder_layers, attention_probs\n",
    "        elif attention_show_fig == False:\n",
    "            return all_encoder_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af4ec36",
   "metadata": {},
   "source": [
    "## 8.2.7 BertPooler 모듈\n",
    "- BertPooler 모듈은 BertEncoder출력에서 입력 문장의 첫 번째 단어인 [CLS]부분의 특징량 텐서(1 x 768차원)을 꺼내 전결합 층을 사용한 후 특징량을 변환하는 모듈입니다.\n",
    "- 전결합 층 뒤에 활성화 함수 Tanh을 사용하고 출력을 1에서 -1까지 범위로 합니다. 출력 텐서의 크기는 (batch_size, hidden_size)입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "41b57475",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPooler(nn.Module):\n",
    "    '''입력 문장의 첫 번째 단어 [cls]의 특징량을 반환하고 유지하기 위한 모듈'''\n",
    "    def __init__(self, config):\n",
    "        super(BertPooler, self).__init__()\n",
    "        \n",
    "        # 전결합 층, 'hidden_size':768\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "        \n",
    "    def forward(self, hidden_states):\n",
    "        # 첫 번째 단어의 특징량 취득\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        \n",
    "        # 전결합 층에서 특징량 변환\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        \n",
    "        # 활성화 함수 Tanh을 계산\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        \n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "9602444b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertPooler(\n",
       "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (activation): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_pooler = BertPooler(config)\n",
    "bert_pooler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "ca1f42c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 768])"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_encoder_layers[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "8af9c256",
   "metadata": {},
   "outputs": [],
   "source": [
    "hiddenstates = all_encoder_layers[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "a52c7479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.5811, -1.1276, -0.4414,  ...,  0.0488,  0.3227,  0.2172],\n",
       "         [-0.2929, -0.3067, -1.8828,  ...,  0.2411, -0.4191,  0.7964],\n",
       "         [ 0.6937, -0.9494,  0.0315,  ...,  0.0509, -0.9975, -0.3903],\n",
       "         ...,\n",
       "         [-0.4095, -1.4744, -2.4974,  ...,  0.8933,  1.2169,  1.3756],\n",
       "         [-0.7637, -1.3061, -0.8962,  ...,  0.3963, -0.0338, -0.0782],\n",
       "         [ 2.0688, -0.9636, -0.4475,  ...,  1.7824, -0.0744,  0.7009]],\n",
       "        grad_fn=<SelectBackward0>),\n",
       " tensor([[ 5.8108e-01, -1.1276e+00, -4.4136e-01, -1.3467e-01, -4.2354e-01,\n",
       "           4.9744e-01, -2.2314e-02,  1.1697e+00, -2.2545e+00,  3.7475e-01,\n",
       "          -1.3885e-01, -2.2705e+00, -1.5102e+00, -1.1789e+00,  9.2932e-01,\n",
       "          -8.5247e-01,  2.1257e-01,  2.4318e+00,  2.3331e-01,  1.7897e-01,\n",
       "          -4.8815e-01, -7.4696e-02,  1.7073e+00, -2.5969e-01,  2.7233e-01,\n",
       "           1.7215e+00,  5.9070e-01, -7.5006e-01, -1.4659e+00, -1.3294e+00,\n",
       "          -2.0931e-01,  6.3464e-01, -2.6210e+00, -4.0849e-02,  5.8336e-01,\n",
       "          -2.6275e-01, -1.1264e+00, -8.7615e-01,  1.5326e-01, -6.8099e-01,\n",
       "           5.1558e-01,  2.0606e+00,  3.3629e-01,  1.8070e+00,  4.9634e-01,\n",
       "          -1.4505e+00, -1.6145e+00, -7.9379e-01, -2.6868e+00, -1.4258e-01,\n",
       "          -1.1626e+00, -3.4142e-02,  5.3409e-01,  4.2922e-01,  1.0514e+00,\n",
       "          -1.7964e+00,  1.3269e-01,  8.6070e-01, -2.1795e+00, -6.4535e-01,\n",
       "          -1.8942e-01, -2.3199e-02,  2.3422e-01,  3.0891e-01, -2.3598e-01,\n",
       "          -6.0606e-01,  7.4090e-01,  1.1549e+00,  3.6986e-01,  1.7123e-01,\n",
       "           1.3663e-01,  1.5744e+00,  6.2250e-01,  6.9935e-01, -2.0815e+00,\n",
       "           1.6785e+00,  1.1061e+00,  2.4593e+00,  1.7472e-01, -1.3907e+00,\n",
       "           2.4530e-01, -2.8133e-01, -2.8724e-01, -1.0966e+00, -9.1408e-01,\n",
       "           1.6804e+00,  9.7667e-02,  2.0890e-01, -2.2875e+00,  9.9780e-01,\n",
       "          -6.6148e-01, -8.1205e-02, -1.1574e+00,  6.4183e-01, -4.2967e-02,\n",
       "           1.8943e+00,  1.2007e+00, -9.7990e-01,  2.2712e+00, -5.8846e-01,\n",
       "          -3.5786e-01,  5.0253e-01,  6.3225e-01,  9.2206e-02,  9.5070e-01,\n",
       "           1.2925e+00,  1.1776e+00,  6.1739e-01,  8.0821e-01,  5.4582e-01,\n",
       "          -2.5822e-01,  8.3927e-01,  1.1628e+00, -5.6886e-02,  4.7766e-01,\n",
       "          -2.7231e-01, -5.6628e-01, -5.2864e-01,  7.0662e-01, -1.8534e-01,\n",
       "          -1.0916e+00, -1.4548e+00,  1.1164e+00, -1.7926e+00,  6.3998e-02,\n",
       "           6.8715e-01, -8.5554e-01, -1.1368e+00, -1.5915e+00, -2.8935e-01,\n",
       "           7.0634e-01, -6.4520e-01, -3.4506e-01,  1.5381e-01, -2.0174e+00,\n",
       "          -1.6466e+00, -5.2284e-01, -4.6790e-01, -6.6592e-02, -1.1137e+00,\n",
       "           2.8900e-01, -6.7358e-01, -2.0649e+00,  4.4766e-01,  1.7450e-01,\n",
       "           1.8003e-02, -4.6796e-02, -2.3208e+00, -1.4129e-01,  1.1512e+00,\n",
       "           8.2480e-01,  7.7648e-01, -1.1409e+00, -4.0913e-02,  2.2127e+00,\n",
       "           1.4979e+00, -8.4421e-01,  9.7480e-01,  4.4715e-02,  1.2635e+00,\n",
       "          -8.2242e-01, -3.7209e-01,  1.5871e-01, -6.0906e-01, -1.9372e-01,\n",
       "          -4.3538e-01, -1.2848e+00,  1.3737e+00, -9.6832e-02,  1.7439e+00,\n",
       "          -7.7329e-01,  8.2887e-01, -5.3548e-02,  9.0608e-01, -5.9055e-01,\n",
       "          -1.1790e+00, -5.0448e-01, -2.6963e-01,  1.8026e+00, -2.7324e-01,\n",
       "           1.9160e-01, -5.8847e-01, -1.0223e-01, -9.4905e-01, -5.4433e-01,\n",
       "          -6.1609e-01, -5.5120e-01, -1.6666e-01,  1.7685e+00, -5.7203e-01,\n",
       "           7.8612e-01, -1.3534e+00,  7.3530e-01,  7.6827e-01,  2.7416e-01,\n",
       "          -3.1037e-01, -6.0404e-01,  2.8146e+00, -2.1132e-01,  4.8261e-01,\n",
       "          -3.3286e-01, -1.3094e-01, -1.3537e-01, -7.0382e-01, -4.6314e-01,\n",
       "           1.1392e-01, -8.4364e-01, -1.6295e-01, -7.6596e-01,  1.9241e+00,\n",
       "          -3.1342e-01, -5.3845e-01,  1.1214e-01,  1.2089e+00,  6.4094e-02,\n",
       "          -1.6866e-01,  4.2722e-01, -1.2647e+00,  5.8231e-01,  1.1094e+00,\n",
       "           1.2088e-02,  1.1459e-01,  2.5076e-01, -9.8781e-01, -3.9839e-01,\n",
       "          -1.6789e-01, -1.5605e+00, -6.7990e-01, -9.0537e-01, -1.9652e+00,\n",
       "           1.5995e+00, -7.2553e-02,  2.0478e+00,  1.5808e+00, -6.6198e-01,\n",
       "           1.4827e+00, -7.4572e-02, -9.7730e-01,  3.8088e-01, -2.8234e-01,\n",
       "           3.4910e-02, -6.7417e-01,  1.7926e+00, -7.9647e-01, -1.7952e+00,\n",
       "          -1.0394e+00, -4.0596e-01,  6.3481e-02, -3.1751e-01, -6.2670e-01,\n",
       "          -4.4591e-01, -3.9564e-01, -2.3735e-01, -1.2382e-03,  1.3363e+00,\n",
       "          -6.4577e-01,  1.7725e+00, -1.2688e+00, -4.9326e-01, -2.6038e+00,\n",
       "           1.3023e-01,  5.6577e-01,  1.1148e+00,  8.9945e-01, -1.1376e+00,\n",
       "           2.4617e+00,  7.5124e-01,  5.9473e-01,  1.5069e-01,  2.5187e-01,\n",
       "           5.7853e-02, -5.3285e-01, -1.3375e+00, -5.3799e-01,  2.9179e-01,\n",
       "           3.0723e-01, -2.0709e-01, -5.3009e-01, -1.9620e+00, -2.0616e+00,\n",
       "          -3.1117e+00, -1.9329e-01, -6.0808e-01, -2.6330e+00,  1.2085e+00,\n",
       "           6.7516e-01, -6.9287e-01, -5.9821e-02, -1.2990e+00, -1.2300e-02,\n",
       "           7.0627e-01,  2.3527e+00,  7.4037e-02, -1.1004e+00, -6.1544e-01,\n",
       "          -1.8103e-01,  4.0621e-01,  1.4811e+00, -8.7178e-02,  3.8028e-01,\n",
       "          -3.4810e-01,  1.1655e+00, -4.4701e-01, -8.0062e-01, -1.2148e-01,\n",
       "           1.8475e-01, -1.2117e+00, -9.9345e-01,  7.2453e-01,  1.7270e+00,\n",
       "           1.5025e+00, -1.4180e+00, -3.8932e-01,  5.8078e-01, -2.6970e-01,\n",
       "          -2.7421e-01, -8.4519e-01,  2.7616e-01,  5.4396e-01,  2.3860e-01,\n",
       "           3.7103e-01,  1.1281e-01, -1.6168e-01, -1.0032e+00, -1.4318e+00,\n",
       "          -5.4311e-01,  1.3250e+00, -6.9530e-01,  4.2117e-02,  3.6952e-01,\n",
       "           1.9918e+00, -1.1022e+00,  1.6490e+00,  6.5977e-01, -9.5754e-02,\n",
       "           3.6147e-01,  2.5637e-01, -2.4201e-01, -9.9196e-01, -1.3148e+00,\n",
       "           1.1070e+00,  1.6948e+00,  2.6605e-01,  1.6086e+00, -4.7712e-01,\n",
       "          -1.0734e+00,  1.8940e+00,  3.9491e-01,  1.2767e+00, -7.3231e-01,\n",
       "          -3.3011e-01,  1.9952e-02,  1.6465e-01,  1.8988e+00, -7.9707e-01,\n",
       "           4.9139e-01,  3.6643e-01, -2.1769e+00,  7.4701e-01, -3.7217e-01,\n",
       "           1.0801e+00,  2.6924e-01,  8.5337e-01,  2.2289e+00,  7.7574e-01,\n",
       "          -1.7443e-01,  8.6824e-02,  1.1896e+00,  6.4744e-01, -1.0021e-01,\n",
       "          -1.2661e-01,  2.7847e-01, -7.3227e-02,  3.1942e-01, -1.9431e+00,\n",
       "           6.5766e-01, -1.6437e-01, -3.9705e-01, -7.6005e-01, -1.1601e+00,\n",
       "          -1.2044e+00, -8.1046e-01, -2.1526e-01, -1.0607e-01, -1.9158e+00,\n",
       "           5.1534e-01, -1.2962e+00, -2.0858e-01,  3.4910e-02, -2.6873e-01,\n",
       "          -2.3655e-01, -4.4558e-01,  1.0512e+00, -2.3102e-01,  4.4685e-01,\n",
       "           1.3383e+00, -2.2706e+00, -9.1275e-01,  8.0639e-01, -5.6282e-01,\n",
       "           8.3030e-01,  4.5974e-02, -4.6899e-02, -1.4099e+00, -7.1838e-01,\n",
       "           4.1105e-01, -2.3241e-01, -2.1116e+00,  1.6913e+00, -7.7196e-01,\n",
       "          -3.8232e-01,  1.2230e+00,  1.2158e+00,  4.3658e-02, -1.1716e+00,\n",
       "           3.7817e-01,  8.3807e-02, -1.0084e+00,  1.2614e+00,  4.3454e-01,\n",
       "          -1.6499e+00,  2.5406e-01, -1.9251e+00,  9.1832e-01,  2.4274e-01,\n",
       "           6.6363e-01, -1.6470e-01,  1.6466e+00,  1.7066e+00, -1.5112e+00,\n",
       "          -4.2914e-01, -3.7979e-01,  1.4537e+00,  4.4596e-01,  5.9999e-01,\n",
       "           2.3570e-01, -1.2430e+00, -1.1279e+00, -3.0850e-02, -1.2021e+00,\n",
       "          -5.5620e-01,  6.7867e-01,  1.0269e+00,  8.7784e-01,  3.4910e-02,\n",
       "           4.9579e-01,  2.0731e-01,  2.4726e+00, -4.8698e-01,  2.7334e+00,\n",
       "          -4.5288e-03, -7.2957e-01, -3.9583e-01,  9.0837e-02, -1.5471e+00,\n",
       "          -1.0376e+00, -2.4556e-01,  6.5032e-01,  1.1045e+00,  6.4193e-01,\n",
       "           2.4792e-01, -2.1646e-02, -6.5853e-01, -2.1166e+00, -1.4639e+00,\n",
       "           4.4991e-03,  1.2198e+00, -1.3730e+00, -6.0357e-01, -1.9537e-01,\n",
       "           2.2730e+00, -6.7068e-01,  8.6776e-02, -6.0771e-01, -1.9855e+00,\n",
       "          -9.0433e-01,  1.8304e-01,  1.4171e+00, -1.2955e+00, -2.9224e-01,\n",
       "           6.6689e-01, -1.9258e-02,  4.4910e-01,  1.9158e+00,  3.6429e-01,\n",
       "           3.7856e-01, -1.4434e+00,  2.2693e-01, -4.1397e-01,  1.8268e+00,\n",
       "          -6.7575e-01,  9.6541e-01,  2.6886e-02,  1.2466e+00, -1.2521e-01,\n",
       "          -8.7980e-01,  5.4792e-01,  1.8302e-01,  2.7075e-01,  1.9928e+00,\n",
       "           3.0034e-01, -4.1839e-01, -1.4242e+00, -1.1528e+00,  1.9070e-01,\n",
       "          -4.3589e-01, -8.1684e-01,  3.1978e-02,  3.5367e-01, -3.0868e-02,\n",
       "          -7.8196e-02,  2.1563e-01,  2.3685e-01,  1.8454e+00,  6.7363e-02,\n",
       "           5.9046e-02, -1.1597e+00,  6.7602e-01,  1.9987e-01, -7.2322e-01,\n",
       "           7.9517e-01, -3.9756e-01, -1.1231e+00,  1.4514e+00, -8.7682e-01,\n",
       "          -1.5307e-02,  3.4910e-02,  3.5124e-02, -4.5273e-01, -7.7597e-01,\n",
       "          -1.3909e-01, -6.9029e-02, -2.4293e+00,  6.7731e-02, -5.3068e-01,\n",
       "          -1.8857e+00, -1.3247e+00, -1.7981e+00, -2.6853e-01, -1.6900e+00,\n",
       "           1.3359e+00,  2.6614e-01, -8.1545e-02,  2.2483e+00,  8.7473e-01,\n",
       "          -7.9084e-02,  3.4771e-01,  6.8899e-01,  6.2380e-01,  4.3361e-02,\n",
       "          -1.3613e+00, -1.3299e+00,  1.5812e+00,  3.0478e-01,  3.8927e-01,\n",
       "          -1.3524e+00, -1.1824e+00, -3.4256e-01, -1.6042e-03,  5.6036e-01,\n",
       "          -1.7064e+00,  4.3421e-03,  7.2044e-02, -2.6880e+00, -1.6446e-01,\n",
       "          -5.2963e-01, -1.0423e+00,  3.2659e-01,  1.1113e-01, -2.2598e-01,\n",
       "           4.2684e-01,  6.7522e-01,  5.4577e-02, -5.4624e-01,  1.0390e+00,\n",
       "           1.6078e+00,  1.0912e+00, -9.3811e-01,  6.1683e-01, -7.2068e-01,\n",
       "           2.1118e+00,  7.2603e-02, -1.1178e+00,  2.6359e+00,  1.3310e+00,\n",
       "           1.9167e+00, -1.0192e+00, -2.5712e-02, -2.5975e-01, -9.6933e-02,\n",
       "           1.7931e+00, -9.2965e-01,  1.2905e-03, -2.3867e-01,  1.0354e+00,\n",
       "           6.2552e-02,  1.4205e-01, -2.5211e-01,  1.1505e+00,  7.7330e-01,\n",
       "          -1.0851e+00,  1.1327e-01, -1.0928e-01,  1.6569e+00, -4.9997e-01,\n",
       "          -2.4851e-01, -7.3083e-01, -2.1631e-01, -2.3629e-01, -1.5517e+00,\n",
       "           1.0387e+00, -1.4596e-02, -1.3104e+00, -5.9218e-01, -5.7556e-01,\n",
       "           1.2641e+00, -5.6460e-01, -8.2352e-01, -5.2551e-01, -1.5961e-01,\n",
       "           8.6771e-01,  3.4910e-02, -2.8501e-01,  7.2657e-01,  1.0423e+00,\n",
       "           3.9047e-01,  1.0787e+00,  4.9955e-01,  9.1501e-01,  1.0248e+00,\n",
       "          -2.2316e+00,  3.4910e-02,  3.7317e-01,  1.0334e+00, -6.5196e-01,\n",
       "          -6.5044e-02,  1.6497e-01, -2.0534e-01,  3.1992e-01,  1.1026e+00,\n",
       "           1.5785e-01,  1.1215e-01,  1.1704e+00,  2.3427e-03,  4.2232e-01,\n",
       "          -8.4184e-01,  1.0613e+00, -5.2995e-01,  3.0084e-01, -1.0889e+00,\n",
       "          -7.7953e-01, -1.5838e+00, -2.6376e-01,  1.0977e+00, -4.5910e-01,\n",
       "          -3.0118e-01, -1.7055e-01,  1.5819e+00, -1.1500e+00,  1.6270e-01,\n",
       "           1.2539e+00,  1.8053e-01,  3.9024e-02, -1.7224e+00,  1.6890e-01,\n",
       "          -1.4122e+00, -9.0963e-01, -1.5677e-01, -1.5137e-01,  3.4910e-02,\n",
       "          -1.8232e-01,  1.6864e-01, -1.7488e-01, -1.7169e+00, -6.4795e-01,\n",
       "          -2.2904e+00, -5.0820e-02,  7.8954e-01, -9.7749e-01,  3.4910e-02,\n",
       "           1.9813e-01,  2.4604e+00,  1.7723e+00, -1.0957e-01, -6.2565e-01,\n",
       "          -6.5823e-01,  3.1504e-02,  6.0887e-01,  3.4910e-02,  9.8200e-01,\n",
       "          -3.4547e-02,  1.5350e+00, -1.8713e-01,  7.2346e-01, -1.5386e-01,\n",
       "           2.1736e-01, -1.0564e+00,  4.4638e-02, -1.3101e+00,  1.1905e+00,\n",
       "           1.3404e+00,  1.9989e+00, -5.7472e-01, -7.2098e-01,  1.1899e+00,\n",
       "           1.8133e-02,  1.0252e+00, -1.1643e-01, -7.5824e-01, -4.8023e-01,\n",
       "           2.7348e+00,  1.0610e+00,  7.9966e-01, -9.2621e-01,  8.1929e-01,\n",
       "           2.2263e+00,  8.3121e-01,  4.6690e-01, -1.0584e-01,  6.4456e-01,\n",
       "           1.0330e+00, -4.8922e-01,  7.5057e-01,  9.1816e-01, -7.8156e-01,\n",
       "          -6.5409e-02,  4.6000e-01,  9.5209e-01, -7.4219e-01,  1.3208e+00,\n",
       "           2.2618e-01, -1.4691e+00,  1.5481e+00,  7.6153e-01, -4.5954e-01,\n",
       "          -5.3101e-01, -1.1500e+00,  1.0732e+00, -1.8935e-01,  1.2166e+00,\n",
       "           2.2742e-01,  1.0709e-01, -1.0999e+00,  3.6955e-01,  6.9419e-01,\n",
       "          -8.6960e-01,  6.8245e-01,  1.1145e+00,  1.5353e+00, -8.6285e-01,\n",
       "          -8.1956e-01, -3.7560e-01, -1.5232e+00,  1.6615e+00,  1.4397e+00,\n",
       "          -4.5722e-01, -3.0024e-01, -5.6718e-02, -2.0772e+00, -3.5747e-02,\n",
       "           2.0368e-01, -7.5619e-01,  1.4215e+00,  2.6899e-01,  5.4199e-02,\n",
       "           4.8780e-02,  3.2266e-01,  2.1720e-01]], grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hiddenstates[0], hiddenstates[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "c4a5d5b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hiddenstates = all_encoder_layers[-1]\n",
    "hiddenstates = hiddenstates[:, 0]\n",
    "hiddenstates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "9538348f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_states [:, 0] shape : torch.Size([1, 768])\n",
      "before bert_pooler last all_encoder_layers shape : torch.Size([1, 7, 768])\n",
      "after bert_pooler last all_encoder_layers shape : torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "hiddenstates = all_encoder_layers[-1]\n",
    "# hiddenstates = hiddenstates[:, 0]\n",
    "print('hidden_states [:, 0] shape : {}'.format(hiddenstates[:, 0].shape))\n",
    "print('before bert_pooler last all_encoder_layers shape : {}'.format(hiddenstates.shape))\n",
    "hiddenstates = bert_pooler(hiddenstates)\n",
    "print('after bert_pooler last all_encoder_layers shape : {}'.format(hiddenstates.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "f2b79182",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768]) tensor([[ 0.0386,  0.0386,  0.8052,  0.2341,  0.2425,  0.0724,  0.4209, -0.7255,\n",
      "          0.3281, -0.6743, -0.2221, -0.4849, -0.8207,  0.2359, -0.3135,  0.7883,\n",
      "         -0.5762,  0.5450,  0.7982, -0.0573,  0.0459, -0.0704, -0.7381,  0.6554,\n",
      "         -0.3530, -0.6023,  0.0130, -0.4715, -0.1234, -0.7505,  0.2417, -0.7780,\n",
      "         -0.2258,  0.1815,  0.6942, -0.0496, -0.5706,  0.7830,  0.5995, -0.1354,\n",
      "         -0.1721,  0.8067, -0.5260,  0.2463, -0.1281,  0.6694, -0.0821, -0.5646,\n",
      "          0.3978, -0.6315,  0.0043, -0.4287,  0.0793,  0.2377, -0.3383, -0.4438,\n",
      "         -0.0436,  0.3002,  0.3886,  0.4272, -0.0567,  0.6415, -0.2289,  0.0599,\n",
      "         -0.5042,  0.4704,  0.0157,  0.5565,  0.1264,  0.5160,  0.3277,  0.1653,\n",
      "         -0.2262, -0.4129,  0.4855, -0.0702,  0.1826,  0.6167, -0.1407, -0.1800,\n",
      "          0.1263,  0.6942, -0.1554, -0.7251, -0.2398, -0.6362, -0.3235,  0.1862,\n",
      "          0.1509,  0.1518, -0.1274,  0.0579, -0.8382, -0.5350, -0.0850, -0.1336,\n",
      "          0.3461, -0.5382, -0.1126, -0.5537,  0.4490, -0.4556,  0.2647, -0.7090,\n",
      "          0.3812, -0.7115, -0.3267, -0.2631,  0.3462,  0.2923,  0.2301, -0.2390,\n",
      "          0.5994,  0.1075, -0.5302, -0.0364,  0.5147, -0.7033, -0.0979,  0.0664,\n",
      "         -0.0747, -0.1602, -0.2028,  0.5225, -0.4343, -0.4369, -0.2517,  0.4568,\n",
      "         -0.2312,  0.8039, -0.0958, -0.6944,  0.8668,  0.2953,  0.2236, -0.5407,\n",
      "         -0.3831,  0.7522,  0.5802, -0.7575, -0.5669,  0.6778,  0.2527,  0.7166,\n",
      "          0.4466,  0.3075, -0.3937,  0.4421, -0.4531,  0.4219,  0.2492, -0.4324,\n",
      "          0.2216, -0.6543,  0.3935, -0.6651, -0.5860,  0.0624,  0.1075,  0.0874,\n",
      "          0.3421, -0.2690, -0.1664, -0.6472,  0.2898,  0.0895, -0.5737,  0.5811,\n",
      "          0.1172,  0.6356, -0.4514,  0.5376,  0.1898, -0.1866,  0.8228, -0.7473,\n",
      "         -0.4921,  0.0895,  0.8552,  0.2705,  0.1117,  0.4038,  0.4308,  0.1346,\n",
      "          0.3111,  0.4396, -0.3213, -0.5571, -0.1318, -0.1437,  0.3239, -0.2731,\n",
      "         -0.2994, -0.8027, -0.1652, -0.6121,  0.7123, -0.3934, -0.1222, -0.2376,\n",
      "         -0.4572, -0.2352,  0.7334,  0.0627, -0.2451,  0.1375,  0.4135, -0.6628,\n",
      "         -0.3031,  0.3302, -0.7674,  0.7567, -0.2777,  0.3738, -0.2930,  0.0249,\n",
      "         -0.8790,  0.1571, -0.0231,  0.3933, -0.5951,  0.0124, -0.2480, -0.1460,\n",
      "         -0.2588,  0.4266, -0.6018, -0.9253, -0.4970, -0.4068, -0.7440,  0.1540,\n",
      "         -0.1618, -0.1885, -0.4867, -0.4057, -0.3173, -0.6961, -0.4587, -0.5469,\n",
      "         -0.7550, -0.2593, -0.5317,  0.3635, -0.2969, -0.8515,  0.2346, -0.6601,\n",
      "          0.2678,  0.0907,  0.0643,  0.5281,  0.3322,  0.3982,  0.4597,  0.0700,\n",
      "          0.1495, -0.3441,  0.6099,  0.4531,  0.6327, -0.3668, -0.4565,  0.1981,\n",
      "         -0.2212,  0.0827,  0.1732,  0.6532,  0.1927, -0.0950,  0.4345,  0.8055,\n",
      "          0.3711,  0.1490,  0.2866, -0.1253,  0.0035,  0.5401,  0.5791,  0.0114,\n",
      "          0.4491,  0.2395, -0.6109,  0.2956,  0.3074,  0.1278,  0.4619, -0.2540,\n",
      "          0.7183,  0.1586, -0.5617,  0.2842,  0.0775,  0.2355, -0.2289,  0.6863,\n",
      "          0.0110, -0.3780,  0.3766,  0.6791, -0.1791,  0.2185,  0.0221, -0.1689,\n",
      "         -0.4672, -0.3477,  0.4023,  0.8281,  0.1866, -0.2504,  0.0829, -0.5773,\n",
      "          0.4375,  0.9163,  0.3489, -0.7644,  0.6969, -0.4405,  0.1708,  0.4243,\n",
      "         -0.3046, -0.0680, -0.6839,  0.6408, -0.8875, -0.6920,  0.6203, -0.4329,\n",
      "          0.1049,  0.6845, -0.2518, -0.5062,  0.0305, -0.8079,  0.6074,  0.2356,\n",
      "          0.1857, -0.0672, -0.7574,  0.1454,  0.7578, -0.8016,  0.1666,  0.0321,\n",
      "          0.5308, -0.3366, -0.1998, -0.3734, -0.2342,  0.0172, -0.2020, -0.6982,\n",
      "          0.6730,  0.0225, -0.0293, -0.7985, -0.0037,  0.7871, -0.5703, -0.5847,\n",
      "          0.7245, -0.1592,  0.4028,  0.6959,  0.2170, -0.0097,  0.7066,  0.6360,\n",
      "         -0.7796, -0.0448, -0.4779,  0.7811, -0.1035, -0.1948,  0.2430,  0.5038,\n",
      "         -0.5196,  0.4525, -0.2629, -0.7937, -0.0305,  0.0234, -0.3451,  0.0431,\n",
      "          0.3568, -0.1886,  0.4642,  0.0338, -0.2741,  0.2314,  0.2878, -0.3759,\n",
      "          0.9185, -0.4229, -0.0771, -0.3513, -0.0914, -0.3751, -0.5239, -0.4229,\n",
      "         -0.4213, -0.6131,  0.4993, -0.6720,  0.2098,  0.5323, -0.6379, -0.3682,\n",
      "         -0.1252,  0.7462,  0.5916,  0.6872, -0.0955, -0.5937, -0.1653, -0.2887,\n",
      "          0.0786,  0.2936,  0.8619,  0.2977,  0.4190, -0.8146,  0.2686, -0.3949,\n",
      "         -0.2834, -0.3866, -0.4384,  0.2503, -0.0635, -0.4721,  0.1414,  0.4472,\n",
      "         -0.4161,  0.0708, -0.0803,  0.4163,  0.2477, -0.0279, -0.4657,  0.2747,\n",
      "         -0.2387,  0.7343,  0.5196, -0.7499,  0.4671,  0.2916,  0.3772, -0.4787,\n",
      "          0.1640, -0.4893, -0.2104, -0.3313, -0.1863,  0.0609, -0.2931,  0.1233,\n",
      "          0.6026, -0.3203, -0.1135, -0.4000, -0.4394, -0.0394, -0.5274, -0.6509,\n",
      "          0.2165,  0.3349,  0.6199, -0.2105,  0.7089, -0.3238,  0.8723,  0.6363,\n",
      "          0.3966,  0.2508, -0.3766,  0.2605, -0.9424, -0.2927, -0.2061, -0.0276,\n",
      "         -0.3786, -0.7093, -0.6499,  0.2826, -0.5312, -0.3173, -0.4238,  0.4704,\n",
      "         -0.9253,  0.4071, -0.2484, -0.3563, -0.0182,  0.2818, -0.5848,  0.6743,\n",
      "         -0.0297,  0.5878, -0.2822, -0.8153, -0.1624,  0.3186,  0.2095, -0.5584,\n",
      "          0.0469, -0.3262,  0.0686,  0.2762,  0.2247, -0.6702,  0.9365,  0.0297,\n",
      "          0.0321, -0.4023,  0.7142, -0.2876, -0.5706,  0.3576, -0.7227, -0.6546,\n",
      "          0.4863, -0.5009,  0.6108, -0.0885, -0.8794, -0.0641,  0.2620,  0.2863,\n",
      "          0.3020,  0.4539,  0.0369, -0.0238,  0.2207, -0.0211, -0.5048, -0.4017,\n",
      "          0.5643,  0.3382,  0.4018, -0.5297, -0.6884,  0.0315, -0.1591, -0.1724,\n",
      "          0.4080, -0.5759,  0.7190,  0.5023, -0.2697,  0.8347,  0.2721, -0.2574,\n",
      "          0.3791,  0.5434, -0.3073, -0.3179,  0.2522, -0.7275,  0.1370, -0.5759,\n",
      "         -0.4180,  0.9101, -0.8752,  0.4980, -0.6804, -0.1843,  0.0204, -0.5679,\n",
      "         -0.2854,  0.1559,  0.2628, -0.3908,  0.5042,  0.1722,  0.4840,  0.0046,\n",
      "          0.4033,  0.9141, -0.0493,  0.5470,  0.2041,  0.3690,  0.4612,  0.7906,\n",
      "         -0.3744,  0.0651,  0.2398,  0.2402, -0.5827, -0.3621, -0.4988,  0.8102,\n",
      "         -0.0274, -0.4231,  0.0400, -0.6101, -0.9155, -0.5730, -0.0073, -0.7598,\n",
      "         -0.2236,  0.4464, -0.5941, -0.5650, -0.2448, -0.0066, -0.2710,  0.8360,\n",
      "          0.0322, -0.3147,  0.1939,  0.0769, -0.7333, -0.3073, -0.1595, -0.3336,\n",
      "         -0.3786,  0.0398,  0.3596,  0.4513,  0.1604,  0.5214,  0.4230,  0.5232,\n",
      "          0.3370, -0.0499,  0.0960,  0.0187,  0.1965,  0.1019, -0.5178,  0.2849,\n",
      "         -0.1180, -0.1473,  0.0951,  0.1650, -0.4007, -0.1387, -0.4758, -0.1579,\n",
      "         -0.4955,  0.2213,  0.2540, -0.5980,  0.5838,  0.5364,  0.7422,  0.1568,\n",
      "         -0.4605, -0.0303,  0.1130,  0.1206,  0.3863, -0.3222, -0.6163,  0.6378,\n",
      "         -0.7401,  0.1165,  0.3955,  0.8046, -0.2064, -0.7684, -0.5256,  0.3207,\n",
      "          0.7959,  0.8875,  0.0646, -0.5330, -0.1211, -0.8201, -0.4236,  0.7094,\n",
      "         -0.6921, -0.6503, -0.7666,  0.1210, -0.4131,  0.8378,  0.7742,  0.2615,\n",
      "          0.2376, -0.7369,  0.2247,  0.1045, -0.1960, -0.6732, -0.2540,  0.4956,\n",
      "          0.2500, -0.1930, -0.2066,  0.3299,  0.7253, -0.3957, -0.0683, -0.1763,\n",
      "         -0.4217,  0.4831, -0.6741, -0.2297,  0.3287,  0.2939,  0.2376, -0.2887,\n",
      "         -0.6998,  0.1722,  0.6771, -0.6147,  0.3688,  0.7067, -0.3403,  0.3789,\n",
      "         -0.4075,  0.0991, -0.5051, -0.5671,  0.5889, -0.4023, -0.2757, -0.3935,\n",
      "          0.1702,  0.4301,  0.3582,  0.0883, -0.5134,  0.8073, -0.3024,  0.2570,\n",
      "         -0.0594, -0.3837, -0.1611,  0.0018,  0.2530,  0.1895, -0.7771, -0.5133,\n",
      "          0.1659,  0.4427, -0.2898, -0.1857,  0.4277, -0.0736, -0.1988, -0.3739,\n",
      "         -0.2328,  0.2380, -0.0679, -0.5957, -0.1217, -0.0405, -0.6725,  0.5643,\n",
      "         -0.8804,  0.0227, -0.5640, -0.0629, -0.3174, -0.8435, -0.5185,  0.2633,\n",
      "         -0.6471, -0.7615, -0.6901,  0.4065,  0.2106, -0.3756,  0.5762, -0.0372]],\n",
      "       grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "hiddenstates = all_encoder_layers[-1]\n",
    "hiddenstates = hiddenstates[:, 0]\n",
    "hiddenstates = bert_pooler.dense(hiddenstates)\n",
    "hiddenstates = bert_pooler.activation(hiddenstates)\n",
    "print(hiddenstates.shape, hiddenstates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777fa063",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86da244d",
   "metadata": {},
   "source": [
    "## 8.2.8 동작 확인\n",
    "- 미니 배치의 크기를 2, 각 미니 배치의 문장 길이르 5로 하여 입력을 적당히 생성\n",
    "- 길이 5에 두 문장이 포함되어 있음. 어떠한 단어까지 첫 번째 문장이고 어떠한 단어부터 두 번째 문장인지 나타내는 문장 ID와 Attention용 마스크도 생성. 이러한 입력으로 동작을 확인\n",
    "- Attention용 마스크를 확장한 extended_attention_mask 변수를 작성한다는 점을 주의해야 한다\n",
    "- Multi-Headed Self-Attention에서 Attention마스크를 사용할 수 있도록 하는 변환.\n",
    "- Attention을 적용하지 않는 부분은 시그모이드를 계산했을 때 0이 되도록 마이너스 무한의 대안으로써 -10000을 대입."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2d97169e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 단어 ID열의 텐서 크기:  torch.Size([2, 5])\n",
      "입력 마스크의 텐서 크기:  torch.Size([2, 5])\n",
      "입력 문장 ID의 텐서 크기:  torch.Size([2, 5])\n",
      "확장된 마스크의 텐서 크기:  torch.Size([2, 1, 1, 5])\n",
      "BertEmbeddings의 출력 텐서 크기: torch.Size([2, 5, 768])\n",
      "BertEncoder 최후 층의 출력 텐서 크기: torch.Size([2, 5, 768])\n",
      "BertPooler의 출력 텐서 크기: torch.Size([2, 768])\n"
     ]
    }
   ],
   "source": [
    "# 동작 확인\n",
    "\n",
    "# 입력 단어 ID열 batch_size는 두 가지\n",
    "input_ids = torch.LongTensor([[31, 51, 12, 23, 99], [15, 5, 1, 0, 0]])\n",
    "print('입력 단어 ID열의 텐서 크기: ', input_ids.shape)\n",
    "# 마스크\n",
    "attention_mask = torch.LongTensor([[1, 1, 1, 1, 1], [1, 1, 1, 0, 0]])\n",
    "print('입력 마스크의 텐서 크기: ', attention_mask.shape)\n",
    "\n",
    "# 문장의 ID, 두 미니 배치 각각에 대한 0은 첫 번째 문장을, 1은 두 번째 문장을 나타낸다.\n",
    "token_type_ids = torch.LongTensor([[0, 0, 1, 1, 1], [0, 1, 1, 1, 1]])\n",
    "print('입력 문장 ID의 텐서 크기: ', token_type_ids.shape)\n",
    "\n",
    "# BERT의 각 모듈 준비\n",
    "embeddings = BertEmbeddings(config)\n",
    "encoder = BertEncoder(config)\n",
    "pooler = BertPooler(config)\n",
    "\n",
    "# 마스크 변형 [batch_size, 1, 1, seq_length]로 한다.\n",
    "# Attention을 적용하지 않는 부분은 마이너스 무한으로 하고 위하여 -10000을 곱한다.\n",
    "extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "extended_attention_mask = extended_attention_mask.to(dtype=torch.float32)\n",
    "extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "print('확장된 마스크의 텐서 크기: ', extended_attention_mask.shape)\n",
    "\n",
    "# 순전파\n",
    "out1 = embeddings(input_ids, token_type_ids)\n",
    "print('BertEmbeddings의 출력 텐서 크기:', out1.shape)\n",
    "\n",
    "out2 = encoder(out1, extended_attention_mask)\n",
    "# out2는 [minibatch, seq_length, embedding_dim]이 12개 리스트\n",
    "print('BertEncoder 최후 층의 출력 텐서 크기:', out2[0].shape)\n",
    "\n",
    "out3 = pooler(out2[-1]) # out2는 12층의 특징량 리스트가 되어 가장 마지막을 사용\n",
    "print('BertPooler의 출력 텐서 크기:', out3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c1816477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5]),\n",
       " torch.Size([2, 1, 1, 5]),\n",
       " tensor([[[[1, 1, 1, 1, 1]]],\n",
       " \n",
       " \n",
       "         [[[1, 1, 1, 0, 0]]]]))"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "attention_mask.shape, extended_attention_mask.shape, extended_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a08a6271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1, 1, 5]),\n",
       " tensor([[[[1., 1., 1., 1., 1.]]],\n",
       " \n",
       " \n",
       "         [[[1., 1., 1., 0., 0.]]]]))"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extended_attention_mask = extended_attention_mask.to(dtype=torch.float32)\n",
    "extended_attention_mask.shape, extended_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a4ebaae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1, 1, 5]),\n",
       " tensor([[[[    -0.,     -0.,     -0.,     -0.,     -0.]]],\n",
       " \n",
       " \n",
       "         [[[    -0.,     -0.,     -0., -10000., -10000.]]]]))"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "extended_attention_mask.shape, extended_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "557d7099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5183,  0.8744, -0.6474,  ..., -0.1523,  0.0000, -1.8388],\n",
       "         [-0.0000, -2.2031,  0.7122,  ...,  0.2697, -0.0000, -1.0999],\n",
       "         [-0.5974,  0.7153, -1.2055,  ...,  1.6777,  0.6225,  0.5458],\n",
       "         [ 1.4920,  0.1230, -0.0000,  ...,  0.1844, -1.1844,  0.2829],\n",
       "         [-0.4239,  0.1712, -1.7108,  ..., -0.6405, -1.0815,  0.5137]],\n",
       "\n",
       "        [[ 1.0652, -0.2365, -0.2122,  ...,  1.3648,  1.8276, -0.9468],\n",
       "         [ 0.7128, -2.1402,  0.8725,  ..., -1.3434, -1.8721, -0.1389],\n",
       "         [-0.4667,  0.1940, -0.7100,  ...,  0.3417,  0.9378,  0.1089],\n",
       "         [ 0.0000,  0.5578, -0.6825,  ...,  0.3566, -2.2600,  0.7890],\n",
       "         [-1.0589, -0.3668, -1.6822,  ..., -1.2145, -1.0332,  1.1162]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "3734b0a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5, 768]),\n",
       " 12,\n",
       " torch.Size([2, 5, 768]),\n",
       " tensor([[[ 5.0371e-01,  8.2984e-01, -5.0412e-01,  ...,  7.2131e-02,\n",
       "            1.7786e-01, -2.2644e+00],\n",
       "          [ 5.6436e-01, -1.9141e+00,  6.7024e-01,  ...,  4.3499e-01,\n",
       "            4.4968e-01, -9.2028e-01],\n",
       "          [-1.4776e-01,  9.9116e-01, -1.0737e+00,  ...,  1.8967e+00,\n",
       "            4.7691e-01,  3.4587e-01],\n",
       "          [ 1.6951e+00,  4.8224e-01, -1.2518e-01,  ...,  4.8702e-01,\n",
       "           -1.2383e+00,  2.1447e-01],\n",
       "          [-2.1283e-01,  7.6516e-01, -1.3443e+00,  ..., -5.4793e-01,\n",
       "           -9.2073e-01,  3.2209e-01]],\n",
       " \n",
       "         [[ 1.1051e+00, -2.8525e-01,  3.5683e-01,  ...,  1.5262e+00,\n",
       "            1.9065e+00, -1.0319e+00],\n",
       "          [ 6.1323e-01, -1.6044e+00,  1.1423e+00,  ..., -8.1751e-01,\n",
       "           -1.8087e+00, -3.5258e-04],\n",
       "          [ 5.6175e-02, -1.1095e-01, -2.0454e-01,  ...,  1.0018e+00,\n",
       "            6.1780e-01, -8.1472e-02],\n",
       "          [-1.1993e-02,  4.7299e-01, -4.4785e-01,  ...,  8.5274e-01,\n",
       "           -1.8880e+00,  4.9705e-01],\n",
       "          [-8.5024e-01, -1.3195e-01, -1.0566e+00,  ..., -9.5625e-01,\n",
       "           -7.2232e-01,  5.4782e-01]]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2 = encoder(out1, extended_attention_mask)\n",
    "out1.shape, len(out2), out2[0].shape, out2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "73e6d849",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macos/anaconda3/envs/dl/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(510)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = nn.Softmax()\n",
    "torch.argmax(softmax(out3[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110843aa",
   "metadata": {},
   "source": [
    "## 8.2.9 모두 연결하여 BERT모델로\n",
    "- 동작을 확인한 후 문제가 없다면 모두 연결한 BERT 모델로 지정."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "17be408a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7])"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "01e2825d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention 마스크와 첫 번째, 두 번째 문장의 id가 없으면 작성\n",
    "if attention_mask is None:\n",
    "    attention_mask = torch.one_like(input_ids)\n",
    "if token_type_ids is None:\n",
    "    token_type_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "# 마스크 변형 [minibatch, 1, 1, seq_length]로 한다.\n",
    "# 나중에 Multi-Headed Self-Attention에서 사용할 수 있는 형태로 하기 위하여\n",
    "extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "# 마스크는 0, 1 이지만 소프트맥스를 계산할 때 마스크가 되도록 0과 -inf로 한다.\n",
    "# -inf 대신 -10000으로 한다\n",
    "extended_attention_mask = extended_attention_mask.to(dtype=torch.float32)\n",
    "extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "874f7788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_mask shape : torch.Size([1, 7])\n",
      "attention_mask unsqueeze shape : torch.Size([1, 1, 7])\n",
      "attention_mask unsqueeze 1 and unsqueeze 2 shape : torch.Size([1, 1, 1, 7])\n",
      "extended_attention_mask shape : torch.Size([1, 1, 1, 7])\n",
      "extended_attention_mask : tensor([[[[1., 1., 1., 1., 1., 1., 1.]]]])\n",
      "masked extended_attention_mask : tensor([[[[-0., -0., -0., -0., -0., -0., -0.]]]])\n"
     ]
    }
   ],
   "source": [
    "print('attention_mask shape : {}'.format(attention_mask.shape))\n",
    "print('attention_mask unsqueeze shape : {}'.format(attention_mask.unsqueeze(1).shape))\n",
    "print('attention_mask unsqueeze 1 and unsqueeze 2 shape : {}'.format(attention_mask.unsqueeze(1).unsqueeze(2).shape))\n",
    "extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "print('extended_attention_mask shape : {}'.format(extended_attention_mask.shape))\n",
    "extended_attention_mask = extended_attention_mask.to(dtype=torch.float32)\n",
    "print('extended_attention_mask : {}'.format(extended_attention_mask))\n",
    "extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "print('masked extended_attention_mask : {}'.format(extended_attention_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "b7a87842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.4897e-01, -2.1518e+00, -4.9180e-01,  ...,  1.0964e+00,\n",
       "           1.2787e+00,  8.3720e-01],\n",
       "         [-1.6423e+00,  1.6222e+00,  5.3157e-01,  ...,  4.2774e-01,\n",
       "           2.4528e+00, -0.0000e+00],\n",
       "         [-5.4673e-01, -1.0264e+00, -9.7880e-04,  ..., -7.9996e-01,\n",
       "           0.0000e+00,  4.2281e-01],\n",
       "         ...,\n",
       "         [ 1.3268e-02, -1.8150e+00, -0.0000e+00,  ..., -3.5489e-01,\n",
       "          -2.5589e-01,  9.7428e-01],\n",
       "         [ 4.2085e-01, -2.6400e+00, -1.5607e+00,  ...,  2.9930e-01,\n",
       "           1.1933e+00,  7.1514e-01],\n",
       "         [ 4.9037e-01, -1.5325e+00,  7.8137e-01,  ..., -0.0000e+00,\n",
       "           1.1185e+00,  9.2136e-02]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_output = final_model.embeddings(input_ids, token_type_ids)\n",
    "embedding_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02981f2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "89d406c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(nn.Module):\n",
    "    '''모듈을 전부 연결한 BERT 모델'''\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(BertModel, self).__init__()\n",
    "        \n",
    "        # 세 가지 모듈 작성\n",
    "        self.embeddings = BertEmbeddings(config)\n",
    "        self.encoder = BertEncoder(config)\n",
    "        self.pooler = BertPooler(config)\n",
    "        \n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, \n",
    "                output_all_encoded_layers=True,\n",
    "                attention_show_fig=False):\n",
    "        '''\n",
    "        input_ids : [batch_size, sequence_length] 문장의 단어 ID 나열\n",
    "        token_type_ids : [batch_size, sequence_length] 각 단어가 첫 번째 문장인지 두 번째 문장인지 나타내는 id\n",
    "        attention_mask: Transformer의 마스크와 같은 기능의 마스킹\n",
    "        output_all_encoded_layers : 마지막 출력에 12단의 Transformer 모두 리스트로 반환할지 마지막만인지 지정\n",
    "        attention_show_fig : Self-Attention의 가중치를 반환할지 플래그\n",
    "        '''\n",
    "        \n",
    "        # Attention 마스크와 첫 번째, 두 번째 문장의 id가 없으면 작성\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.one_like(input_ids)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "            \n",
    "        # 마스크 변형 [minibatch, 1, 1, seq_length]로 한다.\n",
    "        # 나중에 Multi-Headed Self-Attention에서 사용할 수 있는 형태로 하기 위하여\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "        \n",
    "        # 마스크는 0, 1 이지만 소프트맥스를 계산할 때 마스크가 되도록 0과 -inf로 한다.\n",
    "        # -inf 대신 -10000으로 한다\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=torch.float32)\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "        \n",
    "        # 순전파 시킨다\n",
    "        # BertEmbeddings 모듈\n",
    "        embedding_output = self.embeddings(input_ids, token_type_ids)\n",
    "        \n",
    "        # BertLayer 모듈(Transformer)을 반복하는 BertEncoder 모듈\n",
    "        if attention_show_fig == True:\n",
    "            '''attention_show의 경우 attention_probs도 반환'''\n",
    "            \n",
    "            encoded_layers, attention_probs = self.encoder(embedding_output,\n",
    "                                                           extended_attention_mask,\n",
    "                                                           output_all_encoded_layers,\n",
    "                                                           attention_show_fig)\n",
    "            \n",
    "        elif attention_show_fig == False:\n",
    "            encoded_layers = self.encoder(embedding_output,\n",
    "                                          extended_attention_mask,\n",
    "                                          output_all_encoded_layers,\n",
    "                                          attention_show_fig)\n",
    "            \n",
    "        # BertPooler 모듈\n",
    "        # 인코더의 맨 마지막 BertLayer에서 출력된 특징량 사용\n",
    "        pooled_output = self.pooler(encoded_layers[-1])\n",
    "        \n",
    "        # output_all_encoded_layer가 False인 경우는 리스트가 아닌 텐서를 반환\n",
    "        if not output_all_encoded_layers:\n",
    "            encoded_layers = encoded_layers[-1]\n",
    "            \n",
    "        # attention_show의 경우 attention_probs(가장 마지막)도 반환한다.\n",
    "        if attention_show_fig == True:\n",
    "            return encoded_layers, pooled_output, attention_probs\n",
    "        elif attention_show_fig == False:\n",
    "            return encoded_layers, pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "10ac1957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.max_position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "61d31259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (selfattn): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (drop_out): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model = BertModel(config)\n",
    "final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e44005d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===========================================================================\n",
       "Layer (type:depth-idx)                             Param #\n",
       "===========================================================================\n",
       "BertModel                                          --\n",
       "├─BertEmbeddings: 1-1                              --\n",
       "│    └─Embedding: 2-1                              23,440,896\n",
       "│    └─Embedding: 2-2                              393,216\n",
       "│    └─Embedding: 2-3                              1,536\n",
       "│    └─BertLayerNorm: 2-4                          1,536\n",
       "│    └─Dropout: 2-5                                --\n",
       "├─BertEncoder: 1-2                                 --\n",
       "│    └─ModuleList: 2-6                             --\n",
       "│    │    └─BertLayer: 3-1                         7,087,872\n",
       "│    │    └─BertLayer: 3-2                         7,087,872\n",
       "│    │    └─BertLayer: 3-3                         7,087,872\n",
       "│    │    └─BertLayer: 3-4                         7,087,872\n",
       "│    │    └─BertLayer: 3-5                         7,087,872\n",
       "│    │    └─BertLayer: 3-6                         7,087,872\n",
       "│    │    └─BertLayer: 3-7                         7,087,872\n",
       "│    │    └─BertLayer: 3-8                         7,087,872\n",
       "│    │    └─BertLayer: 3-9                         7,087,872\n",
       "│    │    └─BertLayer: 3-10                        7,087,872\n",
       "│    │    └─BertLayer: 3-11                        7,087,872\n",
       "│    │    └─BertLayer: 3-12                        7,087,872\n",
       "├─BertPooler: 1-3                                  --\n",
       "│    └─Linear: 2-7                                 590,592\n",
       "│    └─Tanh: 2-8                                   --\n",
       "===========================================================================\n",
       "Total params: 109,482,240\n",
       "Trainable params: 109,482,240\n",
       "Non-trainable params: 0\n",
       "==========================================================================="
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ti.summary(final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "179e42b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertLayer(\n",
       "  (attention): BertAttention(\n",
       "    (selfattn): BertSelfAttention(\n",
       "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (drop_out): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (output): BertSelfOutput(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (intermediate): BertIntermediate(\n",
       "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "  )\n",
       "  (output): BertOutput(\n",
       "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.encoder.layer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "dae40dd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertSelfAttention(\n",
       "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (drop_out): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.encoder.layer[0].attention.selfattn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "9b175104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_layers의 텐서 크기: torch.Size([2, 5, 768])\n",
      "pooled_output의 텐서 크기: torch.Size([2, 768])\n",
      "attention_probs의 텐서 크기: torch.Size([2, 12, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "# 동작 확인\n",
    "# 입력 준비\n",
    "input_ids = torch.LongTensor([[31, 52, 12, 23, 99], [15, 5, 1, 0, 0]])\n",
    "attention_mask = torch.LongTensor([[1, 1, 1, 1, 1], [1, 1, 1, 0, 0]])\n",
    "token_type_ids = torch.LongTensor([[0, 0, 1, 1, 1], [0, 1, 1, 1, 1]])\n",
    "\n",
    "# BERT 모델을 만든다.\n",
    "net = BertModel(config)\n",
    "\n",
    "# 순전파\n",
    "encoded_layers, pooled_output, attention_probs = net(\n",
    "    input_ids, token_type_ids, attention_mask, output_all_encoded_layers=True, attention_show_fig=True)\n",
    "\n",
    "print('encoded_layers의 텐서 크기:', encoded_layers[-1].shape)\n",
    "print('pooled_output의 텐서 크기:', pooled_output.shape)\n",
    "print('attention_probs의 텐서 크기:', attention_probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "8faebd4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[ 1.2406,  0.8388, -0.1334,  ..., -0.8475,  0.3371,  1.5810],\n",
       "          [-0.2435,  0.0123, -0.2764,  ..., -0.3069,  0.9863,  0.6150],\n",
       "          [-0.3007,  0.1235,  0.7693,  ..., -0.7627,  0.6404, -0.5967],\n",
       "          [-0.4496, -0.4860, -0.5400,  ..., -1.0434,  2.0181, -1.1252],\n",
       "          [-1.3615,  0.2910, -0.8254,  ..., -1.0138,  1.2273,  0.0501]],\n",
       " \n",
       "         [[-0.1457, -1.6330, -0.6535,  ..., -0.2874,  0.0074,  1.1234],\n",
       "          [ 0.5560,  0.3806, -0.4249,  ...,  0.5411,  0.9022, -2.1891],\n",
       "          [-0.2282,  0.1721,  0.5840,  ..., -1.1978,  1.1644, -1.8055],\n",
       "          [-0.8305, -0.8269,  0.5051,  ...,  0.0763,  0.9785, -2.0397],\n",
       "          [-1.0807, -0.3496,  1.1301,  ..., -0.8619,  1.5265, -0.7355]]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([[[ 0.7210,  0.8323,  0.2814,  ..., -0.8977, -0.2409,  1.2964],\n",
       "          [-0.4924, -0.1004,  0.6234,  ..., -0.4198,  1.2488,  0.4650],\n",
       "          [-0.7094,  0.1843,  1.0799,  ..., -1.0826,  0.3717, -0.3686],\n",
       "          [-0.5833, -0.2877, -0.2358,  ..., -0.9736,  1.5546, -0.7453],\n",
       "          [-2.0493,  0.4425, -0.4189,  ..., -1.2346,  1.1235, -0.0434]],\n",
       " \n",
       "         [[-0.5049, -1.4892,  0.3522,  ..., -0.6165, -0.0301,  0.9840],\n",
       "          [-0.0389,  0.6686,  0.4330,  ...,  0.5689,  0.8516, -2.1567],\n",
       "          [-0.5728,  0.2055,  1.0299,  ..., -1.2758,  1.1224, -1.7118],\n",
       "          [-1.0872, -0.7453,  1.1442,  ...,  0.2458,  0.5267, -1.8222],\n",
       "          [-2.1064,  0.0056,  1.5011,  ..., -0.7483,  1.3275, -0.6555]]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([[[ 0.6296,  0.6590,  0.1450,  ..., -0.7264,  0.0446,  1.1457],\n",
       "          [-0.3044, -0.2115,  0.3629,  ..., -0.4365,  1.2016,  0.4032],\n",
       "          [-0.4407, -0.1459,  0.5577,  ..., -1.0276,  0.8706, -0.4891],\n",
       "          [-0.4593, -0.1930, -0.4218,  ..., -1.0248,  1.4431, -0.5957],\n",
       "          [-1.6815,  0.3374, -0.6352,  ..., -1.1543,  1.1859, -0.1013]],\n",
       " \n",
       "         [[-0.3137, -1.6160,  0.0315,  ..., -0.4573,  0.2087,  0.8620],\n",
       "          [-0.0760,  0.2541,  0.4007,  ...,  0.4135,  0.7918, -1.7994],\n",
       "          [-0.4150,  0.0135,  0.6006,  ..., -1.0944,  1.5436, -2.3291],\n",
       "          [-1.3061, -0.6275,  1.0332,  ...,  0.2537,  0.8186, -1.3072],\n",
       "          [-2.0960, -0.0265,  1.3089,  ..., -0.6706,  1.9784, -0.5648]]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([[[ 0.9566,  0.6135,  0.4812,  ..., -0.4071, -0.1410,  0.7506],\n",
       "          [-0.1397, -0.1053,  0.6219,  ..., -0.5250,  0.8802,  0.0382],\n",
       "          [-0.0258,  0.0513,  0.6220,  ..., -1.0633,  0.7466, -0.6839],\n",
       "          [-0.0555, -0.1675, -0.2616,  ..., -1.0152,  1.4574, -0.7292],\n",
       "          [-1.4029,  0.5067, -0.7165,  ..., -1.4930,  1.0689, -0.4664]],\n",
       " \n",
       "         [[-0.2423, -1.6904,  0.3744,  ..., -0.3032,  0.2152,  0.4423],\n",
       "          [-0.3678,  0.0388,  0.6330,  ...,  0.4730,  0.6991, -1.8686],\n",
       "          [-0.6391,  0.1158,  0.8824,  ..., -0.8222,  1.4741, -2.4874],\n",
       "          [-1.0866, -0.4557,  1.2126,  ...,  0.3940,  0.6952, -1.2255],\n",
       "          [-2.1273,  0.1254,  1.4401,  ..., -0.6358,  1.9175, -0.7028]]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([[[ 0.6663,  0.6369,  0.6821,  ..., -0.1440, -0.3655,  0.6220],\n",
       "          [-0.2725, -0.3241,  0.9071,  ..., -0.1299,  0.5661,  0.2214],\n",
       "          [-0.0185,  0.0239,  0.9691,  ..., -0.7463,  0.6734, -0.7454],\n",
       "          [ 0.0281, -0.5053, -0.3512,  ..., -0.8379,  1.3153, -0.9085],\n",
       "          [-1.3435,  0.4095, -0.7446,  ..., -1.2716,  0.8428, -0.4366]],\n",
       " \n",
       "         [[-0.5814, -1.7143,  0.4377,  ..., -0.1577,  0.0316,  0.6651],\n",
       "          [ 0.0914, -0.0591,  0.7199,  ...,  0.8736,  0.2894, -1.6209],\n",
       "          [-0.2226, -0.3139,  0.9386,  ..., -0.3626,  1.4177, -2.4403],\n",
       "          [-0.4217, -0.4592,  1.1507,  ...,  0.2441,  0.5498, -0.9500],\n",
       "          [-1.5816, -0.0300,  1.5092,  ..., -0.1369,  1.5576, -0.7337]]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([[[ 1.0754,  0.3676,  0.5312,  ..., -0.3107, -0.7330,  0.3519],\n",
       "          [ 0.1143, -0.3655,  0.9172,  ..., -0.0216,  0.5624, -0.0976],\n",
       "          [-0.0250, -0.1126,  1.1896,  ..., -1.1356,  0.5725, -0.4701],\n",
       "          [ 0.2157, -0.2025,  0.2171,  ..., -1.0756,  1.3006, -0.7495],\n",
       "          [-0.9492,  0.5916, -0.7269,  ..., -1.4814,  0.8976, -0.0469]],\n",
       " \n",
       "         [[-0.5404, -1.4421,  0.4465,  ..., -0.3242, -0.2647,  0.6472],\n",
       "          [ 0.0807,  0.0339,  0.8511,  ...,  0.8219,  0.1039, -2.0342],\n",
       "          [-0.3479, -0.0582,  1.3397,  ..., -0.6850,  1.1064, -2.5746],\n",
       "          [-0.5545, -0.1138,  1.5925,  ...,  0.3157,  0.3598, -0.9871],\n",
       "          [-1.4992,  0.4455,  1.4890,  ..., -0.1159,  1.1465, -1.0172]]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([[[ 0.9536,  0.6501,  0.9133,  ..., -0.4891, -0.8852, -0.0754],\n",
       "          [ 0.1419, -0.5881,  0.9458,  ..., -0.4647,  0.7322, -0.7429],\n",
       "          [ 0.1627,  0.0369,  1.2475,  ..., -1.5565,  0.5504, -0.6373],\n",
       "          [ 0.5968, -0.3942,  0.3636,  ..., -1.2237,  1.4419, -0.9596],\n",
       "          [-0.8695,  0.4715, -0.6574,  ..., -1.5905,  1.0427, -0.5708]],\n",
       " \n",
       "         [[-0.4323, -1.3965,  0.1870,  ..., -0.0679, -0.0675, -0.0993],\n",
       "          [-0.0149, -0.1457,  0.9223,  ...,  1.0404,  0.1814, -3.0151],\n",
       "          [-0.0322,  0.3126,  1.1964,  ..., -0.6429,  1.0945, -3.1694],\n",
       "          [-0.4298, -0.1286,  1.4449,  ...,  0.3527,  0.8057, -1.5302],\n",
       "          [-1.2939,  0.3569,  0.9484,  ...,  0.0834,  1.1960, -1.6932]]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([[[ 1.1026,  0.7715,  0.2847,  ..., -1.0049, -0.6151, -0.1837],\n",
       "          [-0.0423, -0.4114,  0.5035,  ..., -0.9304,  0.8969, -0.7916],\n",
       "          [-0.0047,  0.1774,  1.0115,  ..., -1.6500,  0.7455, -0.4636],\n",
       "          [ 0.0889, -0.0984,  0.0986,  ..., -1.5829,  1.6132, -1.0917],\n",
       "          [-0.9545,  0.4524, -0.9836,  ..., -1.8564,  1.2866, -0.6231]],\n",
       " \n",
       "         [[-0.6776, -0.9875,  0.2648,  ..., -0.5890,  0.0199,  0.0905],\n",
       "          [-0.4257,  0.1236,  0.7455,  ...,  0.7914,  0.2932, -2.8812],\n",
       "          [-0.4815,  0.6520,  1.0180,  ..., -0.4499,  1.0363, -2.6742],\n",
       "          [-0.8043,  0.1178,  1.3378,  ...,  0.2760,  1.2208, -1.5099],\n",
       "          [-1.6083,  0.5291,  1.0574,  ..., -0.1406,  1.5206, -1.5956]]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([[[ 1.1494,  0.5508,  0.5586,  ..., -1.0512, -0.4095, -0.2766],\n",
       "          [-0.2175, -0.3351,  0.3947,  ..., -0.9671,  0.9674, -1.1552],\n",
       "          [ 0.1015,  0.2564,  1.5277,  ..., -1.8917,  0.7194, -0.5332],\n",
       "          [ 0.1593, -0.2434,  0.0224,  ..., -1.7578,  1.6961, -1.1257],\n",
       "          [-0.8700,  0.4854, -0.5615,  ..., -1.9831,  1.7465, -1.2352]],\n",
       " \n",
       "         [[-0.7714, -0.7905,  0.3829,  ..., -0.9687, -0.2305, -0.0757],\n",
       "          [-0.3927,  0.2579,  0.9038,  ...,  0.2951,  0.2564, -2.9312],\n",
       "          [-0.2619,  0.6337,  1.6653,  ..., -1.1059,  0.6803, -2.4047],\n",
       "          [-0.5164,  0.0789,  1.6766,  ..., -0.2078,  0.9975, -1.8034],\n",
       "          [-1.2719,  0.6130,  1.2931,  ..., -0.6050,  1.2612, -1.6682]]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([[[ 0.6736,  0.2036,  0.6292,  ..., -1.0411,  0.0115,  0.1340],\n",
       "          [-0.5872, -0.2332,  0.4301,  ..., -0.7337,  1.0997, -1.1231],\n",
       "          [ 0.0565,  0.1737,  1.4271,  ..., -2.0861,  0.8920, -0.2094],\n",
       "          [-0.3664, -0.3180,  0.1456,  ..., -1.4445,  1.7383, -0.7298],\n",
       "          [-1.2385,  0.2314, -0.3220,  ..., -1.9313,  1.8960, -0.7776]],\n",
       " \n",
       "         [[-0.5076, -0.7308, -0.1535,  ..., -1.0015, -0.1269,  0.6836],\n",
       "          [-0.3850, -0.1116,  0.3857,  ...,  0.2763,  0.3237, -2.5789],\n",
       "          [-0.0841,  0.2511,  1.2912,  ..., -1.0000,  0.9884, -1.7230],\n",
       "          [-0.4705, -0.1916,  1.3218,  ..., -0.2816,  1.2109, -1.2800],\n",
       "          [-1.3637,  0.2784,  0.6578,  ..., -0.4983,  1.4598, -1.3212]]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([[[ 1.1013,  0.2984,  0.6116,  ..., -1.4037, -0.1254, -0.0062],\n",
       "          [-0.3586,  0.1400,  0.8328,  ..., -1.4843,  0.9112, -0.9098],\n",
       "          [ 0.4963,  0.1693,  1.3659,  ..., -2.6998,  0.4735, -0.4187],\n",
       "          [ 0.0862, -0.1361,  0.4028,  ..., -1.8653,  1.2898, -1.1453],\n",
       "          [-0.8488,  0.4492,  0.1212,  ..., -2.3097,  1.5455, -0.7243]],\n",
       " \n",
       "         [[-0.4833, -0.2818,  0.0177,  ..., -0.9821, -0.0489,  0.2113],\n",
       "          [-0.0555,  0.6640,  0.3484,  ...,  0.1727,  0.1472, -2.4475],\n",
       "          [ 0.3708,  0.4623,  1.3900,  ..., -1.0469,  0.9198, -1.6652],\n",
       "          [ 0.0855,  0.6461,  1.6065,  ..., -0.1661,  1.0007, -1.5019],\n",
       "          [-0.5552,  1.0783,  0.8189,  ..., -0.5615,  1.2748, -1.1875]]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([[[ 0.8347,  0.3839,  0.3003,  ..., -1.4731, -0.2140, -0.1388],\n",
       "          [-0.5314,  0.2007,  0.9269,  ..., -1.3297,  0.8550, -1.0471],\n",
       "          [ 0.2730,  0.6409,  1.5009,  ..., -2.5274,  0.4339, -0.1372],\n",
       "          [-0.0376,  0.0278,  0.6540,  ..., -1.7176,  1.1877, -0.9302],\n",
       "          [-1.0574,  0.5659,  0.3970,  ..., -2.2058,  1.2530, -0.7118]],\n",
       " \n",
       "         [[-0.5220, -0.0888, -0.6445,  ..., -0.8544, -0.2217, -0.0443],\n",
       "          [-0.2584,  0.4025,  0.0739,  ...,  0.3329, -0.0160, -2.2697],\n",
       "          [-0.1682,  0.6888,  0.7571,  ..., -0.6123,  0.7652, -1.6005],\n",
       "          [ 0.0752,  0.7084,  0.8829,  ..., -0.0869,  0.8160, -1.2094],\n",
       "          [-0.6720,  0.9119,  0.1412,  ..., -0.4099,  1.0499, -1.0576]]],\n",
       "        grad_fn=<AddBackward0>)]"
      ]
     },
     "execution_count": 514,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "aad84871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 768])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_layers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "821014ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.num_attention_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "60c8d148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 5, 5])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_probs[0][].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "f9cffdd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc83766479ed473b8d278b8e1c96e6f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "832844fd304e4316977ba22b7dbd94e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c6b4f431db541f79b0a35ae22d9978e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36557953125141d8953d4e2745b2d5ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "e50c564e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids : tensor([[ 101, 7632, 2026, 2171, 2003, 4080,  102],\n",
      "        [ 101, 7632,  102,    0,    0,    0,    0]])\n",
      "token_type_ids : tensor([[0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0]])\n",
      "attention_mask : tensor([[1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(['Hi my name is Andrew', 'hi'], padding=True, return_tensors='pt').input_ids\n",
    "token_type_ids = tokenizer(['Hi my name is Andrew', 'hi'], padding=True, return_tensors='pt').token_type_ids\n",
    "attention_mask = tokenizer(['Hi my name is Andrew', 'hi'], padding=True, return_tensors='pt').attention_mask\n",
    "print('input_ids : {}'.format(input_ids))\n",
    "print('token_type_ids : {}'.format(token_type_ids))\n",
    "print('attention_mask : {}'.format(attention_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "9391dfb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_layers의 텐서 크기: torch.Size([2, 7, 768])\n",
      "pooled_output의 텐서 크기: torch.Size([2, 768])\n",
      "attention_probs의 텐서 크기: torch.Size([2, 12, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "# 순전파\n",
    "encoded_layers, pooled_output, attention_probs = net(\n",
    "    input_ids, token_type_ids, attention_mask, output_all_encoded_layers=True, attention_show_fig=True)\n",
    "\n",
    "print('encoded_layers의 텐서 크기:', encoded_layers[0].shape)\n",
    "print('pooled_output의 텐서 크기:', pooled_output.shape)\n",
    "print('attention_probs의 텐서 크기:', attention_probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "d595b41c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 7632, 2026, 2171, 2003, 4080, 102], [101, 7632, 102, 0, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 0, 0, 0, 0]]}"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['Hi my name is Andrew', 'hi'], padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "119c98f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'hi', 'my', 'name', 'is', 'andrew', '[SEP]']"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = tokenizer.decode(tokenizer.encode('Hi my name is Andrew')).split()\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "f77ee047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1200 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(4, 3, figsize=(12, 12), layout='constrained')\n",
    "\n",
    "# plt.matshow(attention_probs[0][0].detach())\n",
    "\n",
    "t = 0\n",
    "for x in range(4):\n",
    "    for y in range(3):\n",
    "        ax = axs[x][y]\n",
    "        ax.matshow(attention_probs[0][t].detach())\n",
    "        # plt.gca().xaxis.tick_bottom()\n",
    "        # ax.xaxis.tick_bottom()\n",
    "        ax.set_xticks(range(len(token)), token)\n",
    "        ax.set_yticks(range(len(token)), token)\n",
    "        t += 1\n",
    "        \n",
    "fig.suptitle('Attention score visualization', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0801e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74be2265",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "e5976f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-1.2298, -1.2095, -0.9367, -2.7928,  0.0649, -2.0504, -2.1590],\n",
       "        grad_fn=<SelectBackward0>),\n",
       " tensor([[ 0.1819, -1.2298,  0.2724,  ...,  0.5377,  0.5826, -0.4628],\n",
       "         [ 0.7754, -1.2095, -0.2225,  ..., -0.4277,  0.6661,  0.9166],\n",
       "         [ 0.7427, -0.9367, -0.1468,  ..., -2.0131,  0.9924, -0.3608],\n",
       "         ...,\n",
       "         [-0.1848,  0.0649,  0.9206,  ..., -0.8104,  0.8080,  1.3772],\n",
       "         [ 0.3353, -2.0504,  1.1867,  ..., -1.0225, -0.2646, -0.1179],\n",
       "         [ 1.1500, -2.1590, -0.6638,  ..., -0.8899,  1.0325,  0.1894]],\n",
       "        grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_layers[0][1][:, 1], encoded_layers[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8fdccd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286ea86f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067f0814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74496d20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
