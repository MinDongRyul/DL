{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7814163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfd8043",
   "metadata": {},
   "source": [
    "# BERT 구현\n",
    "### 학습 목표\n",
    "1. BERT의 Embeddings모듈 동작을 이해하고 구현할 수 있다.\n",
    "2. BERT의 Self-Attention을 활용한 Transformer 부분인 BertLayer모듈의 동작을 이해하고 구현할 수 있다.\n",
    "3. BERT의 Pooler모듈의 동작을 이해하고 구현할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaff8569",
   "metadata": {},
   "source": [
    "## 8.2.2 BERT_Base의 네트워크 설정 파일 읽기\n",
    "- 먼저 BERT_Base에서 Transformer가 12단인 것과 특징량 벡터가 768차원인 것 등을 적은 weights폴더의 네트워크 설정 파일 bert_config.json을 읽어들입니다.\n",
    "- 읽어들인 JSON파일의 사전형 변수에서 key 'hidden'값을 취하려면 config['hidden_size']로 적어야합니다. 이를 config.hidden_size로 기술하면 깔끔합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c0ec71b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attention_probs_dropout_prob': 0.1,\n",
       " 'hidden_act': 'gelu',\n",
       " 'hidden_dropout_prob': 0.1,\n",
       " 'hidden_size': 768,\n",
       " 'initializer_range': 0.02,\n",
       " 'intermediate_size': 3072,\n",
       " 'max_position_embeddings': 512,\n",
       " 'num_attention_heads': 12,\n",
       " 'num_hidden_layers': 12,\n",
       " 'type_vocab_size': 2,\n",
       " 'vocab_size': 30522}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# config.json에서 설정을 읽어들여 JSON 사전 변수를 오브젝트 변수로 변환\n",
    "import json\n",
    "\n",
    "config_file = './weights/bert_config.json'\n",
    "\n",
    "# 파일을 열어 JSON으로 읽는다.\n",
    "with open(config_file, 'r') as f:\n",
    "    config = json.load(f)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fbb4a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 사전 변수를 오브젝트 변수로\n",
    "from attrdict import AttrDict\n",
    "\n",
    "config = AttrDict(config)\n",
    "config.hidden_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8063c1c",
   "metadata": {},
   "source": [
    "## 8.2.3 BERT에 레이어 정규화 층 정의\n",
    "- BERT 모델 구축의 사전 준비로 레이어 정규화 층의 클래스를 정의합니다. 7장에서 사용한 것 처럼 파이토치에도 레이어 정규화가 있습니다.\n",
    "- 텐서플로와 파이토치에서는 레이어 정규화의 구현 방법이 약간 다릅니다. 텐서의 마지막 채널(즉 단어의 특징량 벡터 768차원)에 평균 0, 표준편차 1이 되도록 레이어 정규화를 수행합니다. 0으로 나누지 않도록 보조 항 엡실론을 넣는 방법은 파이토치와 텐서플로가 서로 다릅니다.\n",
    "- 이번에 사용할 학습된 모델은 구글이 공개한 텐서플로의 학습 결과에 기반하여 텐서플로 버전의 레이어 정규화 층을 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b22ec377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT용으로 레이어 정규화 층 정의\n",
    "# 세부 구현을 텐서플로에 맞춘다.\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class BertLayerNorm(nn.Module):\n",
    "    '''레이어 정규화 층'''\n",
    "    \n",
    "    def __init__(self, hidden_size, eps=1e-12):\n",
    "        super(BertLayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(hidden_size)) # weight에 대한 것\n",
    "        self.beta = nn.Parameter(torch.zeros(hidden_size)) # 바이어스에 대한 것\n",
    "        self.variance_epsilon = eps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        u = x.mean(-1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
    "        return self.gamma * x + self.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13acf22d",
   "metadata": {},
   "source": [
    "## 8.2.4 Embedding 구현\n",
    "### Transformer의 Embeddings 모듈과 두 가지 큰 차이점 존재\n",
    "- 첫째, Positional Embedding(위치 정보를 벡터로 변환)의 표현 기법을 Transformer는 sin, cos으로 계산하지만 BERT는 표현 방법도 학습시킵니다. 학습 시키는 것은 단어의 위치 정보뿐이며 단어 벡터의 차원 정보는 부여하지 않습니다. 즉 첫 번째 단어의 768차원은 동일한 position_embeddings값이 저장 되고 두 번째 단어는 첫 번째 단어와는 다르지만 768차원 방향에 같은 position_embeddings값이 저장됩니다.\n",
    "- 둘째, Sentence Embedding의 존재입니다. BERT는 두 문장을 입력합니다. 첫 번째 문장과 두 번째 문장을 구분하기 위한 Embedding를 준비합니다. Embeddings 모듈에서는 Token Embedding, Positional Embedding, Sentence Embedding에서 각각 구할 세 개의 텐서를 Transformer처럼 더하여 Embeddings 모듈의 출력으로 합니다. Embeddings모듈에 대한 입력 텐서는 (batch_size, seq_len)크기로 이루어진 문장의 단어 ID 나열인 변수 input_ids와 (batch_size, seq_len)의 각 단어가 첫 번째 문장인지 두 번째 문장인지 나타내는 문장 id인 변수 token_type_ids가 됩니다. 출력은 (batch_size, seq_len, hidden_size)의 텐서입니다. seq_len은 512이고 hidden_size는 768입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e1c4886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT의 Embeddings 모듈\n",
    "class BertEmbeddings(nn.Module):\n",
    "    '''문장의 단어 ID열과 첫 번째인지 두 번째 문장인지 정보를 내장 벡터로 변환'''\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(BertEmbeddings, self).__init__()\n",
    "        \n",
    "        # 세 개의 벡터 표현 내장\n",
    "        \n",
    "        # Token Embedding: 단어 ID를 단어 벡터로 변환\n",
    "        # vocab_size = 30522로 BERT의 학습된 모델에 사용된 vocabulary 양\n",
    "        # hidden_size = 768로 특징량 벡터의 길이는 768\n",
    "        self.word_embeddings = nn.Embedding(\n",
    "            config.vocab_size, config.hidden_size, padding_idx=0)\n",
    "        \n",
    "        # padding_idx = 0의 idx = 0 단어 벡터는 0으로 한다. BERT의 vocabulary의 idx=0은 [PAD]이다\n",
    "        \n",
    "        # Transformer Positional Embedding: 위치 정보 텐서를 벡터로 변환\n",
    "        # Transformer의 경우는 sin, cos로 이루어진 고정 값이지만 BERT는 학습시킨다.\n",
    "        # max_position_embeddings = 512로 문장 길이는 512단어\n",
    "        self.position_embeddings = nn.Embedding(\n",
    "            config.max_position_embeddings, config.hidden_size)\n",
    "        \n",
    "        # Sentence Embedding: 첫 번째, 두 번째 문장을 벡터로 변환\n",
    "        # type_vocab_size = 2\n",
    "        self.token_type_embeddings = nn.Embedding(\n",
    "            config.type_vocab_size, config.hidden_size)\n",
    "        \n",
    "        # 작성한 레이어 정규화 층\n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
    "        \n",
    "        # 드롭아웃 'hidden_dropout_prob':0.1\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        \n",
    "    def forward(self, input_ids, token_type_ids=None):\n",
    "        '''\n",
    "        input_ids: [batch_size, seq_len] 문장의 단어 ID 나열\n",
    "        token_type_ids: [batch_size, seq_len] 각 단어가 첫 번째 문장인지 두 번째 문장인지 나타내는 id\n",
    "        '''\n",
    "        \n",
    "        # 1. Token Embeddings\n",
    "        # 단어 ID를 단어 벡터로 변환\n",
    "        words_embeddings = self.word_embeddings(input_ids)\n",
    "        \n",
    "        # 2. Sentence Embeddings\n",
    "        # token_type_ids가 없는 경우는 문장의 모든 단어를 첫 번째 문장으로 하여 0으로 설정\n",
    "        # input_ids와 같은 크기릐 제로 텐서 작성\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "        \n",
    "        # 3. Transformer Positional Embedding:\n",
    "        # [0, 1, 2, ...]로 문장의 길이 만큼 숫자가 하나씩 올라간다.\n",
    "        # [batch_size, seq_len]의 텐서 positional_ids 작성\n",
    "        # positional_ids를 입력하여 position_embeddings 층에서 768차원의 텐서를 꺼낸다\n",
    "        seq_length = input_ids.size(1) # 문장 길이\n",
    "        position_ids = torch.arange(\n",
    "            seq_length, dtype=torch.long, device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        \n",
    "        # 세 개의 내장 텐서를 더한다. [batch_size, seq_len, hidden_size]\n",
    "        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n",
    "        \n",
    "        # 레이어 정규화와 드롭아웃 실행\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        \n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4aa2d4d",
   "metadata": {},
   "source": [
    "## 8.2.5 BertLayer 모듈\n",
    "- BertLayer는 Transformer부분에 해당.\n",
    "- 서브 네트워크로서 Self-Attention을 계산하는 BertAttention과 Self-Attention의 출력을 처리하는 전결합 층인 BertIntermediate, 그리고 Self-Attention 출력과 BertIntermediate에서 처리한 특징량을 더하는 BertOutput 세 가지로 구성됩니다.\n",
    "- BertLayer에 대한 입력은 Embedding 모듈의 출력 또는 앞단의 BertLayer에서의 출력이며 크기는 (batch_size, seq_len, hidden_size)입니다. - BertLayer구현에서 7장 Transformer와 두 가지 다른 점이 있습니다.\n",
    "- 첫째, BertIntermediate 전결합 층 뒤의 활성화 함수에 GELU함수를 사용하는 점입니다. GELU는 기본적으로 RELU와 같은 형태의 함수입니다. 입력이 0이지만 ReLU출력이 거친(매끄러운 변화가 아니라 급격환 변화) 반면 GELU는 입력 0 근처의 출력이 매끄러운 형태입니다.\n",
    "- 둘쩨, Attention이 Multi-Headed Self-Attention입니다. Transformer도 Multi-Headed Self-Attention이지만 7장에서는 이해를 돕기 위하여 단일 Self-Attention으로 구현하였습니다. Multi-Headed Self-Attention은 단순히 Self-Attention이 여러 개 있는것 뿐입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35c9fe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(nn.Module):\n",
    "    '''BERT의 BertLayer모듈이다. Transformer가 된다.'''\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(BertLayer, self).__init__()\n",
    "        \n",
    "        # Self-Attention 부분\n",
    "        self.attention = BertAttention(config)\n",
    "        \n",
    "        # Self-Attention의 출력을 처리하는 전결합 층\n",
    "        self.intermediate = BertIntermediate(config)\n",
    "        \n",
    "        # Self-Attention에 의한 특징량과 BertLayer에 원래의 입력을 더하는 층\n",
    "        self.output = BertOutput(config)\n",
    "        \n",
    "    def forward(self, hidden_states, attention_mask, attention_show_fig=False):\n",
    "        '''\n",
    "        hidden_states : Embedder 모듈의 출력 텐서 [batch_size, seq_len, hidden_size]\n",
    "        attention_mask : Transformer의 마스크와 같은 기능의 마스킹\n",
    "        attention_show_fig : Self-Attention의 가중치를 반환할지의 플래그\n",
    "        '''\n",
    "        if attention_show_fig == True:\n",
    "            '''attention_show일 경우 attention_probs도 반환한다.'''\n",
    "            attention_output, attention_probs = self.attention(\n",
    "                hidden_states, attention_mask, attention_show_fig)\n",
    "            intermediate_output = self.intermediate(attention_output)\n",
    "            layer_output = self.output(intermediate_output, attention_output)\n",
    "            return layer_output, attention_probs\n",
    "        \n",
    "        elif attention_show_fig == False:\n",
    "            attention_output = self.attention(\n",
    "                hidden_states, attention_mask, attention_show_fig)\n",
    "            intermediate_output = self.intermediate(attention_output)\n",
    "            layer_output = self.output(intermediate_output, attention_output)\n",
    "            return layer_output # [batch_size, seq_length, hidden_size]\n",
    "        \n",
    "class BertAttention(nn.Module):\n",
    "    '''BertLayer 모듈의 Self-Attention 부분'''\n",
    "    def __init__(self, config):\n",
    "        super(BertAttention, self).__init__()\n",
    "        self.selfattn = BertSelfAttention(config)\n",
    "        self.output = BertSelfOutput(config)\n",
    "        \n",
    "    def forward(self, input_tensor, attention_mask, attention_show_fig=False):\n",
    "        '''\n",
    "        input_tensor : Embeddings 모듈 또는 앞단의 BertLayer에서의 출력\n",
    "        attention_mask : Transformer의 마스크와 같은 기능의 마스킹\n",
    "        attention_show_fig : Self-Attention의 가중치를 반환할지의 플래그\n",
    "        '''\n",
    "        if attention_show_fig == True:\n",
    "            '''attention_show일 경우 attention_probs도 반환한다.'''\n",
    "            self_output, attention_probs = self.selfattn(input_tensor, attention_mask, attention_show_fig)\n",
    "            attention_output = self.output(self_output, input_tensor)\n",
    "            return attention_output, attention_probs\n",
    "        \n",
    "        elif attention_show_fig == False:\n",
    "            self_output = self.selfattn(input_tensor, attention_mask, attention_show_fig)\n",
    "            attention_output = self.output(self_output, input_tensor)\n",
    "            return attention_output\n",
    "        \n",
    "class BertSelfAttention(nn.Module):\n",
    "    '''BertAttention의 Self-Attention이다'''\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(BertSelfAttention, self).__init__()\n",
    "        \n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        # num_attention_heads = 12\n",
    "        \n",
    "        self.attention_head_size = int(\n",
    "            config.hidden_size / config.num_attention_heads) # 768 / 12 = 64\n",
    "        self.all_head_size = self.num_attention_heads * \\\n",
    "            self.attention_head_size # = 'hidden_size' : 768\n",
    "        \n",
    "        # Self-Attention의 특징량을 작성하는 전결합 층\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        \n",
    "        # drop out\n",
    "        self.drop_out = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "        \n",
    "    def transpose_for_scores(self, x):\n",
    "        '''Multi-Headed Attention용으로 텐서의 형태 변환\n",
    "        [batch_size, seq_len, hidden] -> [batch_size, 12, seq_len, hidden/12]\n",
    "        '''\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "    \n",
    "    def forward(self, hidden_states, attention_mask, attention_show_fig=False):\n",
    "        '''\n",
    "        hidden_states : Embeddings 모듈 또는 앞단의 BertLayer에서의 출력\n",
    "        attention_mask : Transformer의 마스크와 같은 기능의 마스킹\n",
    "        attention_show_fig : Self-Attention의 가중치를 반환할지 플래그\n",
    "        '''\n",
    "        \n",
    "        # 입력의 전결합 층에서 특징량 변환(Multi-headed Attention 전부 한꺼번에 변환)\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "        \n",
    "        # Multi-Headed Attention용으로 텐서 형태 변환\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "        \n",
    "        # 특징량끼리 곱하여 비슷한 정도를 Attention_scores로 구한다.\n",
    "        attention_scores = torch.matmul(\n",
    "            query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / \\\n",
    "            math.sqrt(self.attention_head_size)\n",
    "        \n",
    "        # 마스크가 있는 부분에 마스크 적용\n",
    "        attention_scores = attention_scores + attention_mask\n",
    "        # 마스크는 곰셈이 아니라 덧셈이 직관적이지만 그 후에 소프트맥스로 정규화하므로\n",
    "        # 마스크된 부분은 -inf로 한다. attention_mask에는 원래 0이나 -inf가 있으므로 덧셈으로 한다.\n",
    "        \n",
    "        # Attention 정규화\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "        \n",
    "        # 드롭아웃\n",
    "        attention_probs = self.drop_out(attention_probs)\n",
    "        \n",
    "        # Attention Map을 곱한다.\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        \n",
    "        # Multi-Headed Attention의 텐서 형태를 원래대로 되돌린다.\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "        \n",
    "        # attention_show일 경우 attention_probs도 반환\n",
    "        if attention_show_fig == True:\n",
    "            return context_layer, attention_probs\n",
    "        elif attention_show_fig == False:\n",
    "            return context_layer\n",
    "        \n",
    "class BertSelfOutput(nn.Module):\n",
    "    '''BertSelfAttention의 출력을 처리하는 전결합 층이다'''\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(BertSelfOutput, self).__init__()\n",
    "        \n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        # 'hidden_dropout_prob': 0.1\n",
    "        \n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        '''\n",
    "        hidden_stats : BertSelfAttention의 출력 텐서\n",
    "        input_tensor : Embeddings 모듈 또는 앞단의 BertLayer에서의 출력\n",
    "        '''\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n",
    "    \n",
    "def gelu(x):\n",
    "    '''Gaussian Error Linear Unit라는 활성화 함수이다\n",
    "    ReLU가 0으로 거칠고 불연속적이므로 연속적으로 매끄럽게 한 셩태의 ReLU이다\n",
    "    '''\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
    "        \n",
    "class BertIntermediate(nn.Module):\n",
    "    '''BERT의 TransformerBlock 모듈 FeedForward'''\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(BertIntermediate, self).__init__()\n",
    "        \n",
    "        # 전결합 층 : 'hidden_size': 768, 'intermediate_size': 3072\n",
    "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        # 활성화 함수\n",
    "        self.intermediate_act_fn = gelu\n",
    "        \n",
    "    def forward(self, hidden_states):\n",
    "        '''\n",
    "        hidden_states : BertAttention의 출력\n",
    "        '''\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.intermediate_act_fn(hidden_states) # GELU에 의한 활성화\n",
    "        return hidden_states\n",
    "    \n",
    "class BertOutput(nn.Module):\n",
    "    '''BERT의 TransformerBlock 모듈 FeedForward'''\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(BertOutput, self).__init__()\n",
    "        \n",
    "        # 전결합 층 : 'intermediate_size': 3072, 'hidden_size': 768\n",
    "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        \n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
    "        \n",
    "        # 'hidden_dropout_prob': 0.1\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        \n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        '''\n",
    "        hidden_states : BertIntermediate 출력 텐서\n",
    "        input_tensor : BertAttention 출력 텐서\n",
    "        '''\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f976ed8",
   "metadata": {},
   "source": [
    "## 8.2.6 BertLayer 모듈의 반복 부분\n",
    "- BERT_Base에서는 BertLayer 모듈(Transformer)을 12회 반복.\n",
    "- 이들을 묶어서 BertEncoder클래스로 만듭니다.\n",
    "- 단순히 BertLayer 12개를 nn.ModuleList에 기재하여 순전파.\n",
    "- 순전파 함수 forward의 인수\n",
    "    - output_all_encoded_layer인수는 반환 값으로 BertLayer에서 출력된 특징량을 12단만큼 모두 반환할지 아니면 12단 최종 층의 특징량만 반환할지 여부를 지정하는 변수.\n",
    "    - 12단의 Transformer 중간에 단어 벡터가 어떻게 변해가는지 확인하고 싶을 때 output_all_encoded_layers인수를 True로 하여 12단 만큼의 단어 벡터를 꺼낼 수 있습니다.\n",
    "    - 단순히 12단 출력만을 사용하여 자연어 처리를 작업하는 경우 False로 하여 최종 BertLayer 모듈 출력만 BertEncoder에서 출력시킨 후 사용\n",
    "    - attention_show_fig인수는 BertLayer 모듈에서 사용했던 변수와 동일\n",
    "    - Self-Attention의 가중치를 출력할지 여부를 지정합니다. BERT_Base의 Attention은 각 층이 12개인 Multi-headed Self-Attention입니다.\n",
    "    - BertEncoder에서 attention_show_fig인수를 True로 한 경우에는 BertLayer 모듈 중 12단 끝에 있는 BertLayer 모듈에서 12개의 Multi-Headed Self-Attention가중치를 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ea6fcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BertLayer 모듈의 반복 부분이다.\n",
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        '''BertLayer 모듈의 반복 부분'''\n",
    "        super(BertEncoder, self).__init__()\n",
    "        \n",
    "        # config.num_hidden_layers의 값, 즉 12개의 BertLayer 모듈을 만든다.\n",
    "        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        \n",
    "    def forward(self, hidden_states, attention_mask, output_all_encoded_layers=True, attention_show_fig=False):\n",
    "        '''\n",
    "        hidden_states : Embeddings 모듈 출력\n",
    "        attention_mask : Transformer의 마스크와 동일한 기능의 마스킹\n",
    "        output_all_encoded_layers : 반환 값을 전체 TransformerBlock 모듈의 출력으로 할지 마지막 층만으로 한정할지의 플래그\n",
    "        attention_show_fig : Self-Attention의 가중치를 반환할지의 플래그\n",
    "        '''\n",
    "        \n",
    "        # 반환 값으로 사용할 리스트\n",
    "        all_encoder_layers = []\n",
    "        \n",
    "        # BertLayer 모듈의 처리 반복\n",
    "        for layer_module in self.layer:\n",
    "            \n",
    "            if attention_show_fig == True:\n",
    "                '''attention_show의 경우 attention_probs도 반환'''\n",
    "                hidden_states, attention_probs = layer_module(\n",
    "                    hidden_states, attention_mask, attention_show_fig)\n",
    "            elif attention_show_fig == False:\n",
    "                hidden_states = layer_module(\n",
    "                    hidden_states, attention_mask, attention_show_fig)\n",
    "                \n",
    "            # 반환 값으로 BertLayer에서 출력된 특징량만을 사용할 경우의 처리\n",
    "            if output_all_encoded_layers:\n",
    "                all_encoder_layers.append(hidden_states)\n",
    "                    \n",
    "        # 반환 값으로 마지막 BertLayer에서 출력된 특징량만을 사용할 경우의 처리\n",
    "        if not output_all_encoded_layers:\n",
    "            all_encoder_layers.append(hidden_states)\n",
    "            \n",
    "        # attention_show의 경우 attention_probs(마지막 12단)도 반환한다.\n",
    "        if attention_show_fig == True:\n",
    "            return all_encoder_layers, attention_probs\n",
    "        elif attention_show_fig == False:\n",
    "            return all_encoder_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af4ec36",
   "metadata": {},
   "source": [
    "## 8.2.7 BertPooler 모듈\n",
    "- BertPooler 모듈은 BertEncoder출력에서 입력 문장의 첫 번째 단어인 [CLS]부분의 특징량 텐서(1 x 768차원)을 꺼내 전결합 층을 사용한 후 특징량을 변환하는 모듈입니다.\n",
    "- 전결합 층 뒤에 활성화 함수 Tanh을 사용하고 출력을 1에서 -1까지 범위로 합니다. 출력 텐서의 크기는 (batch_size, hidden_size)입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41b57475",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPooler(nn.Module):\n",
    "    '''입력 문장의 첫 번째 단어 [cls]의 특징량을 반환하고 유지하기 위한 모듈'''\n",
    "    def __init__(self, config):\n",
    "        super(BertPooler, self).__init__()\n",
    "        \n",
    "        # 전결합 층, 'hidden_size':768\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "        \n",
    "    def forward(self, hidden_states):\n",
    "        # 첫 번째 단어의 특징량 취득\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        \n",
    "        # 전결합 층에서 특징량 변환\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        \n",
    "        # 활성화 함수 Tanh을 계산\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        \n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86da244d",
   "metadata": {},
   "source": [
    "## 8.2.8 동작 확인\n",
    "- 미니 배치의 크기를 2, 각 미니 배치의 문장 길이르 5로 하여 입력을 적당히 생성\n",
    "- 길이 5에 두 문장이 포함되어 있음. 어떠한 단어까지 첫 번째 문장이고 어떠한 단어부터 두 번째 문장인지 나타내는 문장 ID와 Attention용 마스크도 생성. 이러한 입력으로 동작을 확인\n",
    "- Attention용 마스크를 확장한 extended_attention_mask 변수를 작성한다는 점을 주의해야 한다\n",
    "- Multi-Headed Self-Attention에서 Attention마스크를 사용할 수 있도록 하는 변환.\n",
    "- Attention을 적용하지 않는 부분은 시그모이드를 계산했을 때 0이 되도록 마이너스 무한의 대안으로써 -10000을 대입."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d97169e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 단어 ID열의 텐서 크기:  torch.Size([2, 5])\n",
      "입력 마스크의 텐서 크기:  torch.Size([2, 5])\n",
      "입력 문장 ID의 텐서 크기:  torch.Size([2, 5])\n",
      "확장된 마스크의 텐서 크기:  torch.Size([2, 1, 1, 5])\n",
      "BertEmbeddings의 출력 텐서 크기: torch.Size([2, 5, 768])\n",
      "BertEncoder 최후 층의 출력 텐서 크기: torch.Size([2, 5, 768])\n",
      "BertPooler의 출력 텐서 크기: torch.Size([2, 768])\n"
     ]
    }
   ],
   "source": [
    "# 동작 확인\n",
    "\n",
    "# 입력 단어 ID열 batch_size는 두 가지\n",
    "input_ids = torch.LongTensor([[31, 51, 12, 23, 99], [15, 5, 1, 0, 0]])\n",
    "print('입력 단어 ID열의 텐서 크기: ', input_ids.shape)\n",
    "# 마스크\n",
    "attention_mask = torch.LongTensor([[1, 1, 1, 1, 1], [1, 1, 1, 0, 0]])\n",
    "print('입력 마스크의 텐서 크기: ', attention_mask.shape)\n",
    "\n",
    "# 문장의 ID, 두 미니 배치 각각에 대한 0은 첫 번째 문장을, 1은 두 번째 문장을 나타낸다.\n",
    "token_type_ids = torch.LongTensor([[0, 0, 1, 1, 1], [0, 1, 1, 1, 1]])\n",
    "print('입력 문장 ID의 텐서 크기: ', token_type_ids.shape)\n",
    "\n",
    "# BERT의 각 모듈 준비\n",
    "embeddings = BertEmbeddings(config)\n",
    "encoder = BertEncoder(config)\n",
    "pooler = BertPooler(config)\n",
    "\n",
    "# 마스크 변형 [batch_size, 1, 1, seq_length]로 한다.\n",
    "# Attention을 적용하지 않는 부분은 마이너스 무한으로 하고 위하여 -10000을 곱한다.\n",
    "extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "extended_attention_mask = extended_attention_mask.to(dtype=torch.float32)\n",
    "extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "print('확장된 마스크의 텐서 크기: ', extended_attention_mask.shape)\n",
    "\n",
    "# 순전파\n",
    "out1 = embeddings(input_ids, token_type_ids)\n",
    "print('BertEmbeddings의 출력 텐서 크기:', out1.shape)\n",
    "\n",
    "out2 = encoder(out1, extended_attention_mask)\n",
    "# out2는 [minibatch, seq_length, embedding_dim]이 12개 리스트\n",
    "print('BertEncoder 최후 층의 출력 텐서 크기:', out2[0].shape)\n",
    "\n",
    "out3 = pooler(out2[-1]) # out2는 12층의 특징량 리스트가 되어 가장 마지막을 사용\n",
    "print('BertPooler의 출력 텐서 크기:', out3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110843aa",
   "metadata": {},
   "source": [
    "## 8.2.9 모두 연결하여 BERT모델로\n",
    "- 동작을 확인한 후 문제가 없다면 모두 연결한 BERT 모델로 지정."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89d406c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(nn.Module):\n",
    "    '''모듈을 전부 연결한 BERT 모델'''\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(BertModel, self).__init__()\n",
    "        \n",
    "        # 세 가지 모듈 작성\n",
    "        self.embeddings = BertEmbeddings(config)\n",
    "        self.encoder = BertEncoder(config)\n",
    "        self.pooler = BertPooler(config)\n",
    "        \n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, output_all_encoded_layers=True,\n",
    "                attention_show_fig=False):\n",
    "        '''\n",
    "        input_ids : [batch_size, sequence_length] 문장의 단어 ID 나열\n",
    "        token_type_ids : [batch_size, sequence_length] 각 단어가 첫 번째 문장인지 두 번째 문장인지 나타내는 id\n",
    "        attention_mask: Transformer의 마스크와 같은 기능의 마스킹\n",
    "        output_all_encoded_layers : 마지막 출력에 12단의 Transformer 모두 리스트로 반환할지 마지막만인지 지정\n",
    "        attention_show_fig : Self-Attention의 가중치를 반환할지 플래그\n",
    "        '''\n",
    "        \n",
    "        # Attention 마스크와 첫 번째, 두 번째 문장의 id가 없으면 작성\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.one_like(input_ids)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "            \n",
    "        # 마스크 변형 [minibatch, 1, 1, seq_length]로 한다.\n",
    "        # 나중에 Multi-Headed Self-Attention에서 사용할 수 있는 형태로 하기 위하여\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "        \n",
    "        # 마스크는 0, 1 이지만 소프트맥스를 계산할 때 마스크가 되도록 0과 -inf로 한다.\n",
    "        # -inf 대신 -10000으로 한다\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=torch.float32)\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "        \n",
    "        # 순전파 시킨다\n",
    "        # BertEmbeddings 모듈\n",
    "        embedding_output = self.embeddings(input_ids, token_type_ids)\n",
    "        \n",
    "        # BertLayer 모듈(Transformer)을 반복하는 BertEncoder 모듈\n",
    "        if attention_show_fig == True:\n",
    "            '''attention_show의 경우 attention_probs도 반환'''\n",
    "            \n",
    "            encoded_layers, attention_probs = self.encoder(embedding_output,\n",
    "                                                           extended_attention_mask,\n",
    "                                                           output_all_encoded_layers,\n",
    "                                                           attention_show_fig)\n",
    "            \n",
    "        elif attention_show_fig == False:\n",
    "            encoded_layers = self.encoder(embedding_output,\n",
    "                                          extended_attention_mask,\n",
    "                                          output_all_encoded_layers,\n",
    "                                          attention_show_fig)\n",
    "            \n",
    "        # BertPooler 모듈\n",
    "        # 인코더의 맨 마지막 BertLayer에서 출력된 특징량 사용\n",
    "        pooled_output = self.pooler(encoded_layers[-1])\n",
    "        \n",
    "        # output_all_encoded_layer가 False인 경우는 리스트가 아닌 텐서를 반환\n",
    "        if not output_all_encoded_layers:\n",
    "            encoded_layers = encoded_layers[-1]\n",
    "            \n",
    "        # attention_show의 경우 attention_probs(가장 마지막)도 반환한다.\n",
    "        if attention_show_fig == True:\n",
    "            return encoded_layers, pooled_output, attention_probs\n",
    "        elif attention_show_fig == False:\n",
    "            return encoded_layers, pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b175104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_layers의 텐서 크기: torch.Size([2, 5, 768])\n",
      "pooled_output의 텐서 크기: torch.Size([2, 768])\n",
      "attention_probs의 텐서 크기: torch.Size([2, 12, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "# 동작 확인\n",
    "# 입력 준비\n",
    "input_ids = torch.LongTensor([[31, 52, 12, 23, 99], [15, 5, 1, 0, 0]])\n",
    "attention_mask = torch.LongTensor([[1, 1, 1, 1, 1], [1, 1, 1, 0, 0]])\n",
    "token_type_ids = torch.LongTensor([[0, 0, 1, 1, 1], [0, 1, 1, 1, 1]])\n",
    "\n",
    "# BERT 모델을 만든다.\n",
    "net = BertModel(config)\n",
    "\n",
    "# 순전파\n",
    "encoded_layers, pooled_output, attention_probs = net(\n",
    "    input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False, attention_show_fig=True)\n",
    "\n",
    "print('encoded_layers의 텐서 크기:', encoded_layers.shape)\n",
    "print('pooled_output의 텐서 크기:', pooled_output.shape)\n",
    "print('attention_probs의 텐서 크기:', attention_probs.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
