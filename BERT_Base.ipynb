{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "a7814163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchsummary as ts\n",
    "import torchinfo as ti\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfd8043",
   "metadata": {},
   "source": [
    "# BERT 구현\n",
    "### 학습 목표\n",
    "1. BERT의 Embeddings모듈 동작을 이해하고 구현할 수 있다.\n",
    "2. BERT의 Self-Attention을 활용한 Transformer 부분인 BertLayer모듈의 동작을 이해하고 구현할 수 있다.\n",
    "3. BERT의 Pooler모듈의 동작을 이해하고 구현할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaff8569",
   "metadata": {},
   "source": [
    "## 8.2.2 BERT_Base의 네트워크 설정 파일 읽기\n",
    "- 먼저 BERT_Base에서 Transformer가 12단인 것과 특징량 벡터가 768차원인 것 등을 적은 weights폴더의 네트워크 설정 파일 bert_config.json을 읽어들입니다.\n",
    "- 읽어들인 JSON파일의 사전형 변수에서 key 'hidden'값을 취하려면 config['hidden_size']로 적어야합니다. 이를 config.hidden_size로 기술하면 깔끔합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c0ec71b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'architectures': ['BertForMaskedLM'],\n",
       " 'attention_probs_dropout_prob': 0.1,\n",
       " 'gradient_checkpointing': False,\n",
       " 'hidden_act': 'gelu',\n",
       " 'hidden_dropout_prob': 0.1,\n",
       " 'hidden_size': 768,\n",
       " 'initializer_range': 0.02,\n",
       " 'intermediate_size': 3072,\n",
       " 'layer_norm_eps': 1e-12,\n",
       " 'max_position_embeddings': 512,\n",
       " 'model_type': 'bert',\n",
       " 'num_attention_heads': 12,\n",
       " 'num_hidden_layers': 12,\n",
       " 'pad_token_id': 0,\n",
       " 'position_embedding_type': 'absolute',\n",
       " 'transformers_version': '4.6.0.dev0',\n",
       " 'type_vocab_size': 2,\n",
       " 'use_cache': True,\n",
       " 'vocab_size': 30522}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# config.json에서 설정을 읽어들여 JSON 사전 변수를 오브젝트 변수로 변환\n",
    "import json\n",
    "\n",
    "config_file = './weights/bert_config.json'\n",
    "\n",
    "# 파일을 열어 JSON으로 읽는다.\n",
    "with open(config_file, 'r') as f:\n",
    "    config = json.load(f)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fbb4a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 사전 변수를 오브젝트 변수로\n",
    "from attrdict import AttrDict\n",
    "\n",
    "config = AttrDict(config)\n",
    "config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abcf9762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8063c1c",
   "metadata": {},
   "source": [
    "## 8.2.3 BERT에 레이어 정규화 층 정의\n",
    "- BERT 모델 구축의 사전 준비로 레이어 정규화 층의 클래스를 정의합니다. 7장에서 사용한 것 처럼 파이토치에도 레이어 정규화가 있습니다.\n",
    "- 텐서플로와 파이토치에서는 레이어 정규화의 구현 방법이 약간 다릅니다. 텐서의 마지막 채널(즉 단어의 특징량 벡터 768차원)에 평균 0, 표준편차 1이 되도록 레이어 정규화를 수행합니다. 0으로 나누지 않도록 보조 항 엡실론을 넣는 방법은 파이토치와 텐서플로가 서로 다릅니다.\n",
    "- 이번에 사용할 학습된 모델은 구글이 공개한 텐서플로의 학습 결과에 기반하여 텐서플로 버전의 레이어 정규화 층을 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b22ec377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT용으로 레이어 정규화 층 정의\n",
    "# 세부 구현을 텐서플로에 맞춘다.\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class BertLayerNorm(nn.Module):\n",
    "    '''레이어 정규화 층'''\n",
    "    \n",
    "    def __init__(self, hidden_size, eps=1e-12):\n",
    "        super(BertLayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(hidden_size)) # weight에 대한 것\n",
    "        self.beta = nn.Parameter(torch.zeros(hidden_size)) # 바이어스에 대한 것\n",
    "        self.variance_epsilon = eps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 평균\n",
    "        u = x.mean(-1, keepdim=True)\n",
    "        # 분산\n",
    "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
    "        # 일반 정규화 적용 variance_epsilon은 0으로 나누지 않도록 보조 항 삽입\n",
    "        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
    "        # 최종적으로 gamma 와 x를 곱하고 beta를 더해서 return\n",
    "        return self.gamma * x + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba3941ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "BertLayerNorm                            1,536\n",
       "=================================================================\n",
       "Total params: 1,536\n",
       "Trainable params: 1,536\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
    "ti.summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e2adbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ts.summary(model, input_size=(10, 768))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13acf22d",
   "metadata": {},
   "source": [
    "## 8.2.4 Embedding 구현\n",
    "### Transformer의 Embeddings 모듈과 두 가지 큰 차이점 존재\n",
    "- 첫째, Positional Embedding(위치 정보를 벡터로 변환)의 표현 기법을 Transformer는 sin, cos으로 계산하지만 BERT는 표현 방법도 학습시킵니다. 학습 시키는 것은 단어의 위치 정보뿐이며 단어 벡터의 차원 정보는 부여하지 않습니다. 즉 첫 번째 단어의 768차원은 동일한 position_embeddings값이 저장 되고 두 번째 단어는 첫 번째 단어와는 다르지만 768차원 방향에 같은 position_embeddings값이 저장됩니다.\n",
    "- 둘째, Sentence Embedding의 존재입니다. BERT는 두 문장을 입력합니다. 첫 번째 문장과 두 번째 문장을 구분하기 위한 Embedding를 준비합니다. Embeddings 모듈에서는 Token Embedding, Positional Embedding, Sentence Embedding에서 각각 구할 세 개의 텐서를 Transformer처럼 더하여 Embeddings 모듈의 출력으로 합니다. Embeddings모듈에 대한 입력 텐서는 (batch_size, seq_len)크기로 이루어진 문장의 단어 ID 나열인 변수 input_ids와 (batch_size, seq_len)의 각 단어가 첫 번째 문장인지 두 번째 문장인지 나타내는 문장 id인 변수 token_type_ids가 됩니다. 출력은 (batch_size, seq_len, hidden_size)의 텐서입니다. seq_len은 512이고 hidden_size는 768입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e1c4886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT의 Embeddings 모듈\n",
    "class BertEmbeddings(nn.Module):\n",
    "    '''문장의 단어 ID열과 첫 번째인지 두 번째 문장인지 정보를 내장 벡터로 변환'''\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(BertEmbeddings, self).__init__()\n",
    "        \n",
    "        # 세 개의 벡터 표현 내장\n",
    "        \n",
    "        # Token Embedding: 단어 ID를 단어 벡터로 변환\n",
    "        # vocab_size = 30522로 BERT의 학습된 모델에 사용된 vocabulary 양\n",
    "        # hidden_size = 768로 특징량 벡터의 길이는 768\n",
    "        self.word_embeddings = nn.Embedding(\n",
    "            config.vocab_size, config.hidden_size, padding_idx=0)\n",
    "        \n",
    "        # padding_idx = 0의 idx = 0 단어 벡터는 0으로 한다. BERT의 vocabulary의 idx=0은 [PAD]이다\n",
    "        \n",
    "        # Transformer Positional Embedding: 위치 정보 텐서를 벡터로 변환\n",
    "        # Transformer의 경우는 sin, cos로 이루어진 고정 값이지만 BERT는 학습시킨다.\n",
    "        # max_position_embeddings = 512로 문장 길이는 512단어\n",
    "        self.position_embeddings = nn.Embedding(\n",
    "            config.max_position_embeddings, config.hidden_size)\n",
    "        \n",
    "        # Sentence Embedding: 첫 번째, 두 번째 문장을 벡터로 변환\n",
    "        # type_vocab_size = 2\n",
    "        self.token_type_embeddings = nn.Embedding(\n",
    "            config.type_vocab_size, config.hidden_size)\n",
    "        \n",
    "        # 작성한 레이어 정규화 층\n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
    "        \n",
    "        # 드롭아웃 'hidden_dropout_prob':0.1\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        \n",
    "    def forward(self, input_ids, token_type_ids=None):\n",
    "        '''\n",
    "        input_ids: [batch_size, seq_len] 문장의 단어 ID 나열\n",
    "        token_type_ids: [batch_size, seq_len] 각 단어가 첫 번째 문장인지 두 번째 문장인지 나타내는 id\n",
    "        '''\n",
    "        \n",
    "        # 1. Token Embeddings\n",
    "        # 단어 ID를 단어 벡터로 변환\n",
    "        words_embeddings = self.word_embeddings(input_ids)\n",
    "        \n",
    "        # 2. Sentence Embeddings\n",
    "        # token_type_ids가 없는 경우는 문장의 모든 단어를 첫 번째 문장으로 하여 0으로 설정\n",
    "        # input_ids와 같은 크기로 제로 텐서 작성\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "        \n",
    "        # 3. Transformer Positional Embedding:\n",
    "        # [0, 1, 2, ...]로 문장의 길이 만큼 숫자가 하나씩 올라간다.\n",
    "        # [batch_size, seq_len]의 텐서 positional_ids 작성\n",
    "        # positional_ids를 입력하여 position_embeddings 층에서 768차원의 텐서를 꺼낸다\n",
    "        seq_length = input_ids.size(1) # 문장 길이\n",
    "        position_ids = torch.arange(\n",
    "            seq_length, dtype=torch.long, device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        \n",
    "        # 세 개의 내장 텐서를 더한다. [batch_size, seq_len, hidden_size]\n",
    "        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n",
    "        \n",
    "        # 레이어 정규화와 드롭아웃 실행\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        \n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "34f3a35a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "BertEmbeddings                           --\n",
       "├─Embedding: 1-1                         23,440,896\n",
       "├─Embedding: 1-2                         393,216\n",
       "├─Embedding: 1-3                         1,536\n",
       "├─BertLayerNorm: 1-4                     1,536\n",
       "├─Dropout: 1-5                           --\n",
       "=================================================================\n",
       "Total params: 23,837,184\n",
       "Trainable params: 23,837,184\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_model = BertEmbeddings(config)\n",
    "ti.summary(emb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "355c15b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = torch.LongTensor([[31, 51, 12, 23, 99], [15, 5, 1, 0, 0]])\n",
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4ad15e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_type_ids = torch.LongTensor([[0, 0, 1, 1, 1], [0, 1, 1, 1, 1]])\n",
    "token_type_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "723f304c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 7632, 2026, 2171, 2003, 4080,  102]])"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "a7516ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 7, 768]),\n",
       " tensor([[[ 1.0441, -1.0624, -0.0035,  ...,  0.4908,  0.4285,  0.1971],\n",
       "          [ 0.1484, -0.3204, -1.8787,  ...,  0.3521, -0.4409,  0.8620],\n",
       "          [ 1.1590, -1.1851, -0.6478,  ...,  0.0799, -0.9569, -0.3837],\n",
       "          ...,\n",
       "          [ 1.7973, -0.0000, -2.3744,  ...,  0.0000,  1.3792,  1.4229],\n",
       "          [-0.1955, -1.1131, -0.6789,  ...,  0.7324, -0.2080, -0.1860],\n",
       "          [ 2.6219, -0.9074, -0.3120,  ...,  1.7529,  0.3887,  0.8767]]],\n",
       "        grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_to_emb = emb_model(input_ids)\n",
    "token_to_emb.shape, token_to_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "0a6d536c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-5.2473e-01,  1.1588e-01,  9.3648e-01,  1.6489e+00,  5.4864e-02,\n",
       "        -4.6306e-01, -1.7095e-01,  1.4399e+00, -1.2468e+00,  9.1688e-01,\n",
       "        -3.5326e-01, -9.2228e-01, -6.0763e-01, -1.3202e+00, -2.9825e-01,\n",
       "        -5.5747e-01,  5.2000e-02, -1.5810e-01,  3.4517e-01, -1.7875e-01,\n",
       "        -5.1647e-01, -9.3223e-01,  1.4936e+00, -9.9227e-01, -9.5211e-01,\n",
       "         5.8768e-01,  5.9252e-01,  6.8447e-02, -1.3360e-01, -1.3938e+00,\n",
       "        -1.0904e+00,  8.8862e-01, -2.5226e+00, -1.7354e+00, -7.6498e-01,\n",
       "        -9.8466e-03, -9.0285e-01, -1.5628e+00, -4.8471e-01, -4.4143e-01,\n",
       "         4.6817e-01,  1.4616e+00,  6.2225e-01,  2.5519e+00,  4.9818e-01,\n",
       "        -1.9967e+00, -1.6518e+00, -1.5871e+00, -7.3923e-01,  9.3160e-01,\n",
       "        -7.0247e-03, -9.7281e-01,  3.4608e-01, -1.2703e-01,  1.9975e+00,\n",
       "        -2.6407e-02,  7.2559e-01,  2.7518e-01, -1.8591e+00,  6.9145e-01,\n",
       "        -1.7654e+00,  6.2779e-01,  1.3450e+00, -6.2132e-01, -3.3217e-02,\n",
       "        -9.0692e-01,  1.5312e-01, -3.7733e-01, -4.7325e-01, -6.5366e-01,\n",
       "        -1.2425e-01,  2.2521e+00,  5.9061e-01,  1.8320e+00, -1.1270e+00,\n",
       "         2.5148e-01,  1.0706e+00,  3.0299e-01,  7.3789e-02,  1.7850e-01,\n",
       "         1.1533e+00, -1.4647e-01, -2.9854e-01, -1.0748e+00, -2.3557e+00,\n",
       "         1.6958e-01, -1.6493e+00, -2.6533e-01, -2.1007e+00,  6.5550e-01,\n",
       "        -8.7316e-01, -1.1465e+00,  1.4410e-01, -3.2928e-01, -9.2248e-02,\n",
       "         1.0246e+00,  7.9574e-01, -1.6367e+00,  7.0467e-01, -3.5615e-01,\n",
       "        -9.0086e-01,  4.5651e-01,  2.2647e+00, -2.3119e-01,  3.5663e-01,\n",
       "         4.7745e-02,  2.1005e+00,  8.1235e-01, -3.1158e-01,  1.0979e+00,\n",
       "         3.2444e-01,  7.5538e-01,  6.2549e-01, -5.9365e-01, -6.7249e-01,\n",
       "        -9.6360e-01,  4.2011e-01, -4.3999e-01,  6.0737e-01,  2.4403e-02,\n",
       "        -1.3960e+00, -1.4797e+00, -8.0132e-01, -1.9291e-01, -4.7938e-01,\n",
       "         1.3350e+00, -7.4682e-01, -7.4739e-01, -3.1267e-02,  4.2594e-01,\n",
       "         1.1863e+00, -1.4124e+00, -1.2820e+00,  4.8170e-01, -3.9602e-01,\n",
       "         1.8945e-02, -7.9278e-01,  4.4396e-02, -5.9688e-03,  5.1621e-02,\n",
       "         1.7404e+00, -1.1028e+00, -2.4473e+00,  2.3669e+00, -4.1653e-01,\n",
       "        -2.7647e-01, -4.0546e-01, -2.0745e+00, -9.7295e-01,  3.3538e-01,\n",
       "        -2.8535e-02,  3.0390e-01,  3.1201e-01, -1.9995e+00,  8.4294e-01,\n",
       "         1.8040e+00, -7.2554e-01,  2.1860e+00, -7.0259e-01,  1.1897e+00,\n",
       "        -7.4175e-01,  7.5258e-01,  2.9793e-01,  2.0622e-01, -3.2737e-01,\n",
       "         3.9769e-01, -1.6386e+00,  3.7948e-01,  2.8321e-01,  1.9281e+00,\n",
       "        -2.0728e+00,  1.9705e+00, -1.7975e-01,  1.9483e+00, -5.5371e-01,\n",
       "        -4.4678e-01,  1.3530e-01, -3.9710e-02,  1.7969e+00, -1.1804e+00,\n",
       "         7.0013e-02,  3.5222e-02,  1.1998e+00,  1.9555e-01, -2.1209e-01,\n",
       "        -4.4129e-01,  3.0858e-01, -1.7120e+00, -7.4367e-01, -4.5197e-01,\n",
       "        -6.3937e-01, -1.3638e+00,  9.5497e-01,  1.4475e+00,  1.2718e-01,\n",
       "         9.1248e-01, -1.2219e+00,  1.5991e-01,  1.9476e-01,  4.9215e-01,\n",
       "        -6.0501e-01,  6.8434e-01, -6.7944e-01,  4.4552e-01, -1.1566e+00,\n",
       "        -5.7060e-01, -3.7494e-01,  1.0279e-01, -1.0889e+00,  2.4045e+00,\n",
       "        -1.1647e+00, -7.4553e-01,  2.1858e-01,  5.1468e-01,  9.5274e-01,\n",
       "         6.7977e-01,  2.5583e-01,  5.5104e-01,  1.9157e-01,  2.7044e-01,\n",
       "        -5.3547e-01,  5.0524e-01,  1.9113e+00,  3.0502e-01, -9.4962e-01,\n",
       "        -9.7152e-01,  6.0179e-01, -3.3768e-01,  1.3119e+00, -1.2176e+00,\n",
       "         9.3066e-01,  3.8749e-01,  1.3782e+00,  2.0629e+00, -7.3328e-01,\n",
       "        -1.4539e-03,  1.3506e+00, -1.3789e+00, -1.2438e+00,  8.2135e-01,\n",
       "        -9.3656e-01, -2.2911e-01,  9.4131e-01,  3.2114e-01, -1.0985e+00,\n",
       "        -1.2459e+00, -1.1902e-01,  8.1054e-01,  1.9615e+00, -1.9219e-01,\n",
       "        -1.1677e+00, -5.0393e-01,  3.5272e-02,  3.6979e-01,  9.8733e-01,\n",
       "         3.2761e-01, -6.2120e-01,  5.6390e-01,  6.2163e-01, -1.4292e+00,\n",
       "        -7.7103e-01,  1.0212e+00,  2.3787e-02, -6.3711e-01,  1.6232e-01,\n",
       "         9.5024e-01,  5.2533e-02,  1.0382e+00, -4.0038e-02,  8.3279e-01,\n",
       "         9.0630e-01, -4.3393e-01, -2.0616e+00, -1.0450e+00, -3.2202e-01,\n",
       "        -6.8510e-01,  1.0043e+00, -1.7637e+00, -1.5940e+00, -1.4893e+00,\n",
       "        -3.5221e+00, -1.5994e+00, -1.1209e+00, -9.9651e-01,  6.0995e-01,\n",
       "         1.1737e+00, -1.4856e+00, -6.3666e-01, -4.8704e-01,  3.2151e-01,\n",
       "         5.3999e-01,  5.9929e-01,  5.2548e-01,  5.4255e-01,  5.3232e-02,\n",
       "         4.7559e-01,  1.0805e+00,  7.9024e-01,  7.7449e-01,  9.2044e-01,\n",
       "         7.1503e-01,  2.1038e+00, -1.3687e+00, -2.0968e+00, -3.1228e-01,\n",
       "        -4.7635e-01, -1.1990e+00, -1.0571e+00, -4.2240e-01,  2.1098e+00,\n",
       "        -1.2646e-01, -7.0462e-02, -1.1815e+00,  6.4741e-01,  1.2705e+00,\n",
       "         2.2485e-01,  4.2333e-02, -1.1056e+00, -8.5400e-01, -1.3873e+00,\n",
       "        -4.9600e-01,  3.6102e-01, -7.3650e-01, -7.0218e-01, -2.1074e+00,\n",
       "        -3.7924e-01, -2.8850e-01, -4.5891e-01, -1.4211e+00, -1.4014e-01,\n",
       "        -4.3989e-01, -5.8724e-02,  2.1569e+00,  8.8537e-01,  1.7874e+00,\n",
       "         3.6943e-01,  1.6280e+00, -1.0073e+00, -6.7446e-01,  2.2232e-01,\n",
       "         6.0120e-01,  1.4967e+00,  2.1106e+00,  2.9095e-01,  4.1686e-01,\n",
       "        -3.8662e-01,  9.6138e-01, -1.0616e-01,  1.8926e-01,  1.2898e-01,\n",
       "        -7.9569e-01,  8.3564e-01,  5.4753e-01,  1.2220e+00, -5.6376e-01,\n",
       "         6.9770e-01, -5.5822e-01, -1.7434e+00,  9.6391e-01, -1.3866e+00,\n",
       "         1.0574e+00,  7.5350e-01,  3.3936e-01,  2.3197e+00, -8.2411e-01,\n",
       "        -5.4330e-01,  5.7605e-01, -5.4293e-01,  1.0181e+00, -5.2961e-02,\n",
       "         9.0136e-01,  7.7772e-02, -3.8303e-01,  2.2802e-01,  3.4522e-01,\n",
       "        -5.8396e-01,  1.3346e+00, -1.1029e+00, -7.3429e-01, -1.1770e+00,\n",
       "        -2.1263e+00, -9.9490e-01, -1.4909e-01,  3.5163e-01, -9.0720e-01,\n",
       "         1.4410e+00, -9.4937e-01, -4.6826e-01, -1.3483e+00, -8.4931e-01,\n",
       "         1.2705e+00,  1.0785e-01, -1.8880e-01, -5.0598e-01,  2.0129e-01,\n",
       "         1.0472e+00, -2.3079e+00,  7.7730e-02,  8.5254e-01,  1.1248e+00,\n",
       "         1.3472e+00,  3.6461e-01, -4.8398e-01, -1.6283e+00, -1.1922e+00,\n",
       "        -1.0272e+00,  1.5020e-01, -1.2371e+00,  3.2285e-01, -3.1192e-01,\n",
       "        -6.3774e-01, -1.2033e+00,  1.1963e+00, -4.4477e-01, -1.9399e+00,\n",
       "        -4.2973e-02,  5.1891e-01, -1.5007e+00,  8.8546e-01,  1.6136e+00,\n",
       "        -1.5274e+00,  1.6793e+00, -7.7742e-01,  2.6893e-01,  9.9938e-01,\n",
       "         1.1141e-01,  9.6090e-02,  9.6252e-01,  1.2326e-01, -1.9971e-01,\n",
       "        -3.8660e-01, -5.6936e-01,  1.6489e+00, -3.0723e-01,  1.0735e+00,\n",
       "         2.5006e-01, -1.2974e+00, -5.8956e-01,  6.2026e-03,  1.8024e-01,\n",
       "        -1.4482e+00,  5.3607e-01,  7.1887e-01,  1.4044e-01,  3.1792e-01,\n",
       "         1.2122e+00,  2.2342e+00,  2.5546e+00, -9.9315e-01,  1.7264e+00,\n",
       "        -5.1605e-01, -4.3188e-01,  1.4363e-01,  5.4367e-01, -6.9247e-01,\n",
       "        -2.1469e+00, -7.5426e-01,  1.0884e-01,  8.8251e-01,  1.1475e+00,\n",
       "         1.8677e+00, -1.7304e+00, -1.0109e+00, -1.0625e+00, -4.7871e-01,\n",
       "        -8.2731e-01,  9.0825e-01,  3.9823e-01, -4.4271e-01,  1.5989e+00,\n",
       "         1.5920e-02, -9.9818e-01, -5.1303e-01, -8.2401e-01, -1.1493e+00,\n",
       "         1.6713e+00, -1.6742e+00,  8.2292e-01, -2.1505e+00,  1.1607e+00,\n",
       "        -3.7909e-01,  3.5782e-02, -3.3976e-01,  8.3782e-01,  1.0115e+00,\n",
       "         4.5694e-01,  6.8455e-01,  7.5236e-01, -8.1638e-01,  6.4215e-01,\n",
       "         2.3587e-01,  4.2200e-02,  4.3344e-01, -8.7341e-01,  1.5926e+00,\n",
       "         3.4531e-01,  1.5254e+00, -1.4914e+00, -1.7548e-01,  5.2829e-01,\n",
       "         8.4815e-01,  1.1797e+00, -9.9017e-01, -7.4520e-01, -5.4848e-01,\n",
       "        -5.9483e-01, -6.2456e-01,  8.8206e-01,  1.0313e-01, -1.9324e-01,\n",
       "         2.5040e-01, -6.4073e-01, -1.0710e-01,  1.0720e+00, -4.7089e-01,\n",
       "         4.7256e-02, -1.3645e+00,  6.7773e-01,  4.8647e-01,  4.9086e-01,\n",
       "         4.1207e-01,  7.8486e-01, -1.0953e-01,  1.4802e+00, -3.4185e-01,\n",
       "         5.5661e-01, -6.7258e-01,  1.7243e+00, -1.3022e+00,  9.7786e-01,\n",
       "        -4.5697e-01,  5.9886e-01, -7.1227e-01, -7.7220e-01,  1.6124e-01,\n",
       "         2.0665e-01, -1.2252e+00,  1.0635e+00, -9.5268e-01, -8.1797e-01,\n",
       "         5.8136e-01,  1.3615e+00, -1.9794e-01, -3.1123e-01,  7.2927e-01,\n",
       "         6.9810e-01,  4.1332e-02,  1.8027e+00,  1.0419e+00,  1.4403e+00,\n",
       "         4.9702e-01, -1.6172e+00,  6.6582e-02, -1.0574e+00, -2.5174e-01,\n",
       "        -1.7720e+00, -1.5908e+00, -2.1229e+00,  4.3753e-01,  5.6065e-01,\n",
       "        -2.0625e+00, -1.9358e-01, -1.4683e-01, -1.0280e+00, -9.1165e-01,\n",
       "        -4.2371e-01, -1.6894e+00,  1.4492e+00, -3.1091e-01,  2.4479e-01,\n",
       "         3.5963e-01,  7.1477e-01, -1.1041e+00, -1.0235e+00,  2.0446e+00,\n",
       "        -4.8292e-01,  7.4377e-01, -1.2920e+00, -7.8003e-01, -6.7284e-01,\n",
       "         1.1400e+00, -7.2149e-01, -1.3006e+00,  1.2047e+00,  9.3721e-02,\n",
       "         1.1166e+00, -2.5158e+00,  9.4084e-01,  5.7094e-01, -5.2351e-01,\n",
       "         1.4452e+00, -1.9280e+00,  3.8525e-01, -5.1837e-01, -1.0496e+00,\n",
       "         7.6837e-01, -3.4731e-01,  1.4291e-01,  1.4662e+00,  1.4934e+00,\n",
       "         7.7607e-02,  5.9160e-01, -1.1547e+00,  1.7223e+00,  3.8764e-01,\n",
       "         3.1441e-01,  4.6771e-01, -9.8762e-01, -3.0647e-01, -4.2256e-01,\n",
       "         2.2125e+00, -5.6591e-02, -9.3524e-01, -6.0319e-01, -6.3727e-01,\n",
       "         4.9496e-01, -4.3252e-01, -3.7675e-01, -1.7241e+00,  4.9289e-01,\n",
       "        -1.8486e-01,  3.0106e-01, -1.2219e+00, -4.7638e-01,  1.5889e+00,\n",
       "         7.3825e-01,  4.2778e-01,  2.0941e+00,  7.6266e-01, -3.1509e-01,\n",
       "        -1.5202e+00,  4.4585e-01,  1.0269e+00,  1.2083e+00, -3.6185e-01,\n",
       "        -4.3806e-01, -7.0708e-01,  5.5328e-01,  6.6532e-01, -4.6342e-01,\n",
       "        -1.0066e+00,  7.2349e-01,  6.7864e-01,  2.7394e-01, -1.9498e-01,\n",
       "         6.0047e-01,  5.6663e-01, -2.8239e+00,  5.9191e-01, -9.8753e-01,\n",
       "        -1.2231e+00, -7.6314e-01, -6.5095e-02, -4.3196e-01,  7.9841e-01,\n",
       "         4.1356e-01,  4.2206e-01,  1.6607e+00, -1.6133e+00, -1.4584e-01,\n",
       "         1.5527e+00, -4.4005e-01,  2.5839e-01, -2.7972e-01,  2.6359e-02,\n",
       "        -1.0526e+00, -6.3489e-01, -2.7228e-02,  2.5674e-03,  7.9338e-01,\n",
       "         1.1550e-01, -1.4442e+00, -6.2519e-01, -5.5710e-01,  4.9705e-01,\n",
       "         6.1694e-01,  9.1893e-01,  9.9493e-01, -7.8262e-01,  2.1021e+00,\n",
       "        -1.0415e+00,  1.9466e+00,  9.8566e-01,  1.1851e+00,  3.6332e-01,\n",
       "        -7.3700e-02,  1.3113e-01, -4.8082e-02, -4.4022e-01,  1.7205e-01,\n",
       "         6.1786e-01,  3.2600e-01, -7.0288e-02,  1.7233e+00,  9.5321e-01,\n",
       "         9.9008e-01, -1.0148e+00, -1.2218e+00, -1.6344e+00,  1.4756e+00,\n",
       "         4.4621e-01, -9.1810e-01,  4.6610e-01, -1.7740e+00,  2.6708e-01,\n",
       "         1.1214e+00,  1.9502e+00, -2.4614e+00, -7.1847e-03, -2.1367e-02,\n",
       "         1.5872e+00, -4.5813e-01,  3.3795e-02, -1.0573e+00,  7.4366e-01,\n",
       "         4.9285e-01,  4.0213e-01,  2.2774e-01,  8.6437e-01, -8.1799e-01,\n",
       "         6.5116e-01,  3.3331e-01,  6.9694e-01,  3.9635e-01, -1.8460e+00,\n",
       "        -7.9314e-01,  1.2248e-01, -5.7664e-01,  6.0360e-01,  4.6910e-01,\n",
       "        -1.7790e+00, -7.8136e-01,  6.1264e-02,  7.6985e-01, -5.4907e-01,\n",
       "        -6.0505e-02,  1.4500e-01, -4.1360e-01, -3.2709e-01,  1.6905e+00,\n",
       "        -7.2586e-01,  5.1594e-01, -2.0225e-01, -2.4872e-01,  9.4430e-01,\n",
       "         5.6167e-01,  1.0251e+00,  2.3209e+00,  2.5240e+00, -2.5066e-01,\n",
       "        -2.0721e-02,  2.4765e-01, -3.5852e-01,  2.5957e+00,  1.0966e+00,\n",
       "         9.8474e-01,  8.8359e-01,  8.6967e-02, -2.4319e+00, -2.6606e-01,\n",
       "        -1.1415e-01,  6.1799e-01,  1.3616e+00, -9.2137e-01, -5.2399e-01,\n",
       "         7.8081e-01, -3.7398e-01,  2.1997e-01], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_model.word_embeddings.weight[101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "c2604240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-5.2473e-01,  1.1588e-01,  9.3648e-01,  1.6489e+00,  5.4864e-02,\n",
       "        -4.6306e-01, -1.7095e-01,  1.4399e+00, -1.2468e+00,  9.1688e-01,\n",
       "        -3.5326e-01, -9.2228e-01, -6.0763e-01, -1.3202e+00, -2.9825e-01,\n",
       "        -5.5747e-01,  5.2000e-02, -1.5810e-01,  3.4517e-01, -1.7875e-01,\n",
       "        -5.1647e-01, -9.3223e-01,  1.4936e+00, -9.9227e-01, -9.5211e-01,\n",
       "         5.8768e-01,  5.9252e-01,  6.8447e-02, -1.3360e-01, -1.3938e+00,\n",
       "        -1.0904e+00,  8.8862e-01, -2.5226e+00, -1.7354e+00, -7.6498e-01,\n",
       "        -9.8466e-03, -9.0285e-01, -1.5628e+00, -4.8471e-01, -4.4143e-01,\n",
       "         4.6817e-01,  1.4616e+00,  6.2225e-01,  2.5519e+00,  4.9818e-01,\n",
       "        -1.9967e+00, -1.6518e+00, -1.5871e+00, -7.3923e-01,  9.3160e-01,\n",
       "        -7.0247e-03, -9.7281e-01,  3.4608e-01, -1.2703e-01,  1.9975e+00,\n",
       "        -2.6407e-02,  7.2559e-01,  2.7518e-01, -1.8591e+00,  6.9145e-01,\n",
       "        -1.7654e+00,  6.2779e-01,  1.3450e+00, -6.2132e-01, -3.3217e-02,\n",
       "        -9.0692e-01,  1.5312e-01, -3.7733e-01, -4.7325e-01, -6.5366e-01,\n",
       "        -1.2425e-01,  2.2521e+00,  5.9061e-01,  1.8320e+00, -1.1270e+00,\n",
       "         2.5148e-01,  1.0706e+00,  3.0299e-01,  7.3789e-02,  1.7850e-01,\n",
       "         1.1533e+00, -1.4647e-01, -2.9854e-01, -1.0748e+00, -2.3557e+00,\n",
       "         1.6958e-01, -1.6493e+00, -2.6533e-01, -2.1007e+00,  6.5550e-01,\n",
       "        -8.7316e-01, -1.1465e+00,  1.4410e-01, -3.2928e-01, -9.2248e-02,\n",
       "         1.0246e+00,  7.9574e-01, -1.6367e+00,  7.0467e-01, -3.5615e-01,\n",
       "        -9.0086e-01,  4.5651e-01,  2.2647e+00, -2.3119e-01,  3.5663e-01,\n",
       "         4.7745e-02,  2.1005e+00,  8.1235e-01, -3.1158e-01,  1.0979e+00,\n",
       "         3.2444e-01,  7.5538e-01,  6.2549e-01, -5.9365e-01, -6.7249e-01,\n",
       "        -9.6360e-01,  4.2011e-01, -4.3999e-01,  6.0737e-01,  2.4403e-02,\n",
       "        -1.3960e+00, -1.4797e+00, -8.0132e-01, -1.9291e-01, -4.7938e-01,\n",
       "         1.3350e+00, -7.4682e-01, -7.4739e-01, -3.1267e-02,  4.2594e-01,\n",
       "         1.1863e+00, -1.4124e+00, -1.2820e+00,  4.8170e-01, -3.9602e-01,\n",
       "         1.8945e-02, -7.9278e-01,  4.4396e-02, -5.9688e-03,  5.1621e-02,\n",
       "         1.7404e+00, -1.1028e+00, -2.4473e+00,  2.3669e+00, -4.1653e-01,\n",
       "        -2.7647e-01, -4.0546e-01, -2.0745e+00, -9.7295e-01,  3.3538e-01,\n",
       "        -2.8535e-02,  3.0390e-01,  3.1201e-01, -1.9995e+00,  8.4294e-01,\n",
       "         1.8040e+00, -7.2554e-01,  2.1860e+00, -7.0259e-01,  1.1897e+00,\n",
       "        -7.4175e-01,  7.5258e-01,  2.9793e-01,  2.0622e-01, -3.2737e-01,\n",
       "         3.9769e-01, -1.6386e+00,  3.7948e-01,  2.8321e-01,  1.9281e+00,\n",
       "        -2.0728e+00,  1.9705e+00, -1.7975e-01,  1.9483e+00, -5.5371e-01,\n",
       "        -4.4678e-01,  1.3530e-01, -3.9710e-02,  1.7969e+00, -1.1804e+00,\n",
       "         7.0013e-02,  3.5222e-02,  1.1998e+00,  1.9555e-01, -2.1209e-01,\n",
       "        -4.4129e-01,  3.0858e-01, -1.7120e+00, -7.4367e-01, -4.5197e-01,\n",
       "        -6.3937e-01, -1.3638e+00,  9.5497e-01,  1.4475e+00,  1.2718e-01,\n",
       "         9.1248e-01, -1.2219e+00,  1.5991e-01,  1.9476e-01,  4.9215e-01,\n",
       "        -6.0501e-01,  6.8434e-01, -6.7944e-01,  4.4552e-01, -1.1566e+00,\n",
       "        -5.7060e-01, -3.7494e-01,  1.0279e-01, -1.0889e+00,  2.4045e+00,\n",
       "        -1.1647e+00, -7.4553e-01,  2.1858e-01,  5.1468e-01,  9.5274e-01,\n",
       "         6.7977e-01,  2.5583e-01,  5.5104e-01,  1.9157e-01,  2.7044e-01,\n",
       "        -5.3547e-01,  5.0524e-01,  1.9113e+00,  3.0502e-01, -9.4962e-01,\n",
       "        -9.7152e-01,  6.0179e-01, -3.3768e-01,  1.3119e+00, -1.2176e+00,\n",
       "         9.3066e-01,  3.8749e-01,  1.3782e+00,  2.0629e+00, -7.3328e-01,\n",
       "        -1.4539e-03,  1.3506e+00, -1.3789e+00, -1.2438e+00,  8.2135e-01,\n",
       "        -9.3656e-01, -2.2911e-01,  9.4131e-01,  3.2114e-01, -1.0985e+00,\n",
       "        -1.2459e+00, -1.1902e-01,  8.1054e-01,  1.9615e+00, -1.9219e-01,\n",
       "        -1.1677e+00, -5.0393e-01,  3.5272e-02,  3.6979e-01,  9.8733e-01,\n",
       "         3.2761e-01, -6.2120e-01,  5.6390e-01,  6.2163e-01, -1.4292e+00,\n",
       "        -7.7103e-01,  1.0212e+00,  2.3787e-02, -6.3711e-01,  1.6232e-01,\n",
       "         9.5024e-01,  5.2533e-02,  1.0382e+00, -4.0038e-02,  8.3279e-01,\n",
       "         9.0630e-01, -4.3393e-01, -2.0616e+00, -1.0450e+00, -3.2202e-01,\n",
       "        -6.8510e-01,  1.0043e+00, -1.7637e+00, -1.5940e+00, -1.4893e+00,\n",
       "        -3.5221e+00, -1.5994e+00, -1.1209e+00, -9.9651e-01,  6.0995e-01,\n",
       "         1.1737e+00, -1.4856e+00, -6.3666e-01, -4.8704e-01,  3.2151e-01,\n",
       "         5.3999e-01,  5.9929e-01,  5.2548e-01,  5.4255e-01,  5.3232e-02,\n",
       "         4.7559e-01,  1.0805e+00,  7.9024e-01,  7.7449e-01,  9.2044e-01,\n",
       "         7.1503e-01,  2.1038e+00, -1.3687e+00, -2.0968e+00, -3.1228e-01,\n",
       "        -4.7635e-01, -1.1990e+00, -1.0571e+00, -4.2240e-01,  2.1098e+00,\n",
       "        -1.2646e-01, -7.0462e-02, -1.1815e+00,  6.4741e-01,  1.2705e+00,\n",
       "         2.2485e-01,  4.2333e-02, -1.1056e+00, -8.5400e-01, -1.3873e+00,\n",
       "        -4.9600e-01,  3.6102e-01, -7.3650e-01, -7.0218e-01, -2.1074e+00,\n",
       "        -3.7924e-01, -2.8850e-01, -4.5891e-01, -1.4211e+00, -1.4014e-01,\n",
       "        -4.3989e-01, -5.8724e-02,  2.1569e+00,  8.8537e-01,  1.7874e+00,\n",
       "         3.6943e-01,  1.6280e+00, -1.0073e+00, -6.7446e-01,  2.2232e-01,\n",
       "         6.0120e-01,  1.4967e+00,  2.1106e+00,  2.9095e-01,  4.1686e-01,\n",
       "        -3.8662e-01,  9.6138e-01, -1.0616e-01,  1.8926e-01,  1.2898e-01,\n",
       "        -7.9569e-01,  8.3564e-01,  5.4753e-01,  1.2220e+00, -5.6376e-01,\n",
       "         6.9770e-01, -5.5822e-01, -1.7434e+00,  9.6391e-01, -1.3866e+00,\n",
       "         1.0574e+00,  7.5350e-01,  3.3936e-01,  2.3197e+00, -8.2411e-01,\n",
       "        -5.4330e-01,  5.7605e-01, -5.4293e-01,  1.0181e+00, -5.2961e-02,\n",
       "         9.0136e-01,  7.7772e-02, -3.8303e-01,  2.2802e-01,  3.4522e-01,\n",
       "        -5.8396e-01,  1.3346e+00, -1.1029e+00, -7.3429e-01, -1.1770e+00,\n",
       "        -2.1263e+00, -9.9490e-01, -1.4909e-01,  3.5163e-01, -9.0720e-01,\n",
       "         1.4410e+00, -9.4937e-01, -4.6826e-01, -1.3483e+00, -8.4931e-01,\n",
       "         1.2705e+00,  1.0785e-01, -1.8880e-01, -5.0598e-01,  2.0129e-01,\n",
       "         1.0472e+00, -2.3079e+00,  7.7730e-02,  8.5254e-01,  1.1248e+00,\n",
       "         1.3472e+00,  3.6461e-01, -4.8398e-01, -1.6283e+00, -1.1922e+00,\n",
       "        -1.0272e+00,  1.5020e-01, -1.2371e+00,  3.2285e-01, -3.1192e-01,\n",
       "        -6.3774e-01, -1.2033e+00,  1.1963e+00, -4.4477e-01, -1.9399e+00,\n",
       "        -4.2973e-02,  5.1891e-01, -1.5007e+00,  8.8546e-01,  1.6136e+00,\n",
       "        -1.5274e+00,  1.6793e+00, -7.7742e-01,  2.6893e-01,  9.9938e-01,\n",
       "         1.1141e-01,  9.6090e-02,  9.6252e-01,  1.2326e-01, -1.9971e-01,\n",
       "        -3.8660e-01, -5.6936e-01,  1.6489e+00, -3.0723e-01,  1.0735e+00,\n",
       "         2.5006e-01, -1.2974e+00, -5.8956e-01,  6.2026e-03,  1.8024e-01,\n",
       "        -1.4482e+00,  5.3607e-01,  7.1887e-01,  1.4044e-01,  3.1792e-01,\n",
       "         1.2122e+00,  2.2342e+00,  2.5546e+00, -9.9315e-01,  1.7264e+00,\n",
       "        -5.1605e-01, -4.3188e-01,  1.4363e-01,  5.4367e-01, -6.9247e-01,\n",
       "        -2.1469e+00, -7.5426e-01,  1.0884e-01,  8.8251e-01,  1.1475e+00,\n",
       "         1.8677e+00, -1.7304e+00, -1.0109e+00, -1.0625e+00, -4.7871e-01,\n",
       "        -8.2731e-01,  9.0825e-01,  3.9823e-01, -4.4271e-01,  1.5989e+00,\n",
       "         1.5920e-02, -9.9818e-01, -5.1303e-01, -8.2401e-01, -1.1493e+00,\n",
       "         1.6713e+00, -1.6742e+00,  8.2292e-01, -2.1505e+00,  1.1607e+00,\n",
       "        -3.7909e-01,  3.5782e-02, -3.3976e-01,  8.3782e-01,  1.0115e+00,\n",
       "         4.5694e-01,  6.8455e-01,  7.5236e-01, -8.1638e-01,  6.4215e-01,\n",
       "         2.3587e-01,  4.2200e-02,  4.3344e-01, -8.7341e-01,  1.5926e+00,\n",
       "         3.4531e-01,  1.5254e+00, -1.4914e+00, -1.7548e-01,  5.2829e-01,\n",
       "         8.4815e-01,  1.1797e+00, -9.9017e-01, -7.4520e-01, -5.4848e-01,\n",
       "        -5.9483e-01, -6.2456e-01,  8.8206e-01,  1.0313e-01, -1.9324e-01,\n",
       "         2.5040e-01, -6.4073e-01, -1.0710e-01,  1.0720e+00, -4.7089e-01,\n",
       "         4.7256e-02, -1.3645e+00,  6.7773e-01,  4.8647e-01,  4.9086e-01,\n",
       "         4.1207e-01,  7.8486e-01, -1.0953e-01,  1.4802e+00, -3.4185e-01,\n",
       "         5.5661e-01, -6.7258e-01,  1.7243e+00, -1.3022e+00,  9.7786e-01,\n",
       "        -4.5697e-01,  5.9886e-01, -7.1227e-01, -7.7220e-01,  1.6124e-01,\n",
       "         2.0665e-01, -1.2252e+00,  1.0635e+00, -9.5268e-01, -8.1797e-01,\n",
       "         5.8136e-01,  1.3615e+00, -1.9794e-01, -3.1123e-01,  7.2927e-01,\n",
       "         6.9810e-01,  4.1332e-02,  1.8027e+00,  1.0419e+00,  1.4403e+00,\n",
       "         4.9702e-01, -1.6172e+00,  6.6582e-02, -1.0574e+00, -2.5174e-01,\n",
       "        -1.7720e+00, -1.5908e+00, -2.1229e+00,  4.3753e-01,  5.6065e-01,\n",
       "        -2.0625e+00, -1.9358e-01, -1.4683e-01, -1.0280e+00, -9.1165e-01,\n",
       "        -4.2371e-01, -1.6894e+00,  1.4492e+00, -3.1091e-01,  2.4479e-01,\n",
       "         3.5963e-01,  7.1477e-01, -1.1041e+00, -1.0235e+00,  2.0446e+00,\n",
       "        -4.8292e-01,  7.4377e-01, -1.2920e+00, -7.8003e-01, -6.7284e-01,\n",
       "         1.1400e+00, -7.2149e-01, -1.3006e+00,  1.2047e+00,  9.3721e-02,\n",
       "         1.1166e+00, -2.5158e+00,  9.4084e-01,  5.7094e-01, -5.2351e-01,\n",
       "         1.4452e+00, -1.9280e+00,  3.8525e-01, -5.1837e-01, -1.0496e+00,\n",
       "         7.6837e-01, -3.4731e-01,  1.4291e-01,  1.4662e+00,  1.4934e+00,\n",
       "         7.7607e-02,  5.9160e-01, -1.1547e+00,  1.7223e+00,  3.8764e-01,\n",
       "         3.1441e-01,  4.6771e-01, -9.8762e-01, -3.0647e-01, -4.2256e-01,\n",
       "         2.2125e+00, -5.6591e-02, -9.3524e-01, -6.0319e-01, -6.3727e-01,\n",
       "         4.9496e-01, -4.3252e-01, -3.7675e-01, -1.7241e+00,  4.9289e-01,\n",
       "        -1.8486e-01,  3.0106e-01, -1.2219e+00, -4.7638e-01,  1.5889e+00,\n",
       "         7.3825e-01,  4.2778e-01,  2.0941e+00,  7.6266e-01, -3.1509e-01,\n",
       "        -1.5202e+00,  4.4585e-01,  1.0269e+00,  1.2083e+00, -3.6185e-01,\n",
       "        -4.3806e-01, -7.0708e-01,  5.5328e-01,  6.6532e-01, -4.6342e-01,\n",
       "        -1.0066e+00,  7.2349e-01,  6.7864e-01,  2.7394e-01, -1.9498e-01,\n",
       "         6.0047e-01,  5.6663e-01, -2.8239e+00,  5.9191e-01, -9.8753e-01,\n",
       "        -1.2231e+00, -7.6314e-01, -6.5095e-02, -4.3196e-01,  7.9841e-01,\n",
       "         4.1356e-01,  4.2206e-01,  1.6607e+00, -1.6133e+00, -1.4584e-01,\n",
       "         1.5527e+00, -4.4005e-01,  2.5839e-01, -2.7972e-01,  2.6359e-02,\n",
       "        -1.0526e+00, -6.3489e-01, -2.7228e-02,  2.5674e-03,  7.9338e-01,\n",
       "         1.1550e-01, -1.4442e+00, -6.2519e-01, -5.5710e-01,  4.9705e-01,\n",
       "         6.1694e-01,  9.1893e-01,  9.9493e-01, -7.8262e-01,  2.1021e+00,\n",
       "        -1.0415e+00,  1.9466e+00,  9.8566e-01,  1.1851e+00,  3.6332e-01,\n",
       "        -7.3700e-02,  1.3113e-01, -4.8082e-02, -4.4022e-01,  1.7205e-01,\n",
       "         6.1786e-01,  3.2600e-01, -7.0288e-02,  1.7233e+00,  9.5321e-01,\n",
       "         9.9008e-01, -1.0148e+00, -1.2218e+00, -1.6344e+00,  1.4756e+00,\n",
       "         4.4621e-01, -9.1810e-01,  4.6610e-01, -1.7740e+00,  2.6708e-01,\n",
       "         1.1214e+00,  1.9502e+00, -2.4614e+00, -7.1847e-03, -2.1367e-02,\n",
       "         1.5872e+00, -4.5813e-01,  3.3795e-02, -1.0573e+00,  7.4366e-01,\n",
       "         4.9285e-01,  4.0213e-01,  2.2774e-01,  8.6437e-01, -8.1799e-01,\n",
       "         6.5116e-01,  3.3331e-01,  6.9694e-01,  3.9635e-01, -1.8460e+00,\n",
       "        -7.9314e-01,  1.2248e-01, -5.7664e-01,  6.0360e-01,  4.6910e-01,\n",
       "        -1.7790e+00, -7.8136e-01,  6.1264e-02,  7.6985e-01, -5.4907e-01,\n",
       "        -6.0505e-02,  1.4500e-01, -4.1360e-01, -3.2709e-01,  1.6905e+00,\n",
       "        -7.2586e-01,  5.1594e-01, -2.0225e-01, -2.4872e-01,  9.4430e-01,\n",
       "         5.6167e-01,  1.0251e+00,  2.3209e+00,  2.5240e+00, -2.5066e-01,\n",
       "        -2.0721e-02,  2.4765e-01, -3.5852e-01,  2.5957e+00,  1.0966e+00,\n",
       "         9.8474e-01,  8.8359e-01,  8.6967e-02, -2.4319e+00, -2.6606e-01,\n",
       "        -1.1415e-01,  6.1799e-01,  1.3616e+00, -9.2137e-01, -5.2399e-01,\n",
       "         7.8081e-01, -3.7398e-01,  2.1997e-01], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = emb_model.word_embeddings(input_ids)\n",
    "input_ids.shape, emb.shape\n",
    "emb[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "1eeb003a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 101, 7632, 2026, 2171, 2003, 4080,  102])"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "769d7803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_length : 7\n",
      "position_ids : tensor([0, 1, 2, 3, 4, 5, 6])\n",
      "after unsqueeze position_ids : tensor([[0, 1, 2, 3, 4, 5, 6]])\n",
      "after embedding position_embeddings : tensor([[[ 0.6665, -0.3817,  0.2419,  ..., -0.7578,  0.2447, -0.6742],\n",
      "         [-1.0725,  0.2605, -0.8837,  ...,  0.3608, -1.1377, -0.4868],\n",
      "         [ 1.5611, -1.0221,  0.5453,  ..., -1.2333, -2.0115, -0.5676],\n",
      "         ...,\n",
      "         [ 0.8230, -0.9710,  1.7839,  ..., -0.2120,  1.5923,  1.1770],\n",
      "         [-0.5308, -0.5556,  0.3099,  ...,  0.2551, -1.6586, -1.1233],\n",
      "         [ 1.1312, -0.5512,  1.2568,  ...,  1.6600, -0.5874,  0.4305]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "position_embeddings shape : torch.Size([1, 7, 768])\n"
     ]
    }
   ],
   "source": [
    "seq_length = input_ids.size(1) # 문장 길이\n",
    "print('seq_length : {}'.format(seq_length))\n",
    "position_ids = torch.arange(\n",
    "    seq_length, dtype=torch.long, device=input_ids.device)\n",
    "print('position_ids : {}'.format(position_ids))\n",
    "position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "print('after unsqueeze position_ids : {}'.format(position_ids))\n",
    "position_embeddings = emb_model.position_embeddings(position_ids)\n",
    "print('after embedding position_embeddings : {}'.format(position_embeddings))\n",
    "print('position_embeddings shape : {}'.format(position_embeddings.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "44f7217a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.5162, -1.3922, -1.1696,  ...,  0.7639,  0.8182,  0.7789],\n",
       "         [ 1.5162, -1.3922, -1.1696,  ...,  0.7639,  0.8182,  0.7789],\n",
       "         [ 1.5162, -1.3922, -1.1696,  ...,  0.7639,  0.8182,  0.7789],\n",
       "         ...,\n",
       "         [ 1.5162, -1.3922, -1.1696,  ...,  0.7639,  0.8182,  0.7789],\n",
       "         [ 1.5162, -1.3922, -1.1696,  ...,  0.7639,  0.8182,  0.7789],\n",
       "         [ 1.5162, -1.3922, -1.1696,  ...,  0.7639,  0.8182,  0.7789]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_emb = emb_model.token_type_embeddings(torch.zeros_like(input_ids))\n",
    "tok_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "7df6d2b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.5162, -1.3922, -1.1696,  ...,  0.7639,  0.8182,  0.7789],\n",
       "         [ 1.5162, -1.3922, -1.1696,  ...,  0.7639,  0.8182,  0.7789],\n",
       "         [ 1.5162, -1.3922, -1.1696,  ...,  0.7639,  0.8182,  0.7789],\n",
       "         ...,\n",
       "         [ 1.5162, -1.3922, -1.1696,  ...,  0.7639,  0.8182,  0.7789],\n",
       "         [ 1.5162, -1.3922, -1.1696,  ...,  0.7639,  0.8182,  0.7789],\n",
       "         [ 1.5162, -1.3922, -1.1696,  ...,  0.7639,  0.8182,  0.7789]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if token_type_ids is None:\n",
    "    token_type_ids = torch.zeros_like(input_ids)\n",
    "token_type_embeddings = emb_model.token_type_embeddings(token_type_ids)\n",
    "token_type_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "6a3a124a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.6579, -1.6581,  0.0088,  ...,  0.7870,  0.6889,  0.3247],\n",
       "         [ 0.2656, -0.4530, -2.8417,  ...,  0.5779, -0.6376,  1.3595],\n",
       "         [ 1.7510, -1.7458, -0.9442,  ...,  0.1413, -1.4053, -0.5503],\n",
       "         ...,\n",
       "         [ 2.6611, -2.3332, -3.5140,  ...,  1.1806,  2.0422,  2.1068],\n",
       "         [-0.2973, -1.6818, -1.0267,  ...,  1.1027, -0.3161, -0.2829],\n",
       "         [ 3.9636, -1.3088, -0.4194,  ...,  2.6655,  0.6275,  1.3564]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb + position_embeddings + tok_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "ebb07ccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.9397, -0.9562, -0.0032,  ...,  0.4417,  0.3857,  0.1774],\n",
       "         [ 0.1335, -0.2884, -1.6908,  ...,  0.3169, -0.3968,  0.7758],\n",
       "         [ 1.0431, -1.0666, -0.5830,  ...,  0.0719, -0.8612, -0.3453],\n",
       "         ...,\n",
       "         [ 1.6176, -1.4190, -2.1370,  ...,  0.7174,  1.2413,  1.2806],\n",
       "         [-0.1760, -1.0018, -0.6111,  ...,  0.6591, -0.1872, -0.1674],\n",
       "         [ 2.3597, -0.8167, -0.2808,  ...,  1.5777,  0.3498,  0.7890]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddig = emb_model.LayerNorm(emb + position_embeddings + tok_emb)\n",
    "embeddig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "05f4020f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0441, -1.0624, -0.0035,  ...,  0.4908,  0.4285,  0.1971],\n",
       "         [ 0.1484, -0.3204, -0.0000,  ...,  0.3521, -0.4409,  0.0000],\n",
       "         [ 1.1590, -0.0000, -0.6478,  ...,  0.0000, -0.9569, -0.3837],\n",
       "         ...,\n",
       "         [ 1.7973, -1.5767, -2.3744,  ...,  0.7972,  1.3792,  1.4229],\n",
       "         [-0.1955, -1.1131, -0.6789,  ...,  0.7324, -0.2080, -0.1860],\n",
       "         [ 2.6219, -0.9074, -0.3120,  ...,  1.7529,  0.3887,  0.8767]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddig = emb_model.dropout(embeddig)\n",
    "embeddig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "3cbe264a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0441, -1.0624, -0.0035,  ...,  0.0000,  0.4285,  0.1971],\n",
       "         [ 0.1484, -0.0000, -1.8787,  ...,  0.3521, -0.4409,  0.8620],\n",
       "         [ 1.1590, -1.1851, -0.6478,  ...,  0.0799, -0.0000, -0.3837],\n",
       "         ...,\n",
       "         [ 1.7973, -1.5767, -2.3744,  ...,  0.7972,  1.3792,  1.4229],\n",
       "         [-0.1955, -1.1131, -0.6789,  ...,  0.7324, -0.2080, -0.1860],\n",
       "         [ 2.6219, -0.9074, -0.3120,  ...,  1.7529,  0.3887,  0.0000]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_to_emb = emb_model(input_ids)\n",
    "token_to_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229ad8ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c1298e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "60121bfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b7d883f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length = input_ids.size(1)\n",
    "seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "248ca704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, tensor([0, 1, 2, 3, 4]))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length = input_ids.size(1) # 문장 길이\n",
    "position_ids = torch.arange(\n",
    "    seq_length, dtype=torch.long, device=input_ids.device)\n",
    "seq_length, position_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bfc47f4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
    "position_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2b8d5a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5]),\n",
       " tensor([[0, 1, 2, 3, 4],\n",
       "         [0, 1, 2, 3, 4]]))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "position_ids.shape, position_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4363b02f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1813,  0.0983, -0.8991,  ..., -0.1996,  0.3542,  1.7562],\n",
       "         [-0.2457, -0.2471, -0.0510,  ..., -2.1774,  0.8956, -0.6549],\n",
       "         [-0.5985,  1.6512, -0.8037,  ...,  1.5534,  0.1627,  2.2195],\n",
       "         [-1.6036,  0.4944,  0.6576,  ...,  1.2255,  0.8074,  2.3766],\n",
       "         [ 0.2873, -0.4889, -0.5152,  ..., -0.4286,  1.9095, -0.5513]],\n",
       "\n",
       "        [[-0.1813,  0.0983, -0.8991,  ..., -0.1996,  0.3542,  1.7562],\n",
       "         [-0.2457, -0.2471, -0.0510,  ..., -2.1774,  0.8956, -0.6549],\n",
       "         [-0.5985,  1.6512, -0.8037,  ...,  1.5534,  0.1627,  2.2195],\n",
       "         [-1.6036,  0.4944,  0.6576,  ...,  1.2255,  0.8074,  2.3766],\n",
       "         [ 0.2873, -0.4889, -0.5152,  ..., -0.4286,  1.9095, -0.5513]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_embeddings = model.position_embeddings(position_ids)\n",
    "position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b3ce5a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1813,  0.0983, -0.8991,  ..., -0.1996,  0.3542,  1.7562],\n",
       "         [-0.2457, -0.2471, -0.0510,  ..., -2.1774,  0.8956, -0.6549],\n",
       "         [-0.5985,  1.6512, -0.8037,  ...,  1.5534,  0.1627,  2.2195],\n",
       "         [-1.6036,  0.4944,  0.6576,  ...,  1.2255,  0.8074,  2.3766],\n",
       "         [ 0.2873, -0.4889, -0.5152,  ..., -0.4286,  1.9095, -0.5513]],\n",
       "\n",
       "        [[-0.1813,  0.0983, -0.8991,  ..., -0.1996,  0.3542,  1.7562],\n",
       "         [-0.2457, -0.2471, -0.0510,  ..., -2.1774,  0.8956, -0.6549],\n",
       "         [-0.5985,  1.6512, -0.8037,  ...,  1.5534,  0.1627,  2.2195],\n",
       "         [-1.6036,  0.4944,  0.6576,  ...,  1.2255,  0.8074,  2.3766],\n",
       "         [ 0.2873, -0.4889, -0.5152,  ..., -0.4286,  1.9095, -0.5513]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_emb = model.position_embeddings(position_ids)\n",
    "pos_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dbce8174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9225,  1.1107, -0.0111,  ..., -2.3113,  2.5776,  1.0327],\n",
       "         [ 0.4528,  1.4399,  0.8410,  ..., -4.3547,  3.6022, -1.0994],\n",
       "         [ 0.6467,  0.2667, -0.0129,  ...,  0.8187,  2.8086,  3.1922],\n",
       "         [ 1.5365, -0.7661,  1.9692,  ..., -0.5671,  2.0402,  2.0562],\n",
       "         [ 0.8985, -0.8304,  1.7427,  ..., -0.9196,  3.9093,  0.1902]],\n",
       "\n",
       "        [[ 0.6833,  0.4515,  0.7226,  ..., -2.7106,  2.2928,  3.3426],\n",
       "         [ 0.4065, -0.2599, -0.2544,  ..., -4.7195,  4.0781, -0.2462],\n",
       "         [ 0.0629,  1.6372, -0.0887,  ...,  1.0221,  1.7888,  4.6532],\n",
       "         [-0.8642,  0.3574,  2.1328,  ..., -0.3988,  2.6559,  2.8398],\n",
       "         [ 1.0267, -0.6259,  0.9600,  ..., -2.0530,  3.7580, -0.0881]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb + pos_emb + tok_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "bfb58021",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Embedding?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4aa2d4d",
   "metadata": {},
   "source": [
    "## 8.2.5 BertLayer 모듈\n",
    "- BertLayer는 Transformer부분에 해당.\n",
    "- 서브 네트워크로서 Self-Attention을 계산하는 BertAttention과 Self-Attention의 출력을 처리하는 전결합 층인 BertIntermediate, 그리고 Self-Attention 출력과 BertIntermediate에서 처리한 특징량을 더하는 BertOutput 세 가지로 구성됩니다.\n",
    "- BertLayer에 대한 입력은 Embedding 모듈의 출력 또는 앞단의 BertLayer에서의 출력이며 크기는 (batch_size, seq_len, hidden_size)입니다. - BertLayer구현에서 7장 Transformer와 두 가지 다른 점이 있습니다.\n",
    "- 첫째, BertIntermediate 전결합 층 뒤의 활성화 함수에 GELU함수를 사용하는 점입니다. GELU는 기본적으로 RELU와 같은 형태의 함수입니다. 입력이 0이지만 ReLU출력이 거친(매끄러운 변화가 아니라 급격환 변화) 반면 GELU는 입력 0 근처의 출력이 매끄러운 형태입니다.\n",
    "- 둘쩨, Attention이 Multi-Headed Self-Attention입니다. Transformer도 Multi-Headed Self-Attention이지만 7장에서는 이해를 돕기 위하여 단일 Self-Attention으로 구현하였습니다. Multi-Headed Self-Attention은 단순히 Self-Attention이 여러 개 있는것 뿐입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "22a3ba9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 64, 768)"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_attention_heads = config.num_attention_heads\n",
    "attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "all_head_size = num_attention_heads * attention_head_size\n",
    "num_attention_heads, attention_head_size, all_head_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "8bfe266f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSelfAttention(nn.Module):\n",
    "    '''BertAttention의 Self-Attention이다'''\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(BertSelfAttention, self).__init__()\n",
    "        \n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        # num_attention_heads = 12\n",
    "        \n",
    "        self.attention_head_size = int(\n",
    "            config.hidden_size / config.num_attention_heads) # 768 / 12 = 64\n",
    "        self.all_head_size = self.num_attention_heads * \\\n",
    "            self.attention_head_size # = 'hidden_size' : 768\n",
    "        \n",
    "        # Self-Attention의 특징량을 작성하는 전결합 층\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        \n",
    "        # drop out\n",
    "        self.drop_out = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "        \n",
    "    def transpose_for_scores(self, x):\n",
    "        '''Multi-Headed Attention용으로 텐서의 형태 변환\n",
    "        [batch_size, seq_len, hidden] -> [batch_size, 12, seq_len, hidden/12]\n",
    "        '''\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "    \n",
    "    def forward(self, hidden_states, attention_mask, attention_show_fig=False):\n",
    "        '''\n",
    "        hidden_states : Embeddings 모듈 또는 앞단의 BertLayer에서의 출력\n",
    "        attention_mask : Transformer의 마스크와 같은 기능의 마스킹\n",
    "        attention_show_fig : Self-Attention의 가중치를 반환할지 플래그\n",
    "        '''\n",
    "        \n",
    "        # 입력의 전결합 층에서 특징량 변환(Multi-headed Attention 전부 한꺼번에 변환)\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "        \n",
    "        # Multi-Headed Attention용으로 텐서 형태 변환\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "        \n",
    "        # 특징량끼리 곱하여 비슷한 정도를 Attention_scores로 구한다.\n",
    "        attention_scores = torch.matmul(\n",
    "            query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / \\\n",
    "            math.sqrt(self.attention_head_size)\n",
    "        \n",
    "        # 마스크가 있는 부분에 마스크 적용\n",
    "        attention_scores = attention_scores + attention_mask\n",
    "        # 마스크는 곰셈이 아니라 덧셈이 직관적이지만 그 후에 소프트맥스로 정규화하므로\n",
    "        # 마스크된 부분은 -inf로 한다. attention_mask에는 원래 0이나 -inf가 있으므로 덧셈으로 한다.\n",
    "        \n",
    "        # Attention 정규화\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "        \n",
    "        # 드롭아웃\n",
    "        attention_probs = self.drop_out(attention_probs)\n",
    "        \n",
    "        # Attention Map을 곱한다.\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        \n",
    "        # Multi-Headed Attention의 텐서 형태를 원래대로 되돌린다.\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "        \n",
    "        # attention_show일 경우 attention_probs도 반환\n",
    "        if attention_show_fig == True:\n",
    "            return context_layer, attention_probs\n",
    "        elif attention_show_fig == False:\n",
    "            return context_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "4f9763fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertSelfAttention(\n",
       "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (drop_out): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_self_attn = BertSelfAttention(config)\n",
    "bert_self_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "228b5ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding shape : torch.Size([1, 7, 768])\n",
      "embedding to query shape : torch.Size([1, 7, 768])\n",
      "transpose for score shape : torch.Size([1, 12, 7, 64])\n"
     ]
    }
   ],
   "source": [
    "embedding = emb_model(input_ids)\n",
    "print('embedding shape : {}'.format(embedding.shape))\n",
    "query = bert_self_attn.query(embedding)\n",
    "print('embedding to query shape : {}'.format(query.shape))\n",
    "query_layer = bert_self_attn.transpose_for_scores(query)\n",
    "print('transpose for score shape : {}'.format(query_layer.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "e0161c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0441, -1.0624, -0.0035,  ...,  0.4908,  0.4285,  0.1971],\n",
       "         [ 0.1484, -0.3204, -1.8787,  ...,  0.3521, -0.4409,  0.8620],\n",
       "         [ 1.1590, -1.1851, -0.0000,  ...,  0.0799, -0.9569, -0.3837],\n",
       "         ...,\n",
       "         [ 0.0000, -1.5767, -2.3744,  ...,  0.7972,  1.3792,  1.4229],\n",
       "         [-0.1955, -1.1131, -0.6789,  ...,  0.7324, -0.0000, -0.1860],\n",
       "         [ 2.6219, -0.9074, -0.3120,  ...,  1.7529,  0.0000,  0.8767]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "ce28bd21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x size : torch.Size([1, 7, 768]), x size [:-1] : torch.Size([1, 7])\n",
      "new_x_shape : torch.Size([1, 7, 12, 64])\n",
      "*new_x_shape : 1 7 12 64\n",
      "before view x shape : torch.Size([1, 7, 768])\n",
      "after view x shape : torch.Size([1, 7, 12, 64])\n",
      "after view * permute(0, 2, 1, 3) x shape : torch.Size([1, 12, 7, 64])\n"
     ]
    }
   ],
   "source": [
    "x = embedding\n",
    "print('x size : {}, x size [:-1] : {}'.format(x.size(), x.size()[:-1]))\n",
    "new_x_shape = x.size()[:-1] + (num_attention_heads, attention_head_size)\n",
    "print('new_x_shape : {}'.format(new_x_shape))\n",
    "print('*new_x_shape :', *new_x_shape)\n",
    "print('before view x shape : {}'.format(x.shape))\n",
    "x = x.view(*new_x_shape)\n",
    "print('after view x shape : {}'.format(x.shape))\n",
    "x = x.permute(0, 2, 1, 3)\n",
    "print('after view * permute(0, 2, 1, 3) x shape : {}'.format(x.shape))\n",
    "# torch.Size([1, 12, 7, 64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "818219fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2688.0"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5376 / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "b2d45b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 768])"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "ff11815d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5376"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7 * 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "d18e12a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2688])"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = embedding\n",
    "x.view(2, 2688).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "23d40914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 12, 64])"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = embedding\n",
    "new_x_shape = x.size()[:-1] + (num_attention_heads, attention_head_size)\n",
    "x.view(*new_x_shape).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "772c07ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 7 12 64\n"
     ]
    }
   ],
   "source": [
    "print(*new_x_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "db494a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 12])"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "b5a72cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 12, 12, 64])"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()[:-1] + (num_attention_heads, attention_head_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "d3993341",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 7, 64])"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed_key_layer = bert_self_attn.key(embedding)\n",
    "key_layer = bert_self_attn.transpose_for_scores(mixed_key_layer)\n",
    "key_layer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "9155808d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 7, 64])"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed_value_layer = bert_self_attn.key(embedding)\n",
    "value_layer = bert_self_attn.transpose_for_scores(mixed_value_layer)\n",
    "value_layer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "b46b7184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attenscore shape : torch.Size([1, 12, 7, 7])\n",
      "attenscore shape : torch.Size([1, 12, 7, 7])\n",
      "attenscore shape : torch.Size([1, 12, 7, 7])\n",
      "attention_probs shape : torch.Size([1, 12, 7, 7])\n",
      "attention_probs shape : torch.Size([1, 12, 7, 7])\n",
      "context_layer shape : torch.Size([1, 12, 7, 64])\n",
      "after permute & cntignous context_layer shape : torch.Size([1, 7, 12, 64])\n",
      "new_context_layer shape : torch.Size([1, 7, 768])\n",
      "before context_layer shape : torch.Size([1, 7, 12, 64])\n",
      "after view(new_context_layer_shape) context_layer shape : torch.Size([1, 7, 768])\n"
     ]
    }
   ],
   "source": [
    "attention_scores = torch.matmul(\n",
    "            query_layer, key_layer.transpose(-1, -2))\n",
    "print('attenscore shape : {}'.format(attention_scores.shape))\n",
    "attention_scores = attention_scores / \\\n",
    "            math.sqrt(attention_head_size)\n",
    "print('attenscore shape : {}'.format(attention_scores.shape))\n",
    "attention_scores = attention_scores + extended_attention_mask\n",
    "print('attenscore shape : {}'.format(attention_scores.shape))\n",
    "attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "print('attention_probs shape : {}'.format(attention_probs.shape))\n",
    "attention_probs = bert_self_attn.drop_out(attention_probs)\n",
    "print('attention_probs shape : {}'.format(attention_probs.shape))\n",
    "context_layer = torch.matmul(attention_probs, value_layer)\n",
    "print('context_layer shape : {}'.format(context_layer.shape))\n",
    "context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "# .contiguous()는 텐서를 메모리에서 연속적인 구조로 변환해, 특정 연산에서 발생할 수 있는 문제를 방지하고 효율성을 확보하는 데 사용됨. \n",
    "# 비연속적 텐서를 다룰 때 자주 사용.\n",
    "print('after permute & cntignous context_layer shape : {}'.format(context_layer.shape))\n",
    "new_context_layer_shape = context_layer.size()[:-2] + (all_head_size,)\n",
    "print('new_context_layer shape : {}'.format(new_context_layer_shape))\n",
    "print('before context_layer shape : {}'.format(context_layer.shape))\n",
    "context_layer = context_layer.view(*new_context_layer_shape)\n",
    "print('after view(new_context_layer_shape) context_layer shape : {}'.format(context_layer.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc9e8ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "96a68434",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSelfOutput(nn.Module):\n",
    "    '''BertSelfAttention의 출력을 처리하는 전결합 층이다'''\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(BertSelfOutput, self).__init__()\n",
    "        \n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        # 'hidden_dropout_prob': 0.1\n",
    "        \n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        '''\n",
    "        hidden_stats : BertSelfAttention의 출력 텐서\n",
    "        input_tensor : Embeddings 모듈 또는 앞단의 BertLayer에서의 출력\n",
    "        '''\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "e1727faf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertSelfOutput(\n",
       "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (LayerNorm): BertLayerNorm()\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_self_output = BertSelfOutput(config)\n",
    "bert_self_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "938ee27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('attention_probs shape : {}'.format(attention_probs.shape))\n",
    "context_layer = torch.matmul(attention_probs, value_layer)\n",
    "# print('context_layer shape : {}'.format(context_layer.shape))\n",
    "context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "# .contiguous()는 텐서를 메모리에서 연속적인 구조로 변환해, 특정 연산에서 발생할 수 있는 문제를 방지하고 효율성을 확보하는 데 사용됨. \n",
    "# 비연속적 텐서를 다룰 때 자주 사용.\n",
    "# print('after permute & cntignous context_layer shape : {}'.format(context_layer.shape))\n",
    "new_context_layer_shape = context_layer.size()[:-2] + (all_head_size,)\n",
    "# print('new_context_layer shape : {}'.format(new_context_layer_shape))\n",
    "# print('before context_layer shape : {}'.format(context_layer.shape))\n",
    "context_layer = context_layer.view(*new_context_layer_shape)\n",
    "\n",
    "hiddenstates = bert_self_output(context_layer, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "7a71cdcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_layer shape : torch.Size([1, 7, 768])\n",
      "context_layer shape : torch.Size([1, 7, 768])\n",
      "context_layer shape : torch.Size([1, 7, 768])\n",
      "context_layer shape : torch.Size([1, 7, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0043, -0.7704, -0.2196,  ...,  0.3540,  0.6049, -0.1678],\n",
       "         [ 0.0584, -0.1665, -2.0488,  ...,  0.2063, -0.2671,  0.4716],\n",
       "         [ 1.0142, -0.9420, -0.2485,  ...,  0.0330, -0.6917, -0.6882],\n",
       "         ...,\n",
       "         [-0.0578, -1.4090, -2.4367,  ...,  0.6246,  1.3533,  0.9206],\n",
       "         [-0.2972, -0.8860, -0.8465,  ...,  0.5522,  0.2657, -0.4653],\n",
       "         [ 2.3881, -0.6177, -0.6395,  ...,  1.5153,  0.2489,  0.4453]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_layer = torch.matmul(attention_probs, value_layer)\n",
    "# print('context_layer shape : {}'.format(context_layer.shape))\n",
    "context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "# .contiguous()는 텐서를 메모리에서 연속적인 구조로 변환해, 특정 연산에서 발생할 수 있는 문제를 방지하고 효율성을 확보하는 데 사용됨. \n",
    "# 비연속적 텐서를 다룰 때 자주 사용.\n",
    "# print('after permute & cntignous context_layer shape : {}'.format(context_layer.shape))\n",
    "new_context_layer_shape = context_layer.size()[:-2] + (all_head_size,)\n",
    "# print('new_context_layer shape : {}'.format(new_context_layer_shape))\n",
    "# print('before context_layer shape : {}'.format(context_layer.shape))\n",
    "context_layer = context_layer.view(*new_context_layer_shape)\n",
    "# print('after view(new_context_layer_shape) context_layer shape : {}'.format(context_layer.shape))\n",
    "print('context_layer shape : {}'.format(context_layer.shape))\n",
    "context_layer = bert_self_output.dense(context_layer)\n",
    "print('context_layer shape : {}'.format(context_layer.shape))\n",
    "context_layer = bert_self_output.dropout(context_layer)\n",
    "print('context_layer shape : {}'.format(context_layer.shape))\n",
    "context_layer = bert_self_output.LayerNorm(context_layer + embedding)\n",
    "print('context_layer shape : {}'.format(context_layer.shape))\n",
    "context_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "1c9620ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0043, -0.7704, -0.2196,  ...,  0.3540,  0.6049, -0.1678],\n",
       "         [ 0.0584, -0.1665, -2.0488,  ...,  0.2063, -0.2671,  0.4716],\n",
       "         [ 1.0142, -0.9420, -0.2485,  ...,  0.0330, -0.6917, -0.6882],\n",
       "         ...,\n",
       "         [-0.0578, -1.4090, -2.4367,  ...,  0.6246,  1.3533,  0.9206],\n",
       "         [-0.2972, -0.8860, -0.8465,  ...,  0.5522,  0.2657, -0.4653],\n",
       "         [ 2.3881, -0.6177, -0.6395,  ...,  1.5153,  0.2489,  0.4453]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "3429663a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertAttention(nn.Module):\n",
    "    '''BertLayer 모듈의 Self-Attention 부분'''\n",
    "    def __init__(self, config):\n",
    "        super(BertAttention, self).__init__()\n",
    "        self.selfattn = BertSelfAttention(config)\n",
    "        self.output = BertSelfOutput(config)\n",
    "        \n",
    "    def forward(self, input_tensor, attention_mask, attention_show_fig=False):\n",
    "        '''\n",
    "        input_tensor : Embeddings 모듈 또는 앞단의 BertLayer에서의 출력\n",
    "        attention_mask : Transformer의 마스크와 같은 기능의 마스킹\n",
    "        attention_show_fig : Self-Attention의 가중치를 반환할지의 플래그\n",
    "        '''\n",
    "        if attention_show_fig == True:\n",
    "            '''attention_show일 경우 attention_probs도 반환한다.'''\n",
    "            self_output, attention_probs = self.selfattn(input_tensor, attention_mask, attention_show_fig)\n",
    "            attention_output = self.output(self_output, input_tensor)\n",
    "            return attention_output, attention_probs\n",
    "        \n",
    "        elif attention_show_fig == False:\n",
    "            self_output = self.selfattn(input_tensor, attention_mask, attention_show_fig)\n",
    "            attention_output = self.output(self_output, input_tensor)\n",
    "            return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "2fbe0bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    '''Gaussian Error Linear Unit라는 활성화 함수이다\n",
    "    ReLU가 0으로 거칠고 불연속적이므로 연속적으로 매끄럽게 한 셩태의 ReLU이다\n",
    "    '''\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
    "\n",
    "class BertIntermediate(nn.Module):\n",
    "    '''BERT의 TransformerBlock 모듈 FeedForward'''\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(BertIntermediate, self).__init__()\n",
    "        \n",
    "        # 전결합 층 : 'hidden_size': 768, 'intermediate_size': 3072\n",
    "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        # 활성화 함수\n",
    "        self.intermediate_act_fn = gelu\n",
    "        \n",
    "    def forward(self, hidden_states):\n",
    "        '''\n",
    "        hidden_states : BertAttention의 출력\n",
    "        '''\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.intermediate_act_fn(hidden_states) # GELU에 의한 활성화\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "cc4fa29f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertIntermediate(\n",
       "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_intermediate = BertIntermediate(config)\n",
    "bert_intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "963d56d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1538, -0.1689,  0.5047,  ...,  0.0127, -0.1653, -0.0041],\n",
       "         [-0.1561,  0.2039, -0.1468,  ..., -0.1289,  0.0439, -0.0933],\n",
       "         [-0.0907, -0.0419,  0.5353,  ..., -0.1095,  0.3296, -0.1376],\n",
       "         ...,\n",
       "         [-0.0516, -0.1687,  0.0517,  ..., -0.1412,  0.0033,  0.1908],\n",
       "         [-0.1659, -0.0535,  0.4830,  ..., -0.1675,  0.7587,  0.0525],\n",
       "         [-0.1671, -0.1440,  0.2304,  ..., -0.1108, -0.0572,  0.3275]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_intermediate(hiddenstates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "02b27224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 3072])"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hiddenstates = bert_self_output(context_layer, embedding)\n",
    "hiddenstates = bert_intermediate.dense(hiddenstates)\n",
    "hiddenstates = bert_intermediate.intermediate_act_fn(hiddenstates)\n",
    "hiddenstates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "04a8effa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertOutput(nn.Module):\n",
    "    '''BERT의 TransformerBlock 모듈 FeedForward'''\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(BertOutput, self).__init__()\n",
    "        \n",
    "        # 전결합 층 : 'intermediate_size': 3072, 'hidden_size': 768\n",
    "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        \n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
    "        \n",
    "        # 'hidden_dropout_prob': 0.1\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        \n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        '''\n",
    "        hidden_states : BertIntermediate 출력 텐서\n",
    "        input_tensor : BertAttention 출력 텐서\n",
    "        '''\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "0aa4b18b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertOutput(\n",
       "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "  (LayerNorm): BertLayerNorm()\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_output = BertOutput(config)\n",
    "bert_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "6b2ae907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_states shape : torch.Size([1, 7, 3072])\n",
      "after BertOutput hidden_states shape : torch.Size([1, 7, 768])\n"
     ]
    }
   ],
   "source": [
    "hiddenstates = bert_self_output(context_layer, embedding)\n",
    "hiddenstates = bert_intermediate.dense(hiddenstates)\n",
    "hiddenstates = bert_intermediate.intermediate_act_fn(hiddenstates)\n",
    "\n",
    "print('hidden_states shape : {}'.format(hiddenstates.shape))\n",
    "hiddenstates = bert_output(hiddenstates, context_layer)\n",
    "print('after BertOutput hidden_states shape : {}'.format(hiddenstates.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "ac5d1642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before dense layer hiddenstates shape : torch.Size([1, 7, 3072])\n",
      "after dense layer hiddenstates shape : torch.Size([1, 7, 768])\n",
      "after dropout layer hiddenstates shape : torch.Size([1, 7, 768])\n",
      "after LayerNorm layer hiddenstates shape : torch.Size([1, 7, 768])\n"
     ]
    }
   ],
   "source": [
    "hiddenstates = bert_self_output(context_layer, embedding)\n",
    "hiddenstates = bert_intermediate.dense(hiddenstates)\n",
    "hiddenstates = bert_intermediate.intermediate_act_fn(hiddenstates)\n",
    "print('before dense layer hiddenstates shape : {}'.format(hiddenstates.shape))\n",
    "hiddenstates = bert_output.dense(hiddenstates)\n",
    "print('after dense layer hiddenstates shape : {}'.format(hiddenstates.shape))\n",
    "hiddenstates = bert_output.dropout(hiddenstates)\n",
    "print('after dropout layer hiddenstates shape : {}'.format(hiddenstates.shape))\n",
    "hiddenstates = bert_output.LayerNorm(hiddenstates + embedding)\n",
    "print('after LayerNorm layer hiddenstates shape : {}'.format(hiddenstates.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "783f5e1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 768])"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_encoder_layers = [hiddenstates]\n",
    "all_encoder_layers[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af3a33a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e3827f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(nn.Module):\n",
    "    '''BERT의 BertLayer모듈이다. Transformer가 된다.'''\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(BertLayer, self).__init__()\n",
    "        \n",
    "        # Self-Attention 부분\n",
    "        self.attention = BertAttention(config)\n",
    "        \n",
    "        # Self-Attention의 출력을 처리하는 전결합 층\n",
    "        self.intermediate = BertIntermediate(config)\n",
    "        \n",
    "        # Self-Attention에 의한 특징량과 BertLayer에 원래의 입력을 더하는 층\n",
    "        self.output = BertOutput(config)\n",
    "        \n",
    "    def forward(self, hidden_states, attention_mask, attention_show_fig=False):\n",
    "        '''\n",
    "        hidden_states : Embedder 모듈의 출력 텐서 [batch_size, seq_len, hidden_size]\n",
    "        attention_mask : Transformer의 마스크와 같은 기능의 마스킹\n",
    "        attention_show_fig : Self-Attention의 가중치를 반환할지의 플래그\n",
    "        '''\n",
    "        if attention_show_fig == True:\n",
    "            '''attention_show일 경우 attention_probs도 반환한다.'''\n",
    "            attention_output, attention_probs = self.attention(hidden_states, \n",
    "                                                               attention_mask, \n",
    "                                                               attention_show_fig)\n",
    "            \n",
    "            intermediate_output = self.intermediate(attention_output)\n",
    "            \n",
    "            layer_output = self.output(intermediate_output, attention_output)\n",
    "            \n",
    "            return layer_output, attention_probs\n",
    "        \n",
    "        elif attention_show_fig == False:\n",
    "            attention_output = self.attention(hidden_states, \n",
    "                                              attention_mask, \n",
    "                                              attention_show_fig)\n",
    "            \n",
    "            intermediate_output = self.intermediate(attention_output)\n",
    "            \n",
    "            layer_output = self.output(intermediate_output, attention_output)\n",
    "            \n",
    "            return layer_output # [batch_size, seq_length, hidden_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f976ed8",
   "metadata": {},
   "source": [
    "## 8.2.6 BertLayer 모듈의 반복 부분\n",
    "- BERT_Base에서는 BertLayer 모듈(Transformer)을 12회 반복.\n",
    "- 이들을 묶어서 BertEncoder클래스로 만듭니다.\n",
    "- 단순히 BertLayer 12개를 nn.ModuleList에 기재하여 순전파.\n",
    "- 순전파 함수 forward의 인수\n",
    "    - output_all_encoded_layer인수는 반환 값으로 BertLayer에서 출력된 특징량을 12단만큼 모두 반환할지 아니면 12단 최종 층의 특징량만 반환할지 여부를 지정하는 변수.\n",
    "    - 12단의 Transformer 중간에 단어 벡터가 어떻게 변해가는지 확인하고 싶을 때 output_all_encoded_layers인수를 True로 하여 12단 만큼의 단어 벡터를 꺼낼 수 있습니다.\n",
    "    - 단순히 12단 출력만을 사용하여 자연어 처리를 작업하는 경우 False로 하여 최종 BertLayer 모듈 출력만 BertEncoder에서 출력시킨 후 사용\n",
    "    - attention_show_fig인수는 BertLayer 모듈에서 사용했던 변수와 동일\n",
    "    - Self-Attention의 가중치를 출력할지 여부를 지정합니다. BERT_Base의 Attention은 각 층이 12개인 Multi-headed Self-Attention입니다.\n",
    "    - BertEncoder에서 attention_show_fig인수를 True로 한 경우에는 BertLayer 모듈 중 12단 끝에 있는 BertLayer 모듈에서 12개의 Multi-Headed Self-Attention가중치를 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1ea6fcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BertLayer 모듈의 반복 부분이다.\n",
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        '''BertLayer 모듈의 반복 부분'''\n",
    "        super(BertEncoder, self).__init__()\n",
    "        \n",
    "        # config.num_hidden_layers의 값, 즉 12개의 BertLayer 모듈을 만든다.\n",
    "        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        \n",
    "    def forward(self, hidden_states, attention_mask, output_all_encoded_layers=True, attention_show_fig=False):\n",
    "        '''\n",
    "        hidden_states : Embeddings 모듈 출력\n",
    "        attention_mask : Transformer의 마스크와 동일한 기능의 마스킹\n",
    "        output_all_encoded_layers : 반환 값을 전체 TransformerBlock 모듈의 출력으로 할지 마지막 층만으로 한정할지의 플래그\n",
    "        attention_show_fig : Self-Attention의 가중치를 반환할지의 플래그\n",
    "        '''\n",
    "        \n",
    "        # 반환 값으로 사용할 리스트\n",
    "        all_encoder_layers = []\n",
    "        \n",
    "        # BertLayer 모듈의 처리 반복\n",
    "        for layer_module in self.layer:\n",
    "            \n",
    "            if attention_show_fig == True:\n",
    "                '''attention_show의 경우 attention_probs도 반환'''\n",
    "                hidden_states, attention_probs = layer_module(\n",
    "                    hidden_states, attention_mask, attention_show_fig)\n",
    "            elif attention_show_fig == False:\n",
    "                hidden_states = layer_module(\n",
    "                    hidden_states, attention_mask, attention_show_fig)\n",
    "                \n",
    "            # 반환 값으로 BertLayer에서 출력된 특징량만을 사용할 경우의 처리\n",
    "            if output_all_encoded_layers:\n",
    "                all_encoder_layers.append(hidden_states)\n",
    "                    \n",
    "        # 반환 값으로 마지막 BertLayer에서 출력된 특징량만을 사용할 경우의 처리\n",
    "        if not output_all_encoded_layers:\n",
    "            all_encoder_layers.append(hidden_states)\n",
    "            \n",
    "        # attention_show의 경우 attention_probs(마지막 12단)도 반환한다.\n",
    "        if attention_show_fig == True:\n",
    "            return all_encoder_layers, attention_probs\n",
    "        elif attention_show_fig == False:\n",
    "            return all_encoder_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af4ec36",
   "metadata": {},
   "source": [
    "## 8.2.7 BertPooler 모듈\n",
    "- BertPooler 모듈은 BertEncoder출력에서 입력 문장의 첫 번째 단어인 [CLS]부분의 특징량 텐서(1 x 768차원)을 꺼내 전결합 층을 사용한 후 특징량을 변환하는 모듈입니다.\n",
    "- 전결합 층 뒤에 활성화 함수 Tanh을 사용하고 출력을 1에서 -1까지 범위로 합니다. 출력 텐서의 크기는 (batch_size, hidden_size)입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "41b57475",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPooler(nn.Module):\n",
    "    '''입력 문장의 첫 번째 단어 [cls]의 특징량을 반환하고 유지하기 위한 모듈'''\n",
    "    def __init__(self, config):\n",
    "        super(BertPooler, self).__init__()\n",
    "        \n",
    "        # 전결합 층, 'hidden_size':768\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "        \n",
    "    def forward(self, hidden_states):\n",
    "        # 첫 번째 단어의 특징량 취득\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        \n",
    "        # 전결합 층에서 특징량 변환\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        \n",
    "        # 활성화 함수 Tanh을 계산\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        \n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "d041e5a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertPooler(\n",
       "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (activation): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_pooler = BertPooler(config)\n",
    "bert_pooler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "87e94a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 768])"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_encoder_layers[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "ba04534f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hiddenstates = all_encoder_layers[-1]\n",
    "hiddenstates = hiddenstates[:, 0]\n",
    "hiddenstates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "4364827d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_states [:, 0] shape : torch.Size([1, 768])\n",
      "before bert_pooler last all_encoder_layers shape : torch.Size([1, 7, 768])\n",
      "after bert_pooler last all_encoder_layers shape : torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "hiddenstates = all_encoder_layers[-1]\n",
    "# hiddenstates = hiddenstates[:, 0]\n",
    "print('hidden_states [:, 0] shape : {}'.format(hiddenstates[:, 0].shape))\n",
    "print('before bert_pooler last all_encoder_layers shape : {}'.format(hiddenstates.shape))\n",
    "hiddenstates = bert_pooler(hiddenstates)\n",
    "print('after bert_pooler last all_encoder_layers shape : {}'.format(hiddenstates.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "ed2fcd6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768]) tensor([[ 0.0386,  0.0386,  0.8052,  0.2341,  0.2425,  0.0724,  0.4209, -0.7255,\n",
      "          0.3281, -0.6743, -0.2221, -0.4849, -0.8207,  0.2359, -0.3135,  0.7883,\n",
      "         -0.5762,  0.5450,  0.7982, -0.0573,  0.0459, -0.0704, -0.7381,  0.6554,\n",
      "         -0.3530, -0.6023,  0.0130, -0.4715, -0.1234, -0.7505,  0.2417, -0.7780,\n",
      "         -0.2258,  0.1815,  0.6942, -0.0496, -0.5706,  0.7830,  0.5995, -0.1354,\n",
      "         -0.1721,  0.8067, -0.5260,  0.2463, -0.1281,  0.6694, -0.0821, -0.5646,\n",
      "          0.3978, -0.6315,  0.0043, -0.4287,  0.0793,  0.2377, -0.3383, -0.4438,\n",
      "         -0.0436,  0.3002,  0.3886,  0.4272, -0.0567,  0.6415, -0.2289,  0.0599,\n",
      "         -0.5042,  0.4704,  0.0157,  0.5565,  0.1264,  0.5160,  0.3277,  0.1653,\n",
      "         -0.2262, -0.4129,  0.4855, -0.0702,  0.1826,  0.6167, -0.1407, -0.1800,\n",
      "          0.1263,  0.6942, -0.1554, -0.7251, -0.2398, -0.6362, -0.3235,  0.1862,\n",
      "          0.1509,  0.1518, -0.1274,  0.0579, -0.8382, -0.5350, -0.0850, -0.1336,\n",
      "          0.3461, -0.5382, -0.1126, -0.5537,  0.4490, -0.4556,  0.2647, -0.7090,\n",
      "          0.3812, -0.7115, -0.3267, -0.2631,  0.3462,  0.2923,  0.2301, -0.2390,\n",
      "          0.5994,  0.1075, -0.5302, -0.0364,  0.5147, -0.7033, -0.0979,  0.0664,\n",
      "         -0.0747, -0.1602, -0.2028,  0.5225, -0.4343, -0.4369, -0.2517,  0.4568,\n",
      "         -0.2312,  0.8039, -0.0958, -0.6944,  0.8668,  0.2953,  0.2236, -0.5407,\n",
      "         -0.3831,  0.7522,  0.5802, -0.7575, -0.5669,  0.6778,  0.2527,  0.7166,\n",
      "          0.4466,  0.3075, -0.3937,  0.4421, -0.4531,  0.4219,  0.2492, -0.4324,\n",
      "          0.2216, -0.6543,  0.3935, -0.6651, -0.5860,  0.0624,  0.1075,  0.0874,\n",
      "          0.3421, -0.2690, -0.1664, -0.6472,  0.2898,  0.0895, -0.5737,  0.5811,\n",
      "          0.1172,  0.6356, -0.4514,  0.5376,  0.1898, -0.1866,  0.8228, -0.7473,\n",
      "         -0.4921,  0.0895,  0.8552,  0.2705,  0.1117,  0.4038,  0.4308,  0.1346,\n",
      "          0.3111,  0.4396, -0.3213, -0.5571, -0.1318, -0.1437,  0.3239, -0.2731,\n",
      "         -0.2994, -0.8027, -0.1652, -0.6121,  0.7123, -0.3934, -0.1222, -0.2376,\n",
      "         -0.4572, -0.2352,  0.7334,  0.0627, -0.2451,  0.1375,  0.4135, -0.6628,\n",
      "         -0.3031,  0.3302, -0.7674,  0.7567, -0.2777,  0.3738, -0.2930,  0.0249,\n",
      "         -0.8790,  0.1571, -0.0231,  0.3933, -0.5951,  0.0124, -0.2480, -0.1460,\n",
      "         -0.2588,  0.4266, -0.6018, -0.9253, -0.4970, -0.4068, -0.7440,  0.1540,\n",
      "         -0.1618, -0.1885, -0.4867, -0.4057, -0.3173, -0.6961, -0.4587, -0.5469,\n",
      "         -0.7550, -0.2593, -0.5317,  0.3635, -0.2969, -0.8515,  0.2346, -0.6601,\n",
      "          0.2678,  0.0907,  0.0643,  0.5281,  0.3322,  0.3982,  0.4597,  0.0700,\n",
      "          0.1495, -0.3441,  0.6099,  0.4531,  0.6327, -0.3668, -0.4565,  0.1981,\n",
      "         -0.2212,  0.0827,  0.1732,  0.6532,  0.1927, -0.0950,  0.4345,  0.8055,\n",
      "          0.3711,  0.1490,  0.2866, -0.1253,  0.0035,  0.5401,  0.5791,  0.0114,\n",
      "          0.4491,  0.2395, -0.6109,  0.2956,  0.3074,  0.1278,  0.4619, -0.2540,\n",
      "          0.7183,  0.1586, -0.5617,  0.2842,  0.0775,  0.2355, -0.2289,  0.6863,\n",
      "          0.0110, -0.3780,  0.3766,  0.6791, -0.1791,  0.2185,  0.0221, -0.1689,\n",
      "         -0.4672, -0.3477,  0.4023,  0.8281,  0.1866, -0.2504,  0.0829, -0.5773,\n",
      "          0.4375,  0.9163,  0.3489, -0.7644,  0.6969, -0.4405,  0.1708,  0.4243,\n",
      "         -0.3046, -0.0680, -0.6839,  0.6408, -0.8875, -0.6920,  0.6203, -0.4329,\n",
      "          0.1049,  0.6845, -0.2518, -0.5062,  0.0305, -0.8079,  0.6074,  0.2356,\n",
      "          0.1857, -0.0672, -0.7574,  0.1454,  0.7578, -0.8016,  0.1666,  0.0321,\n",
      "          0.5308, -0.3366, -0.1998, -0.3734, -0.2342,  0.0172, -0.2020, -0.6982,\n",
      "          0.6730,  0.0225, -0.0293, -0.7985, -0.0037,  0.7871, -0.5703, -0.5847,\n",
      "          0.7245, -0.1592,  0.4028,  0.6959,  0.2170, -0.0097,  0.7066,  0.6360,\n",
      "         -0.7796, -0.0448, -0.4779,  0.7811, -0.1035, -0.1948,  0.2430,  0.5038,\n",
      "         -0.5196,  0.4525, -0.2629, -0.7937, -0.0305,  0.0234, -0.3451,  0.0431,\n",
      "          0.3568, -0.1886,  0.4642,  0.0338, -0.2741,  0.2314,  0.2878, -0.3759,\n",
      "          0.9185, -0.4229, -0.0771, -0.3513, -0.0914, -0.3751, -0.5239, -0.4229,\n",
      "         -0.4213, -0.6131,  0.4993, -0.6720,  0.2098,  0.5323, -0.6379, -0.3682,\n",
      "         -0.1252,  0.7462,  0.5916,  0.6872, -0.0955, -0.5937, -0.1653, -0.2887,\n",
      "          0.0786,  0.2936,  0.8619,  0.2977,  0.4190, -0.8146,  0.2686, -0.3949,\n",
      "         -0.2834, -0.3866, -0.4384,  0.2503, -0.0635, -0.4721,  0.1414,  0.4472,\n",
      "         -0.4161,  0.0708, -0.0803,  0.4163,  0.2477, -0.0279, -0.4657,  0.2747,\n",
      "         -0.2387,  0.7343,  0.5196, -0.7499,  0.4671,  0.2916,  0.3772, -0.4787,\n",
      "          0.1640, -0.4893, -0.2104, -0.3313, -0.1863,  0.0609, -0.2931,  0.1233,\n",
      "          0.6026, -0.3203, -0.1135, -0.4000, -0.4394, -0.0394, -0.5274, -0.6509,\n",
      "          0.2165,  0.3349,  0.6199, -0.2105,  0.7089, -0.3238,  0.8723,  0.6363,\n",
      "          0.3966,  0.2508, -0.3766,  0.2605, -0.9424, -0.2927, -0.2061, -0.0276,\n",
      "         -0.3786, -0.7093, -0.6499,  0.2826, -0.5312, -0.3173, -0.4238,  0.4704,\n",
      "         -0.9253,  0.4071, -0.2484, -0.3563, -0.0182,  0.2818, -0.5848,  0.6743,\n",
      "         -0.0297,  0.5878, -0.2822, -0.8153, -0.1624,  0.3186,  0.2095, -0.5584,\n",
      "          0.0469, -0.3262,  0.0686,  0.2762,  0.2247, -0.6702,  0.9365,  0.0297,\n",
      "          0.0321, -0.4023,  0.7142, -0.2876, -0.5706,  0.3576, -0.7227, -0.6546,\n",
      "          0.4863, -0.5009,  0.6108, -0.0885, -0.8794, -0.0641,  0.2620,  0.2863,\n",
      "          0.3020,  0.4539,  0.0369, -0.0238,  0.2207, -0.0211, -0.5048, -0.4017,\n",
      "          0.5643,  0.3382,  0.4018, -0.5297, -0.6884,  0.0315, -0.1591, -0.1724,\n",
      "          0.4080, -0.5759,  0.7190,  0.5023, -0.2697,  0.8347,  0.2721, -0.2574,\n",
      "          0.3791,  0.5434, -0.3073, -0.3179,  0.2522, -0.7275,  0.1370, -0.5759,\n",
      "         -0.4180,  0.9101, -0.8752,  0.4980, -0.6804, -0.1843,  0.0204, -0.5679,\n",
      "         -0.2854,  0.1559,  0.2628, -0.3908,  0.5042,  0.1722,  0.4840,  0.0046,\n",
      "          0.4033,  0.9141, -0.0493,  0.5470,  0.2041,  0.3690,  0.4612,  0.7906,\n",
      "         -0.3744,  0.0651,  0.2398,  0.2402, -0.5827, -0.3621, -0.4988,  0.8102,\n",
      "         -0.0274, -0.4231,  0.0400, -0.6101, -0.9155, -0.5730, -0.0073, -0.7598,\n",
      "         -0.2236,  0.4464, -0.5941, -0.5650, -0.2448, -0.0066, -0.2710,  0.8360,\n",
      "          0.0322, -0.3147,  0.1939,  0.0769, -0.7333, -0.3073, -0.1595, -0.3336,\n",
      "         -0.3786,  0.0398,  0.3596,  0.4513,  0.1604,  0.5214,  0.4230,  0.5232,\n",
      "          0.3370, -0.0499,  0.0960,  0.0187,  0.1965,  0.1019, -0.5178,  0.2849,\n",
      "         -0.1180, -0.1473,  0.0951,  0.1650, -0.4007, -0.1387, -0.4758, -0.1579,\n",
      "         -0.4955,  0.2213,  0.2540, -0.5980,  0.5838,  0.5364,  0.7422,  0.1568,\n",
      "         -0.4605, -0.0303,  0.1130,  0.1206,  0.3863, -0.3222, -0.6163,  0.6378,\n",
      "         -0.7401,  0.1165,  0.3955,  0.8046, -0.2064, -0.7684, -0.5256,  0.3207,\n",
      "          0.7959,  0.8875,  0.0646, -0.5330, -0.1211, -0.8201, -0.4236,  0.7094,\n",
      "         -0.6921, -0.6503, -0.7666,  0.1210, -0.4131,  0.8378,  0.7742,  0.2615,\n",
      "          0.2376, -0.7369,  0.2247,  0.1045, -0.1960, -0.6732, -0.2540,  0.4956,\n",
      "          0.2500, -0.1930, -0.2066,  0.3299,  0.7253, -0.3957, -0.0683, -0.1763,\n",
      "         -0.4217,  0.4831, -0.6741, -0.2297,  0.3287,  0.2939,  0.2376, -0.2887,\n",
      "         -0.6998,  0.1722,  0.6771, -0.6147,  0.3688,  0.7067, -0.3403,  0.3789,\n",
      "         -0.4075,  0.0991, -0.5051, -0.5671,  0.5889, -0.4023, -0.2757, -0.3935,\n",
      "          0.1702,  0.4301,  0.3582,  0.0883, -0.5134,  0.8073, -0.3024,  0.2570,\n",
      "         -0.0594, -0.3837, -0.1611,  0.0018,  0.2530,  0.1895, -0.7771, -0.5133,\n",
      "          0.1659,  0.4427, -0.2898, -0.1857,  0.4277, -0.0736, -0.1988, -0.3739,\n",
      "         -0.2328,  0.2380, -0.0679, -0.5957, -0.1217, -0.0405, -0.6725,  0.5643,\n",
      "         -0.8804,  0.0227, -0.5640, -0.0629, -0.3174, -0.8435, -0.5185,  0.2633,\n",
      "         -0.6471, -0.7615, -0.6901,  0.4065,  0.2106, -0.3756,  0.5762, -0.0372]],\n",
      "       grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "hiddenstates = all_encoder_layers[-1]\n",
    "hiddenstates = hiddenstates[:, 0]\n",
    "hiddenstates = bert_pooler.dense(hiddenstates)\n",
    "hiddenstates = bert_pooler.activation(hiddenstates)\n",
    "print(hiddenstates.shape, hiddenstates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a6f256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86da244d",
   "metadata": {},
   "source": [
    "## 8.2.8 동작 확인\n",
    "- 미니 배치의 크기를 2, 각 미니 배치의 문장 길이르 5로 하여 입력을 적당히 생성\n",
    "- 길이 5에 두 문장이 포함되어 있음. 어떠한 단어까지 첫 번째 문장이고 어떠한 단어부터 두 번째 문장인지 나타내는 문장 ID와 Attention용 마스크도 생성. 이러한 입력으로 동작을 확인\n",
    "- Attention용 마스크를 확장한 extended_attention_mask 변수를 작성한다는 점을 주의해야 한다\n",
    "- Multi-Headed Self-Attention에서 Attention마스크를 사용할 수 있도록 하는 변환.\n",
    "- Attention을 적용하지 않는 부분은 시그모이드를 계산했을 때 0이 되도록 마이너스 무한의 대안으로써 -10000을 대입."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2d97169e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 단어 ID열의 텐서 크기:  torch.Size([2, 5])\n",
      "입력 마스크의 텐서 크기:  torch.Size([2, 5])\n",
      "입력 문장 ID의 텐서 크기:  torch.Size([2, 5])\n",
      "확장된 마스크의 텐서 크기:  torch.Size([2, 1, 1, 5])\n",
      "BertEmbeddings의 출력 텐서 크기: torch.Size([2, 5, 768])\n",
      "BertEncoder 최후 층의 출력 텐서 크기: torch.Size([2, 5, 768])\n",
      "BertPooler의 출력 텐서 크기: torch.Size([2, 768])\n"
     ]
    }
   ],
   "source": [
    "# 동작 확인\n",
    "\n",
    "# 입력 단어 ID열 batch_size는 두 가지\n",
    "input_ids = torch.LongTensor([[31, 51, 12, 23, 99], [15, 5, 1, 0, 0]])\n",
    "print('입력 단어 ID열의 텐서 크기: ', input_ids.shape)\n",
    "# 마스크\n",
    "attention_mask = torch.LongTensor([[1, 1, 1, 1, 1], [1, 1, 1, 0, 0]])\n",
    "print('입력 마스크의 텐서 크기: ', attention_mask.shape)\n",
    "\n",
    "# 문장의 ID, 두 미니 배치 각각에 대한 0은 첫 번째 문장을, 1은 두 번째 문장을 나타낸다.\n",
    "token_type_ids = torch.LongTensor([[0, 0, 1, 1, 1], [0, 1, 1, 1, 1]])\n",
    "print('입력 문장 ID의 텐서 크기: ', token_type_ids.shape)\n",
    "\n",
    "# BERT의 각 모듈 준비\n",
    "embeddings = BertEmbeddings(config)\n",
    "encoder = BertEncoder(config)\n",
    "pooler = BertPooler(config)\n",
    "\n",
    "# 마스크 변형 [batch_size, 1, 1, seq_length]로 한다.\n",
    "# Attention을 적용하지 않는 부분은 마이너스 무한으로 하고 위하여 -10000을 곱한다.\n",
    "extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "extended_attention_mask = extended_attention_mask.to(dtype=torch.float32)\n",
    "extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "print('확장된 마스크의 텐서 크기: ', extended_attention_mask.shape)\n",
    "\n",
    "# 순전파\n",
    "out1 = embeddings(input_ids, token_type_ids)\n",
    "print('BertEmbeddings의 출력 텐서 크기:', out1.shape)\n",
    "\n",
    "out2 = encoder(out1, extended_attention_mask)\n",
    "# out2는 [minibatch, seq_length, embedding_dim]이 12개 리스트\n",
    "print('BertEncoder 최후 층의 출력 텐서 크기:', out2[0].shape)\n",
    "\n",
    "out3 = pooler(out2[-1]) # out2는 12층의 특징량 리스트가 되어 가장 마지막을 사용\n",
    "print('BertPooler의 출력 텐서 크기:', out3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4a7e0a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5]),\n",
       " torch.Size([2, 1, 1, 5]),\n",
       " tensor([[[[1, 1, 1, 1, 1]]],\n",
       " \n",
       " \n",
       "         [[[1, 1, 1, 0, 0]]]]))"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "attention_mask.shape, extended_attention_mask.shape, extended_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "3ce1487e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1, 1, 5]),\n",
       " tensor([[[[1., 1., 1., 1., 1.]]],\n",
       " \n",
       " \n",
       "         [[[1., 1., 1., 0., 0.]]]]))"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extended_attention_mask = extended_attention_mask.to(dtype=torch.float32)\n",
    "extended_attention_mask.shape, extended_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "06bd3913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1, 1, 5]),\n",
       " tensor([[[[    -0.,     -0.,     -0.,     -0.,     -0.]]],\n",
       " \n",
       " \n",
       "         [[[    -0.,     -0.,     -0., -10000., -10000.]]]]))"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "extended_attention_mask.shape, extended_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "388b69c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5183,  0.8744, -0.6474,  ..., -0.1523,  0.0000, -1.8388],\n",
       "         [-0.0000, -2.2031,  0.7122,  ...,  0.2697, -0.0000, -1.0999],\n",
       "         [-0.5974,  0.7153, -1.2055,  ...,  1.6777,  0.6225,  0.5458],\n",
       "         [ 1.4920,  0.1230, -0.0000,  ...,  0.1844, -1.1844,  0.2829],\n",
       "         [-0.4239,  0.1712, -1.7108,  ..., -0.6405, -1.0815,  0.5137]],\n",
       "\n",
       "        [[ 1.0652, -0.2365, -0.2122,  ...,  1.3648,  1.8276, -0.9468],\n",
       "         [ 0.7128, -2.1402,  0.8725,  ..., -1.3434, -1.8721, -0.1389],\n",
       "         [-0.4667,  0.1940, -0.7100,  ...,  0.3417,  0.9378,  0.1089],\n",
       "         [ 0.0000,  0.5578, -0.6825,  ...,  0.3566, -2.2600,  0.7890],\n",
       "         [-1.0589, -0.3668, -1.6822,  ..., -1.2145, -1.0332,  1.1162]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "41913227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5, 768]),\n",
       " 12,\n",
       " torch.Size([2, 5, 768]),\n",
       " tensor([[[ 5.0371e-01,  8.2984e-01, -5.0412e-01,  ...,  7.2131e-02,\n",
       "            1.7786e-01, -2.2644e+00],\n",
       "          [ 5.6436e-01, -1.9141e+00,  6.7024e-01,  ...,  4.3499e-01,\n",
       "            4.4968e-01, -9.2028e-01],\n",
       "          [-1.4776e-01,  9.9116e-01, -1.0737e+00,  ...,  1.8967e+00,\n",
       "            4.7691e-01,  3.4587e-01],\n",
       "          [ 1.6951e+00,  4.8224e-01, -1.2518e-01,  ...,  4.8702e-01,\n",
       "           -1.2383e+00,  2.1447e-01],\n",
       "          [-2.1283e-01,  7.6516e-01, -1.3443e+00,  ..., -5.4793e-01,\n",
       "           -9.2073e-01,  3.2209e-01]],\n",
       " \n",
       "         [[ 1.1051e+00, -2.8525e-01,  3.5683e-01,  ...,  1.5262e+00,\n",
       "            1.9065e+00, -1.0319e+00],\n",
       "          [ 6.1323e-01, -1.6044e+00,  1.1423e+00,  ..., -8.1751e-01,\n",
       "           -1.8087e+00, -3.5258e-04],\n",
       "          [ 5.6175e-02, -1.1095e-01, -2.0454e-01,  ...,  1.0018e+00,\n",
       "            6.1780e-01, -8.1472e-02],\n",
       "          [-1.1993e-02,  4.7299e-01, -4.4785e-01,  ...,  8.5274e-01,\n",
       "           -1.8880e+00,  4.9705e-01],\n",
       "          [-8.5024e-01, -1.3195e-01, -1.0566e+00,  ..., -9.5625e-01,\n",
       "           -7.2232e-01,  5.4782e-01]]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2 = encoder(out1, extended_attention_mask)\n",
    "out1.shape, len(out2), out2[0].shape, out2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "37b71507",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macos/anaconda3/envs/dl/lib/python3.8/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(510)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = nn.Softmax()\n",
    "torch.argmax(softmax(out3[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110843aa",
   "metadata": {},
   "source": [
    "## 8.2.9 모두 연결하여 BERT모델로\n",
    "- 동작을 확인한 후 문제가 없다면 모두 연결한 BERT 모델로 지정."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "92faacb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7])"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "d62fa6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention 마스크와 첫 번째, 두 번째 문장의 id가 없으면 작성\n",
    "if attention_mask is None:\n",
    "    attention_mask = torch.one_like(input_ids)\n",
    "if token_type_ids is None:\n",
    "    token_type_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "# 마스크 변형 [minibatch, 1, 1, seq_length]로 한다.\n",
    "# 나중에 Multi-Headed Self-Attention에서 사용할 수 있는 형태로 하기 위하여\n",
    "extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "# 마스크는 0, 1 이지만 소프트맥스를 계산할 때 마스크가 되도록 0과 -inf로 한다.\n",
    "# -inf 대신 -10000으로 한다\n",
    "extended_attention_mask = extended_attention_mask.to(dtype=torch.float32)\n",
    "extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "31861d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_mask shape : torch.Size([1, 7])\n",
      "attention_mask unsqueeze shape : torch.Size([1, 1, 7])\n",
      "attention_mask unsqueeze 1 and unsqueeze 2 shape : torch.Size([1, 1, 1, 7])\n",
      "extended_attention_mask shape : torch.Size([1, 1, 1, 7])\n",
      "extended_attention_mask : tensor([[[[1., 1., 1., 1., 1., 1., 1.]]]])\n",
      "masked extended_attention_mask : tensor([[[[-0., -0., -0., -0., -0., -0., -0.]]]])\n"
     ]
    }
   ],
   "source": [
    "print('attention_mask shape : {}'.format(attention_mask.shape))\n",
    "print('attention_mask unsqueeze shape : {}'.format(attention_mask.unsqueeze(1).shape))\n",
    "print('attention_mask unsqueeze 1 and unsqueeze 2 shape : {}'.format(attention_mask.unsqueeze(1).unsqueeze(2).shape))\n",
    "extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "print('extended_attention_mask shape : {}'.format(extended_attention_mask.shape))\n",
    "extended_attention_mask = extended_attention_mask.to(dtype=torch.float32)\n",
    "print('extended_attention_mask : {}'.format(extended_attention_mask))\n",
    "extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "print('masked extended_attention_mask : {}'.format(extended_attention_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "35ac2b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.4897e-01, -2.1518e+00, -4.9180e-01,  ...,  1.0964e+00,\n",
       "           1.2787e+00,  8.3720e-01],\n",
       "         [-1.6423e+00,  1.6222e+00,  5.3157e-01,  ...,  4.2774e-01,\n",
       "           2.4528e+00, -0.0000e+00],\n",
       "         [-5.4673e-01, -1.0264e+00, -9.7880e-04,  ..., -7.9996e-01,\n",
       "           0.0000e+00,  4.2281e-01],\n",
       "         ...,\n",
       "         [ 1.3268e-02, -1.8150e+00, -0.0000e+00,  ..., -3.5489e-01,\n",
       "          -2.5589e-01,  9.7428e-01],\n",
       "         [ 4.2085e-01, -2.6400e+00, -1.5607e+00,  ...,  2.9930e-01,\n",
       "           1.1933e+00,  7.1514e-01],\n",
       "         [ 4.9037e-01, -1.5325e+00,  7.8137e-01,  ..., -0.0000e+00,\n",
       "           1.1185e+00,  9.2136e-02]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_output = final_model.embeddings(input_ids, token_type_ids)\n",
    "embedding_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c63821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "89d406c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(nn.Module):\n",
    "    '''모듈을 전부 연결한 BERT 모델'''\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(BertModel, self).__init__()\n",
    "        \n",
    "        # 세 가지 모듈 작성\n",
    "        self.embeddings = BertEmbeddings(config)\n",
    "        self.encoder = BertEncoder(config)\n",
    "        self.pooler = BertPooler(config)\n",
    "        \n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, \n",
    "                output_all_encoded_layers=True,\n",
    "                attention_show_fig=False):\n",
    "        '''\n",
    "        input_ids : [batch_size, sequence_length] 문장의 단어 ID 나열\n",
    "        token_type_ids : [batch_size, sequence_length] 각 단어가 첫 번째 문장인지 두 번째 문장인지 나타내는 id\n",
    "        attention_mask: Transformer의 마스크와 같은 기능의 마스킹\n",
    "        output_all_encoded_layers : 마지막 출력에 12단의 Transformer 모두 리스트로 반환할지 마지막만인지 지정\n",
    "        attention_show_fig : Self-Attention의 가중치를 반환할지 플래그\n",
    "        '''\n",
    "        \n",
    "        # Attention 마스크와 첫 번째, 두 번째 문장의 id가 없으면 작성\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.one_like(input_ids)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "            \n",
    "        # 마스크 변형 [minibatch, 1, 1, seq_length]로 한다.\n",
    "        # 나중에 Multi-Headed Self-Attention에서 사용할 수 있는 형태로 하기 위하여\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "        \n",
    "        # 마스크는 0, 1 이지만 소프트맥스를 계산할 때 마스크가 되도록 0과 -inf로 한다.\n",
    "        # -inf 대신 -10000으로 한다\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=torch.float32)\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "        \n",
    "        # 순전파 시킨다\n",
    "        # BertEmbeddings 모듈\n",
    "        embedding_output = self.embeddings(input_ids, token_type_ids)\n",
    "        \n",
    "        # BertLayer 모듈(Transformer)을 반복하는 BertEncoder 모듈\n",
    "        if attention_show_fig == True:\n",
    "            '''attention_show의 경우 attention_probs도 반환'''\n",
    "            \n",
    "            encoded_layers, attention_probs = self.encoder(embedding_output,\n",
    "                                                           extended_attention_mask,\n",
    "                                                           output_all_encoded_layers,\n",
    "                                                           attention_show_fig)\n",
    "            \n",
    "        elif attention_show_fig == False:\n",
    "            encoded_layers = self.encoder(embedding_output,\n",
    "                                          extended_attention_mask,\n",
    "                                          output_all_encoded_layers,\n",
    "                                          attention_show_fig)\n",
    "            \n",
    "        # BertPooler 모듈\n",
    "        # 인코더의 맨 마지막 BertLayer에서 출력된 특징량 사용\n",
    "        pooled_output = self.pooler(encoded_layers[-1])\n",
    "        \n",
    "        # output_all_encoded_layer가 False인 경우는 리스트가 아닌 텐서를 반환\n",
    "        if not output_all_encoded_layers:\n",
    "            encoded_layers = encoded_layers[-1]\n",
    "            \n",
    "        # attention_show의 경우 attention_probs(가장 마지막)도 반환한다.\n",
    "        if attention_show_fig == True:\n",
    "            return encoded_layers, pooled_output, attention_probs\n",
    "        elif attention_show_fig == False:\n",
    "            return encoded_layers, pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "f8585e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.max_position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7a643972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (selfattn): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (drop_out): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model = BertModel(config)\n",
    "final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "daea0ae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===========================================================================\n",
       "Layer (type:depth-idx)                             Param #\n",
       "===========================================================================\n",
       "BertModel                                          --\n",
       "├─BertEmbeddings: 1-1                              --\n",
       "│    └─Embedding: 2-1                              23,440,896\n",
       "│    └─Embedding: 2-2                              393,216\n",
       "│    └─Embedding: 2-3                              1,536\n",
       "│    └─BertLayerNorm: 2-4                          1,536\n",
       "│    └─Dropout: 2-5                                --\n",
       "├─BertEncoder: 1-2                                 --\n",
       "│    └─ModuleList: 2-6                             --\n",
       "│    │    └─BertLayer: 3-1                         7,087,872\n",
       "│    │    └─BertLayer: 3-2                         7,087,872\n",
       "│    │    └─BertLayer: 3-3                         7,087,872\n",
       "│    │    └─BertLayer: 3-4                         7,087,872\n",
       "│    │    └─BertLayer: 3-5                         7,087,872\n",
       "│    │    └─BertLayer: 3-6                         7,087,872\n",
       "│    │    └─BertLayer: 3-7                         7,087,872\n",
       "│    │    └─BertLayer: 3-8                         7,087,872\n",
       "│    │    └─BertLayer: 3-9                         7,087,872\n",
       "│    │    └─BertLayer: 3-10                        7,087,872\n",
       "│    │    └─BertLayer: 3-11                        7,087,872\n",
       "│    │    └─BertLayer: 3-12                        7,087,872\n",
       "├─BertPooler: 1-3                                  --\n",
       "│    └─Linear: 2-7                                 590,592\n",
       "│    └─Tanh: 2-8                                   --\n",
       "===========================================================================\n",
       "Total params: 109,482,240\n",
       "Trainable params: 109,482,240\n",
       "Non-trainable params: 0\n",
       "==========================================================================="
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ti.summary(final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "43037cf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertLayer(\n",
       "  (attention): BertAttention(\n",
       "    (selfattn): BertSelfAttention(\n",
       "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (drop_out): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (output): BertSelfOutput(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (intermediate): BertIntermediate(\n",
       "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "  )\n",
       "  (output): BertOutput(\n",
       "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.encoder.layer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "04cb0177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertSelfAttention(\n",
       "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (drop_out): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.encoder.layer[0].attention.selfattn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "9b175104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_layers의 텐서 크기: torch.Size([2, 5, 768])\n",
      "pooled_output의 텐서 크기: torch.Size([2, 768])\n",
      "attention_probs의 텐서 크기: torch.Size([2, 12, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "# 동작 확인\n",
    "# 입력 준비\n",
    "input_ids = torch.LongTensor([[31, 52, 12, 23, 99], [15, 5, 1, 0, 0]])\n",
    "attention_mask = torch.LongTensor([[1, 1, 1, 1, 1], [1, 1, 1, 0, 0]])\n",
    "token_type_ids = torch.LongTensor([[0, 0, 1, 1, 1], [0, 1, 1, 1, 1]])\n",
    "\n",
    "# BERT 모델을 만든다.\n",
    "net = BertModel(config)\n",
    "\n",
    "# 순전파\n",
    "encoded_layers, pooled_output, attention_probs = net(\n",
    "    input_ids, token_type_ids, attention_mask, output_all_encoded_layers=True, attention_show_fig=True)\n",
    "\n",
    "print('encoded_layers의 텐서 크기:', encoded_layers[-1].shape)\n",
    "print('pooled_output의 텐서 크기:', pooled_output.shape)\n",
    "print('attention_probs의 텐서 크기:', attention_probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "04016bf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[ 1.2406,  0.8388, -0.1334,  ..., -0.8475,  0.3371,  1.5810],\n",
       "          [-0.2435,  0.0123, -0.2764,  ..., -0.3069,  0.9863,  0.6150],\n",
       "          [-0.3007,  0.1235,  0.7693,  ..., -0.7627,  0.6404, -0.5967],\n",
       "          [-0.4496, -0.4860, -0.5400,  ..., -1.0434,  2.0181, -1.1252],\n",
       "          [-1.3615,  0.2910, -0.8254,  ..., -1.0138,  1.2273,  0.0501]],\n",
       " \n",
       "         [[-0.1457, -1.6330, -0.6535,  ..., -0.2874,  0.0074,  1.1234],\n",
       "          [ 0.5560,  0.3806, -0.4249,  ...,  0.5411,  0.9022, -2.1891],\n",
       "          [-0.2282,  0.1721,  0.5840,  ..., -1.1978,  1.1644, -1.8055],\n",
       "          [-0.8305, -0.8269,  0.5051,  ...,  0.0763,  0.9785, -2.0397],\n",
       "          [-1.0807, -0.3496,  1.1301,  ..., -0.8619,  1.5265, -0.7355]]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([[[ 0.7210,  0.8323,  0.2814,  ..., -0.8977, -0.2409,  1.2964],\n",
       "          [-0.4924, -0.1004,  0.6234,  ..., -0.4198,  1.2488,  0.4650],\n",
       "          [-0.7094,  0.1843,  1.0799,  ..., -1.0826,  0.3717, -0.3686],\n",
       "          [-0.5833, -0.2877, -0.2358,  ..., -0.9736,  1.5546, -0.7453],\n",
       "          [-2.0493,  0.4425, -0.4189,  ..., -1.2346,  1.1235, -0.0434]],\n",
       " \n",
       "         [[-0.5049, -1.4892,  0.3522,  ..., -0.6165, -0.0301,  0.9840],\n",
       "          [-0.0389,  0.6686,  0.4330,  ...,  0.5689,  0.8516, -2.1567],\n",
       "          [-0.5728,  0.2055,  1.0299,  ..., -1.2758,  1.1224, -1.7118],\n",
       "          [-1.0872, -0.7453,  1.1442,  ...,  0.2458,  0.5267, -1.8222],\n",
       "          [-2.1064,  0.0056,  1.5011,  ..., -0.7483,  1.3275, -0.6555]]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([[[ 0.6296,  0.6590,  0.1450,  ..., -0.7264,  0.0446,  1.1457],\n",
       "          [-0.3044, -0.2115,  0.3629,  ..., -0.4365,  1.2016,  0.4032],\n",
       "          [-0.4407, -0.1459,  0.5577,  ..., -1.0276,  0.8706, -0.4891],\n",
       "          [-0.4593, -0.1930, -0.4218,  ..., -1.0248,  1.4431, -0.5957],\n",
       "          [-1.6815,  0.3374, -0.6352,  ..., -1.1543,  1.1859, -0.1013]],\n",
       " \n",
       "         [[-0.3137, -1.6160,  0.0315,  ..., -0.4573,  0.2087,  0.8620],\n",
       "          [-0.0760,  0.2541,  0.4007,  ...,  0.4135,  0.7918, -1.7994],\n",
       "          [-0.4150,  0.0135,  0.6006,  ..., -1.0944,  1.5436, -2.3291],\n",
       "          [-1.3061, -0.6275,  1.0332,  ...,  0.2537,  0.8186, -1.3072],\n",
       "          [-2.0960, -0.0265,  1.3089,  ..., -0.6706,  1.9784, -0.5648]]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([[[ 0.9566,  0.6135,  0.4812,  ..., -0.4071, -0.1410,  0.7506],\n",
       "          [-0.1397, -0.1053,  0.6219,  ..., -0.5250,  0.8802,  0.0382],\n",
       "          [-0.0258,  0.0513,  0.6220,  ..., -1.0633,  0.7466, -0.6839],\n",
       "          [-0.0555, -0.1675, -0.2616,  ..., -1.0152,  1.4574, -0.7292],\n",
       "          [-1.4029,  0.5067, -0.7165,  ..., -1.4930,  1.0689, -0.4664]],\n",
       " \n",
       "         [[-0.2423, -1.6904,  0.3744,  ..., -0.3032,  0.2152,  0.4423],\n",
       "          [-0.3678,  0.0388,  0.6330,  ...,  0.4730,  0.6991, -1.8686],\n",
       "          [-0.6391,  0.1158,  0.8824,  ..., -0.8222,  1.4741, -2.4874],\n",
       "          [-1.0866, -0.4557,  1.2126,  ...,  0.3940,  0.6952, -1.2255],\n",
       "          [-2.1273,  0.1254,  1.4401,  ..., -0.6358,  1.9175, -0.7028]]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([[[ 0.6663,  0.6369,  0.6821,  ..., -0.1440, -0.3655,  0.6220],\n",
       "          [-0.2725, -0.3241,  0.9071,  ..., -0.1299,  0.5661,  0.2214],\n",
       "          [-0.0185,  0.0239,  0.9691,  ..., -0.7463,  0.6734, -0.7454],\n",
       "          [ 0.0281, -0.5053, -0.3512,  ..., -0.8379,  1.3153, -0.9085],\n",
       "          [-1.3435,  0.4095, -0.7446,  ..., -1.2716,  0.8428, -0.4366]],\n",
       " \n",
       "         [[-0.5814, -1.7143,  0.4377,  ..., -0.1577,  0.0316,  0.6651],\n",
       "          [ 0.0914, -0.0591,  0.7199,  ...,  0.8736,  0.2894, -1.6209],\n",
       "          [-0.2226, -0.3139,  0.9386,  ..., -0.3626,  1.4177, -2.4403],\n",
       "          [-0.4217, -0.4592,  1.1507,  ...,  0.2441,  0.5498, -0.9500],\n",
       "          [-1.5816, -0.0300,  1.5092,  ..., -0.1369,  1.5576, -0.7337]]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([[[ 1.0754,  0.3676,  0.5312,  ..., -0.3107, -0.7330,  0.3519],\n",
       "          [ 0.1143, -0.3655,  0.9172,  ..., -0.0216,  0.5624, -0.0976],\n",
       "          [-0.0250, -0.1126,  1.1896,  ..., -1.1356,  0.5725, -0.4701],\n",
       "          [ 0.2157, -0.2025,  0.2171,  ..., -1.0756,  1.3006, -0.7495],\n",
       "          [-0.9492,  0.5916, -0.7269,  ..., -1.4814,  0.8976, -0.0469]],\n",
       " \n",
       "         [[-0.5404, -1.4421,  0.4465,  ..., -0.3242, -0.2647,  0.6472],\n",
       "          [ 0.0807,  0.0339,  0.8511,  ...,  0.8219,  0.1039, -2.0342],\n",
       "          [-0.3479, -0.0582,  1.3397,  ..., -0.6850,  1.1064, -2.5746],\n",
       "          [-0.5545, -0.1138,  1.5925,  ...,  0.3157,  0.3598, -0.9871],\n",
       "          [-1.4992,  0.4455,  1.4890,  ..., -0.1159,  1.1465, -1.0172]]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([[[ 0.9536,  0.6501,  0.9133,  ..., -0.4891, -0.8852, -0.0754],\n",
       "          [ 0.1419, -0.5881,  0.9458,  ..., -0.4647,  0.7322, -0.7429],\n",
       "          [ 0.1627,  0.0369,  1.2475,  ..., -1.5565,  0.5504, -0.6373],\n",
       "          [ 0.5968, -0.3942,  0.3636,  ..., -1.2237,  1.4419, -0.9596],\n",
       "          [-0.8695,  0.4715, -0.6574,  ..., -1.5905,  1.0427, -0.5708]],\n",
       " \n",
       "         [[-0.4323, -1.3965,  0.1870,  ..., -0.0679, -0.0675, -0.0993],\n",
       "          [-0.0149, -0.1457,  0.9223,  ...,  1.0404,  0.1814, -3.0151],\n",
       "          [-0.0322,  0.3126,  1.1964,  ..., -0.6429,  1.0945, -3.1694],\n",
       "          [-0.4298, -0.1286,  1.4449,  ...,  0.3527,  0.8057, -1.5302],\n",
       "          [-1.2939,  0.3569,  0.9484,  ...,  0.0834,  1.1960, -1.6932]]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([[[ 1.1026,  0.7715,  0.2847,  ..., -1.0049, -0.6151, -0.1837],\n",
       "          [-0.0423, -0.4114,  0.5035,  ..., -0.9304,  0.8969, -0.7916],\n",
       "          [-0.0047,  0.1774,  1.0115,  ..., -1.6500,  0.7455, -0.4636],\n",
       "          [ 0.0889, -0.0984,  0.0986,  ..., -1.5829,  1.6132, -1.0917],\n",
       "          [-0.9545,  0.4524, -0.9836,  ..., -1.8564,  1.2866, -0.6231]],\n",
       " \n",
       "         [[-0.6776, -0.9875,  0.2648,  ..., -0.5890,  0.0199,  0.0905],\n",
       "          [-0.4257,  0.1236,  0.7455,  ...,  0.7914,  0.2932, -2.8812],\n",
       "          [-0.4815,  0.6520,  1.0180,  ..., -0.4499,  1.0363, -2.6742],\n",
       "          [-0.8043,  0.1178,  1.3378,  ...,  0.2760,  1.2208, -1.5099],\n",
       "          [-1.6083,  0.5291,  1.0574,  ..., -0.1406,  1.5206, -1.5956]]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([[[ 1.1494,  0.5508,  0.5586,  ..., -1.0512, -0.4095, -0.2766],\n",
       "          [-0.2175, -0.3351,  0.3947,  ..., -0.9671,  0.9674, -1.1552],\n",
       "          [ 0.1015,  0.2564,  1.5277,  ..., -1.8917,  0.7194, -0.5332],\n",
       "          [ 0.1593, -0.2434,  0.0224,  ..., -1.7578,  1.6961, -1.1257],\n",
       "          [-0.8700,  0.4854, -0.5615,  ..., -1.9831,  1.7465, -1.2352]],\n",
       " \n",
       "         [[-0.7714, -0.7905,  0.3829,  ..., -0.9687, -0.2305, -0.0757],\n",
       "          [-0.3927,  0.2579,  0.9038,  ...,  0.2951,  0.2564, -2.9312],\n",
       "          [-0.2619,  0.6337,  1.6653,  ..., -1.1059,  0.6803, -2.4047],\n",
       "          [-0.5164,  0.0789,  1.6766,  ..., -0.2078,  0.9975, -1.8034],\n",
       "          [-1.2719,  0.6130,  1.2931,  ..., -0.6050,  1.2612, -1.6682]]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([[[ 0.6736,  0.2036,  0.6292,  ..., -1.0411,  0.0115,  0.1340],\n",
       "          [-0.5872, -0.2332,  0.4301,  ..., -0.7337,  1.0997, -1.1231],\n",
       "          [ 0.0565,  0.1737,  1.4271,  ..., -2.0861,  0.8920, -0.2094],\n",
       "          [-0.3664, -0.3180,  0.1456,  ..., -1.4445,  1.7383, -0.7298],\n",
       "          [-1.2385,  0.2314, -0.3220,  ..., -1.9313,  1.8960, -0.7776]],\n",
       " \n",
       "         [[-0.5076, -0.7308, -0.1535,  ..., -1.0015, -0.1269,  0.6836],\n",
       "          [-0.3850, -0.1116,  0.3857,  ...,  0.2763,  0.3237, -2.5789],\n",
       "          [-0.0841,  0.2511,  1.2912,  ..., -1.0000,  0.9884, -1.7230],\n",
       "          [-0.4705, -0.1916,  1.3218,  ..., -0.2816,  1.2109, -1.2800],\n",
       "          [-1.3637,  0.2784,  0.6578,  ..., -0.4983,  1.4598, -1.3212]]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([[[ 1.1013,  0.2984,  0.6116,  ..., -1.4037, -0.1254, -0.0062],\n",
       "          [-0.3586,  0.1400,  0.8328,  ..., -1.4843,  0.9112, -0.9098],\n",
       "          [ 0.4963,  0.1693,  1.3659,  ..., -2.6998,  0.4735, -0.4187],\n",
       "          [ 0.0862, -0.1361,  0.4028,  ..., -1.8653,  1.2898, -1.1453],\n",
       "          [-0.8488,  0.4492,  0.1212,  ..., -2.3097,  1.5455, -0.7243]],\n",
       " \n",
       "         [[-0.4833, -0.2818,  0.0177,  ..., -0.9821, -0.0489,  0.2113],\n",
       "          [-0.0555,  0.6640,  0.3484,  ...,  0.1727,  0.1472, -2.4475],\n",
       "          [ 0.3708,  0.4623,  1.3900,  ..., -1.0469,  0.9198, -1.6652],\n",
       "          [ 0.0855,  0.6461,  1.6065,  ..., -0.1661,  1.0007, -1.5019],\n",
       "          [-0.5552,  1.0783,  0.8189,  ..., -0.5615,  1.2748, -1.1875]]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([[[ 0.8347,  0.3839,  0.3003,  ..., -1.4731, -0.2140, -0.1388],\n",
       "          [-0.5314,  0.2007,  0.9269,  ..., -1.3297,  0.8550, -1.0471],\n",
       "          [ 0.2730,  0.6409,  1.5009,  ..., -2.5274,  0.4339, -0.1372],\n",
       "          [-0.0376,  0.0278,  0.6540,  ..., -1.7176,  1.1877, -0.9302],\n",
       "          [-1.0574,  0.5659,  0.3970,  ..., -2.2058,  1.2530, -0.7118]],\n",
       " \n",
       "         [[-0.5220, -0.0888, -0.6445,  ..., -0.8544, -0.2217, -0.0443],\n",
       "          [-0.2584,  0.4025,  0.0739,  ...,  0.3329, -0.0160, -2.2697],\n",
       "          [-0.1682,  0.6888,  0.7571,  ..., -0.6123,  0.7652, -1.6005],\n",
       "          [ 0.0752,  0.7084,  0.8829,  ..., -0.0869,  0.8160, -1.2094],\n",
       "          [-0.6720,  0.9119,  0.1412,  ..., -0.4099,  1.0499, -1.0576]]],\n",
       "        grad_fn=<AddBackward0>)]"
      ]
     },
     "execution_count": 514,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "cd63403d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 768])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_layers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "4e08d505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.num_attention_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "774dd042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 5, 5])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_probs[0][].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "6b7999e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc83766479ed473b8d278b8e1c96e6f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "832844fd304e4316977ba22b7dbd94e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c6b4f431db541f79b0a35ae22d9978e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36557953125141d8953d4e2745b2d5ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "74a7a7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids : tensor([[ 101, 7632, 2026, 2171, 2003, 4080,  102]])\n",
      "token_type_ids : tensor([[0, 0, 0, 0, 0, 0, 0]])\n",
      "attention_mask : tensor([[1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer('Hi my name is Andrew', return_tensors='pt').input_ids\n",
    "token_type_ids = tokenizer('Hi my name is Andrew', return_tensors='pt').token_type_ids\n",
    "attention_mask = tokenizer('Hi my name is Andrew', return_tensors='pt').attention_mask\n",
    "print('input_ids : {}'.format(input_ids))\n",
    "print('token_type_ids : {}'.format(token_type_ids))\n",
    "print('attention_mask : {}'.format(attention_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "79a4f772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_layers의 텐서 크기: torch.Size([1, 7, 768])\n",
      "pooled_output의 텐서 크기: torch.Size([1, 768])\n",
      "attention_probs의 텐서 크기: torch.Size([1, 12, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "# 순전파\n",
    "encoded_layers, pooled_output, attention_probs = net(\n",
    "    input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False, attention_show_fig=True)\n",
    "\n",
    "print('encoded_layers의 텐서 크기:', encoded_layers.shape)\n",
    "print('pooled_output의 텐서 크기:', pooled_output.shape)\n",
    "print('attention_probs의 텐서 크기:', attention_probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "db3e1adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 7632, 2026, 2171, 2003, 4080, 102], [101, 7632, 102, 0, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 0, 0, 0, 0]]}"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['Hi my name is Andrew', 'hi'], padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "2b073532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'hi', 'my', 'name', 'is', 'andrew', '[SEP]']"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = tokenizer.decode(tokenizer.encode('Hi my name is Andrew')).split()\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "97482155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1200 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(4, 3, figsize=(12, 12), layout='constrained')\n",
    "\n",
    "# plt.matshow(attention_probs[0][0].detach())\n",
    "\n",
    "t = 0\n",
    "for x in range(4):\n",
    "    for y in range(3):\n",
    "        ax = axs[x][y]\n",
    "        ax.matshow(attention_probs[0][t].detach())\n",
    "        # plt.gca().xaxis.tick_bottom()\n",
    "        # ax.xaxis.tick_bottom()\n",
    "        ax.set_xticks(range(len(token)), token)\n",
    "        ax.set_yticks(range(len(token)), token)\n",
    "        t += 1\n",
    "        \n",
    "fig.suptitle('Attention score visualization', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65d66dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
