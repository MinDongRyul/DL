{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66926ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macos/anaconda3/envs/dl/lib/python3.8/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93177925",
   "metadata": {},
   "outputs": [],
   "source": [
    "AutoModelForCausalLM.from_pretrained?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c649fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039cf3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "@classmethod\n",
    "def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n",
    "    config = kwargs.pop(\"config\", None)\n",
    "    trust_remote_code = kwargs.pop(\"trust_remote_code\", None)\n",
    "    kwargs[\"_from_auto\"] = True\n",
    "    hub_kwargs_names = [\n",
    "        \"cache_dir\",\n",
    "        \"force_download\",\n",
    "        \"local_files_only\",\n",
    "        \"proxies\",\n",
    "        \"resume_download\",\n",
    "        \"revision\",\n",
    "        \"subfolder\",\n",
    "        \"use_auth_token\",\n",
    "        \"token\",\n",
    "    ]\n",
    "    hub_kwargs = {name: kwargs.pop(name) for name in hub_kwargs_names if name in kwargs}\n",
    "    code_revision = kwargs.pop(\"code_revision\", None)\n",
    "    commit_hash = kwargs.pop(\"_commit_hash\", None)\n",
    "    adapter_kwargs = kwargs.pop(\"adapter_kwargs\", None)\n",
    "\n",
    "    token = hub_kwargs.pop(\"token\", None)\n",
    "    use_auth_token = hub_kwargs.pop(\"use_auth_token\", None)\n",
    "    if use_auth_token is not None:\n",
    "        warnings.warn(\n",
    "            \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n",
    "            FutureWarning,\n",
    "        )\n",
    "        if token is not None:\n",
    "            raise ValueError(\n",
    "                \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n",
    "            )\n",
    "        token = use_auth_token\n",
    "\n",
    "    if token is not None:\n",
    "        hub_kwargs[\"token\"] = token\n",
    "\n",
    "    if commit_hash is None:\n",
    "        if not isinstance(config, PretrainedConfig):\n",
    "            # We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\n",
    "            resolved_config_file = cached_file(\n",
    "                pretrained_model_name_or_path,\n",
    "                CONFIG_NAME,\n",
    "                _raise_exceptions_for_gated_repo=False,\n",
    "                _raise_exceptions_for_missing_entries=False,\n",
    "                _raise_exceptions_for_connection_errors=False,\n",
    "                **hub_kwargs,\n",
    "            )\n",
    "            commit_hash = extract_commit_hash(resolved_config_file, commit_hash)\n",
    "        else:\n",
    "            commit_hash = getattr(config, \"_commit_hash\", None)\n",
    "\n",
    "    if is_peft_available():\n",
    "        if adapter_kwargs is None:\n",
    "            adapter_kwargs = {}\n",
    "            if token is not None:\n",
    "                adapter_kwargs[\"token\"] = token\n",
    "\n",
    "        maybe_adapter_path = find_adapter_config_file(\n",
    "            pretrained_model_name_or_path, _commit_hash=commit_hash, **adapter_kwargs\n",
    "        )\n",
    "\n",
    "        if maybe_adapter_path is not None:\n",
    "            with open(maybe_adapter_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                adapter_config = json.load(f)\n",
    "\n",
    "                adapter_kwargs[\"_adapter_model_path\"] = pretrained_model_name_or_path\n",
    "                pretrained_model_name_or_path = adapter_config[\"base_model_name_or_path\"]\n",
    "\n",
    "    if not isinstance(config, PretrainedConfig):\n",
    "        kwargs_orig = copy.deepcopy(kwargs)\n",
    "        # ensure not to pollute the config object with torch_dtype=\"auto\" - since it's\n",
    "        # meaningless in the context of the config object - torch.dtype values are acceptable\n",
    "        if kwargs.get(\"torch_dtype\", None) == \"auto\":\n",
    "            _ = kwargs.pop(\"torch_dtype\")\n",
    "        # to not overwrite the quantization_config if config has a quantization_config\n",
    "        if kwargs.get(\"quantization_config\", None) is not None:\n",
    "            _ = kwargs.pop(\"quantization_config\")\n",
    "\n",
    "        config, kwargs = AutoConfig.from_pretrained(\n",
    "            pretrained_model_name_or_path,\n",
    "            return_unused_kwargs=True,\n",
    "            trust_remote_code=trust_remote_code,\n",
    "            code_revision=code_revision,\n",
    "            _commit_hash=commit_hash,\n",
    "            **hub_kwargs,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        # if torch_dtype=auto was passed here, ensure to pass it on\n",
    "        if kwargs_orig.get(\"torch_dtype\", None) == \"auto\":\n",
    "            kwargs[\"torch_dtype\"] = \"auto\"\n",
    "        if kwargs_orig.get(\"quantization_config\", None) is not None:\n",
    "            kwargs[\"quantization_config\"] = kwargs_orig[\"quantization_config\"]\n",
    "\n",
    "    has_remote_code = hasattr(config, \"auto_map\") and cls.__name__ in config.auto_map\n",
    "    has_local_code = type(config) in cls._model_mapping.keys()\n",
    "    trust_remote_code = resolve_trust_remote_code(\n",
    "        trust_remote_code, pretrained_model_name_or_path, has_local_code, has_remote_code\n",
    "    )\n",
    "\n",
    "    # Set the adapter kwargs\n",
    "    kwargs[\"adapter_kwargs\"] = adapter_kwargs\n",
    "\n",
    "    if has_remote_code and trust_remote_code:\n",
    "        class_ref = config.auto_map[cls.__name__]\n",
    "        model_class = get_class_from_dynamic_module(\n",
    "            class_ref, pretrained_model_name_or_path, code_revision=code_revision, **hub_kwargs, **kwargs\n",
    "        )\n",
    "        _ = hub_kwargs.pop(\"code_revision\", None)\n",
    "        if os.path.isdir(pretrained_model_name_or_path):\n",
    "            model_class.register_for_auto_class(cls.__name__)\n",
    "        else:\n",
    "            cls.register(config.__class__, model_class, exist_ok=True)\n",
    "        return model_class.from_pretrained(\n",
    "            pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs\n",
    "        )\n",
    "    elif type(config) in cls._model_mapping.keys():\n",
    "        model_class = _get_model_class(config, cls._model_mapping)\n",
    "        return model_class.from_pretrained(\n",
    "            pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs\n",
    "        )\n",
    "    raise ValueError(\n",
    "        f\"Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\n",
    "        f\"Model type should be one of {', '.join(c.__name__ for c in cls._model_mapping.keys())}.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1eec094c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# coding=utf-8\r\n",
      "# Copyright 2021 The HuggingFace Inc. team.\r\n",
      "#\r\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n",
      "# you may not use this file except in compliance with the License.\r\n",
      "# You may obtain a copy of the License at\r\n",
      "#\r\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\r\n",
      "#\r\n",
      "# Unless required by applicable law or agreed to in writing, software\r\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n",
      "# See the License for the specific language governing permissions and\r\n",
      "# limitations under the License.\r\n",
      "\"\"\"Factory function to build auto-model classes.\"\"\"\r\n",
      "\r\n",
      "import copy\r\n",
      "import importlib\r\n",
      "import json\r\n",
      "import os\r\n",
      "import warnings\r\n",
      "from collections import OrderedDict\r\n",
      "\r\n",
      "from ...configuration_utils import PretrainedConfig\r\n",
      "from ...dynamic_module_utils import get_class_from_dynamic_module, resolve_trust_remote_code\r\n",
      "from ...utils import (\r\n",
      "    CONFIG_NAME,\r\n",
      "    cached_file,\r\n",
      "    copy_func,\r\n",
      "    extract_commit_hash,\r\n",
      "    find_adapter_config_file,\r\n",
      "    is_peft_available,\r\n",
      "    logging,\r\n",
      "    requires_backends,\r\n",
      ")\r\n",
      "from .configuration_auto import AutoConfig, model_type_to_module_name, replace_list_option_in_docstrings\r\n",
      "\r\n",
      "\r\n",
      "logger = logging.get_logger(__name__)\r\n",
      "\r\n",
      "\r\n",
      "CLASS_DOCSTRING = \"\"\"\r\n",
      "    This is a generic model class that will be instantiated as one of the model classes of the library when created\r\n",
      "    with the [`~BaseAutoModelClass.from_pretrained`] class method or the [`~BaseAutoModelClass.from_config`] class\r\n",
      "    method.\r\n",
      "\r\n",
      "    This class cannot be instantiated directly using `__init__()` (throws an error).\r\n",
      "\"\"\"\r\n",
      "\r\n",
      "FROM_CONFIG_DOCSTRING = \"\"\"\r\n",
      "        Instantiates one of the model classes of the library from a configuration.\r\n",
      "\r\n",
      "        Note:\r\n",
      "            Loading a model from its configuration file does **not** load the model weights. It only affects the\r\n",
      "            model's configuration. Use [`~BaseAutoModelClass.from_pretrained`] to load the model weights.\r\n",
      "\r\n",
      "        Args:\r\n",
      "            config ([`PretrainedConfig`]):\r\n",
      "                The model class to instantiate is selected based on the configuration class:\r\n",
      "\r\n",
      "                List options\r\n",
      "            attn_implementation (`str`, *optional*):\r\n",
      "                The attention implementation to use in the model (if relevant). Can be any of `\"eager\"` (manual implementation of the attention), `\"sdpa\"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `\"flash_attention_2\"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `\"eager\"` implementation.\r\n",
      "\r\n",
      "        Examples:\r\n",
      "\r\n",
      "        ```python\r\n",
      "        >>> from transformers import AutoConfig, BaseAutoModelClass\r\n",
      "\r\n",
      "        >>> # Download configuration from huggingface.co and cache.\r\n",
      "        >>> config = AutoConfig.from_pretrained(\"checkpoint_placeholder\")\r\n",
      "        >>> model = BaseAutoModelClass.from_config(config)\r\n",
      "        ```\r\n",
      "\"\"\"\r\n",
      "\r\n",
      "FROM_PRETRAINED_TORCH_DOCSTRING = \"\"\"\r\n",
      "        Instantiate one of the model classes of the library from a pretrained model.\r\n",
      "\r\n",
      "        The model class to instantiate is selected based on the `model_type` property of the config object (either\r\n",
      "        passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by\r\n",
      "        falling back to using pattern matching on `pretrained_model_name_or_path`:\r\n",
      "\r\n",
      "        List options\r\n",
      "\r\n",
      "        The model is set in evaluation mode by default using `model.eval()` (so for instance, dropout modules are\r\n",
      "        deactivated). To train the model, you should first set it back in training mode with `model.train()`\r\n",
      "\r\n",
      "        Args:\r\n",
      "            pretrained_model_name_or_path (`str` or `os.PathLike`):\r\n",
      "                Can be either:\r\n",
      "\r\n",
      "                    - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.\r\n",
      "                    - A path to a *directory* containing model weights saved using\r\n",
      "                      [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.\r\n",
      "                    - A path or url to a *tensorflow index checkpoint file* (e.g, `./tf_model/model.ckpt.index`). In\r\n",
      "                      this case, `from_tf` should be set to `True` and a configuration object should be provided as\r\n",
      "                      `config` argument. This loading path is slower than converting the TensorFlow checkpoint in a\r\n",
      "                      PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\r\n",
      "            model_args (additional positional arguments, *optional*):\r\n",
      "                Will be passed along to the underlying model `__init__()` method.\r\n",
      "            config ([`PretrainedConfig`], *optional*):\r\n",
      "                Configuration for the model to use instead of an automatically loaded configuration. Configuration can\r\n",
      "                be automatically loaded when:\r\n",
      "\r\n",
      "                    - The model is a model provided by the library (loaded with the *model id* string of a pretrained\r\n",
      "                      model).\r\n",
      "                    - The model was saved using [`~PreTrainedModel.save_pretrained`] and is reloaded by supplying the\r\n",
      "                      save directory.\r\n",
      "                    - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a\r\n",
      "                      configuration JSON file named *config.json* is found in the directory.\r\n",
      "            state_dict (*Dict[str, torch.Tensor]*, *optional*):\r\n",
      "                A state dictionary to use instead of a state dictionary loaded from saved weights file.\r\n",
      "\r\n",
      "                This option can be used if you want to create a model from a pretrained configuration but load your own\r\n",
      "                weights. In this case though, you should check if using [`~PreTrainedModel.save_pretrained`] and\r\n",
      "                [`~PreTrainedModel.from_pretrained`] is not a simpler option.\r\n",
      "            cache_dir (`str` or `os.PathLike`, *optional*):\r\n",
      "                Path to a directory in which a downloaded pretrained model configuration should be cached if the\r\n",
      "                standard cache should not be used.\r\n",
      "            from_tf (`bool`, *optional*, defaults to `False`):\r\n",
      "                Load the model weights from a TensorFlow checkpoint save file (see docstring of\r\n",
      "                `pretrained_model_name_or_path` argument).\r\n",
      "            force_download (`bool`, *optional*, defaults to `False`):\r\n",
      "                Whether or not to force the (re-)download of the model weights and configuration files, overriding the\r\n",
      "                cached versions if they exist.\r\n",
      "            resume_download:\r\n",
      "                Deprecated and ignored. All downloads are now resumed by default when possible.\r\n",
      "                Will be removed in v5 of Transformers.\r\n",
      "            proxies (`Dict[str, str]`, *optional*):\r\n",
      "                A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\r\n",
      "                'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\r\n",
      "            output_loading_info(`bool`, *optional*, defaults to `False`):\r\n",
      "                Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.\r\n",
      "            local_files_only(`bool`, *optional*, defaults to `False`):\r\n",
      "                Whether or not to only look at local files (e.g., not try downloading the model).\r\n",
      "            revision (`str`, *optional*, defaults to `\"main\"`):\r\n",
      "                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\r\n",
      "                git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\r\n",
      "                identifier allowed by git.\r\n",
      "            trust_remote_code (`bool`, *optional*, defaults to `False`):\r\n",
      "                Whether or not to allow for custom models defined on the Hub in their own modeling files. This option\r\n",
      "                should only be set to `True` for repositories you trust and in which you have read the code, as it will\r\n",
      "                execute code present on the Hub on your local machine.\r\n",
      "            code_revision (`str`, *optional*, defaults to `\"main\"`):\r\n",
      "                The specific revision to use for the code on the Hub, if the code leaves in a different repository than\r\n",
      "                the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based\r\n",
      "                system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier\r\n",
      "                allowed by git.\r\n",
      "            kwargs (additional keyword arguments, *optional*):\r\n",
      "                Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,\r\n",
      "                `output_attentions=True`). Behaves differently depending on whether a `config` is provided or\r\n",
      "                automatically loaded:\r\n",
      "\r\n",
      "                    - If a configuration is provided with `config`, `**kwargs` will be directly passed to the\r\n",
      "                      underlying model's `__init__` method (we assume all relevant updates to the configuration have\r\n",
      "                      already been done)\r\n",
      "                    - If a configuration is not provided, `kwargs` will be first passed to the configuration class\r\n",
      "                      initialization function ([`~PretrainedConfig.from_pretrained`]). Each key of `kwargs` that\r\n",
      "                      corresponds to a configuration attribute will be used to override said attribute with the\r\n",
      "                      supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute\r\n",
      "                      will be passed to the underlying model's `__init__` function.\r\n",
      "\r\n",
      "        Examples:\r\n",
      "\r\n",
      "        ```python\r\n",
      "        >>> from transformers import AutoConfig, BaseAutoModelClass\r\n",
      "\r\n",
      "        >>> # Download model and configuration from huggingface.co and cache.\r\n",
      "        >>> model = BaseAutoModelClass.from_pretrained(\"checkpoint_placeholder\")\r\n",
      "\r\n",
      "        >>> # Update configuration during loading\r\n",
      "        >>> model = BaseAutoModelClass.from_pretrained(\"checkpoint_placeholder\", output_attentions=True)\r\n",
      "        >>> model.config.output_attentions\r\n",
      "        True\r\n",
      "\r\n",
      "        >>> # Loading from a TF checkpoint file instead of a PyTorch model (slower)\r\n",
      "        >>> config = AutoConfig.from_pretrained(\"./tf_model/shortcut_placeholder_tf_model_config.json\")\r\n",
      "        >>> model = BaseAutoModelClass.from_pretrained(\r\n",
      "        ...     \"./tf_model/shortcut_placeholder_tf_checkpoint.ckpt.index\", from_tf=True, config=config\r\n",
      "        ... )\r\n",
      "        ```\r\n",
      "\"\"\"\r\n",
      "\r\n",
      "FROM_PRETRAINED_TF_DOCSTRING = \"\"\"\r\n",
      "        Instantiate one of the model classes of the library from a pretrained model.\r\n",
      "\r\n",
      "        The model class to instantiate is selected based on the `model_type` property of the config object (either\r\n",
      "        passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by\r\n",
      "        falling back to using pattern matching on `pretrained_model_name_or_path`:\r\n",
      "\r\n",
      "        List options\r\n",
      "\r\n",
      "        Args:\r\n",
      "            pretrained_model_name_or_path (`str` or `os.PathLike`):\r\n",
      "                Can be either:\r\n",
      "\r\n",
      "                    - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.\r\n",
      "                    - A path to a *directory* containing model weights saved using\r\n",
      "                      [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.\r\n",
      "                    - A path or url to a *PyTorch state_dict save file* (e.g, `./pt_model/pytorch_model.bin`). In this\r\n",
      "                      case, `from_pt` should be set to `True` and a configuration object should be provided as `config`\r\n",
      "                      argument. This loading path is slower than converting the PyTorch model in a TensorFlow model\r\n",
      "                      using the provided conversion scripts and loading the TensorFlow model afterwards.\r\n",
      "            model_args (additional positional arguments, *optional*):\r\n",
      "                Will be passed along to the underlying model `__init__()` method.\r\n",
      "            config ([`PretrainedConfig`], *optional*):\r\n",
      "                Configuration for the model to use instead of an automatically loaded configuration. Configuration can\r\n",
      "                be automatically loaded when:\r\n",
      "\r\n",
      "                    - The model is a model provided by the library (loaded with the *model id* string of a pretrained\r\n",
      "                      model).\r\n",
      "                    - The model was saved using [`~PreTrainedModel.save_pretrained`] and is reloaded by supplying the\r\n",
      "                      save directory.\r\n",
      "                    - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a\r\n",
      "                      configuration JSON file named *config.json* is found in the directory.\r\n",
      "            cache_dir (`str` or `os.PathLike`, *optional*):\r\n",
      "                Path to a directory in which a downloaded pretrained model configuration should be cached if the\r\n",
      "                standard cache should not be used.\r\n",
      "            from_pt (`bool`, *optional*, defaults to `False`):\r\n",
      "                Load the model weights from a PyTorch checkpoint save file (see docstring of\r\n",
      "                `pretrained_model_name_or_path` argument).\r\n",
      "            force_download (`bool`, *optional*, defaults to `False`):\r\n",
      "                Whether or not to force the (re-)download of the model weights and configuration files, overriding the\r\n",
      "                cached versions if they exist.\r\n",
      "            resume_download:\r\n",
      "                Deprecated and ignored. All downloads are now resumed by default when possible.\r\n",
      "                Will be removed in v5 of Transformers.\r\n",
      "            proxies (`Dict[str, str]`, *optional*):\r\n",
      "                A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\r\n",
      "                'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\r\n",
      "            output_loading_info(`bool`, *optional*, defaults to `False`):\r\n",
      "                Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.\r\n",
      "            local_files_only(`bool`, *optional*, defaults to `False`):\r\n",
      "                Whether or not to only look at local files (e.g., not try downloading the model).\r\n",
      "            revision (`str`, *optional*, defaults to `\"main\"`):\r\n",
      "                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\r\n",
      "                git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\r\n",
      "                identifier allowed by git.\r\n",
      "            trust_remote_code (`bool`, *optional*, defaults to `False`):\r\n",
      "                Whether or not to allow for custom models defined on the Hub in their own modeling files. This option\r\n",
      "                should only be set to `True` for repositories you trust and in which you have read the code, as it will\r\n",
      "                execute code present on the Hub on your local machine.\r\n",
      "            code_revision (`str`, *optional*, defaults to `\"main\"`):\r\n",
      "                The specific revision to use for the code on the Hub, if the code leaves in a different repository than\r\n",
      "                the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based\r\n",
      "                system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier\r\n",
      "                allowed by git.\r\n",
      "            kwargs (additional keyword arguments, *optional*):\r\n",
      "                Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,\r\n",
      "                `output_attentions=True`). Behaves differently depending on whether a `config` is provided or\r\n",
      "                automatically loaded:\r\n",
      "\r\n",
      "                    - If a configuration is provided with `config`, `**kwargs` will be directly passed to the\r\n",
      "                      underlying model's `__init__` method (we assume all relevant updates to the configuration have\r\n",
      "                      already been done)\r\n",
      "                    - If a configuration is not provided, `kwargs` will be first passed to the configuration class\r\n",
      "                      initialization function ([`~PretrainedConfig.from_pretrained`]). Each key of `kwargs` that\r\n",
      "                      corresponds to a configuration attribute will be used to override said attribute with the\r\n",
      "                      supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute\r\n",
      "                      will be passed to the underlying model's `__init__` function.\r\n",
      "\r\n",
      "        Examples:\r\n",
      "\r\n",
      "        ```python\r\n",
      "        >>> from transformers import AutoConfig, BaseAutoModelClass\r\n",
      "\r\n",
      "        >>> # Download model and configuration from huggingface.co and cache.\r\n",
      "        >>> model = BaseAutoModelClass.from_pretrained(\"checkpoint_placeholder\")\r\n",
      "\r\n",
      "        >>> # Update configuration during loading\r\n",
      "        >>> model = BaseAutoModelClass.from_pretrained(\"checkpoint_placeholder\", output_attentions=True)\r\n",
      "        >>> model.config.output_attentions\r\n",
      "        True\r\n",
      "\r\n",
      "        >>> # Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)\r\n",
      "        >>> config = AutoConfig.from_pretrained(\"./pt_model/shortcut_placeholder_pt_model_config.json\")\r\n",
      "        >>> model = BaseAutoModelClass.from_pretrained(\r\n",
      "        ...     \"./pt_model/shortcut_placeholder_pytorch_model.bin\", from_pt=True, config=config\r\n",
      "        ... )\r\n",
      "        ```\r\n",
      "\"\"\"\r\n",
      "\r\n",
      "FROM_PRETRAINED_FLAX_DOCSTRING = \"\"\"\r\n",
      "        Instantiate one of the model classes of the library from a pretrained model.\r\n",
      "\r\n",
      "        The model class to instantiate is selected based on the `model_type` property of the config object (either\r\n",
      "        passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by\r\n",
      "        falling back to using pattern matching on `pretrained_model_name_or_path`:\r\n",
      "\r\n",
      "        List options\r\n",
      "\r\n",
      "        Args:\r\n",
      "            pretrained_model_name_or_path (`str` or `os.PathLike`):\r\n",
      "                Can be either:\r\n",
      "\r\n",
      "                    - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.\r\n",
      "                    - A path to a *directory* containing model weights saved using\r\n",
      "                      [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.\r\n",
      "                    - A path or url to a *PyTorch state_dict save file* (e.g, `./pt_model/pytorch_model.bin`). In this\r\n",
      "                      case, `from_pt` should be set to `True` and a configuration object should be provided as `config`\r\n",
      "                      argument. This loading path is slower than converting the PyTorch model in a TensorFlow model\r\n",
      "                      using the provided conversion scripts and loading the TensorFlow model afterwards.\r\n",
      "            model_args (additional positional arguments, *optional*):\r\n",
      "                Will be passed along to the underlying model `__init__()` method.\r\n",
      "            config ([`PretrainedConfig`], *optional*):\r\n",
      "                Configuration for the model to use instead of an automatically loaded configuration. Configuration can\r\n",
      "                be automatically loaded when:\r\n",
      "\r\n",
      "                    - The model is a model provided by the library (loaded with the *model id* string of a pretrained\r\n",
      "                      model).\r\n",
      "                    - The model was saved using [`~PreTrainedModel.save_pretrained`] and is reloaded by supplying the\r\n",
      "                      save directory.\r\n",
      "                    - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a\r\n",
      "                      configuration JSON file named *config.json* is found in the directory.\r\n",
      "            cache_dir (`str` or `os.PathLike`, *optional*):\r\n",
      "                Path to a directory in which a downloaded pretrained model configuration should be cached if the\r\n",
      "                standard cache should not be used.\r\n",
      "            from_pt (`bool`, *optional*, defaults to `False`):\r\n",
      "                Load the model weights from a PyTorch checkpoint save file (see docstring of\r\n",
      "                `pretrained_model_name_or_path` argument).\r\n",
      "            force_download (`bool`, *optional*, defaults to `False`):\r\n",
      "                Whether or not to force the (re-)download of the model weights and configuration files, overriding the\r\n",
      "                cached versions if they exist.\r\n",
      "            resume_download:\r\n",
      "                Deprecated and ignored. All downloads are now resumed by default when possible.\r\n",
      "                Will be removed in v5 of Transformers.\r\n",
      "            proxies (`Dict[str, str]`, *optional*):\r\n",
      "                A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\r\n",
      "                'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\r\n",
      "            output_loading_info(`bool`, *optional*, defaults to `False`):\r\n",
      "                Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.\r\n",
      "            local_files_only(`bool`, *optional*, defaults to `False`):\r\n",
      "                Whether or not to only look at local files (e.g., not try downloading the model).\r\n",
      "            revision (`str`, *optional*, defaults to `\"main\"`):\r\n",
      "                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\r\n",
      "                git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\r\n",
      "                identifier allowed by git.\r\n",
      "            trust_remote_code (`bool`, *optional*, defaults to `False`):\r\n",
      "                Whether or not to allow for custom models defined on the Hub in their own modeling files. This option\r\n",
      "                should only be set to `True` for repositories you trust and in which you have read the code, as it will\r\n",
      "                execute code present on the Hub on your local machine.\r\n",
      "            code_revision (`str`, *optional*, defaults to `\"main\"`):\r\n",
      "                The specific revision to use for the code on the Hub, if the code leaves in a different repository than\r\n",
      "                the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based\r\n",
      "                system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier\r\n",
      "                allowed by git.\r\n",
      "            kwargs (additional keyword arguments, *optional*):\r\n",
      "                Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,\r\n",
      "                `output_attentions=True`). Behaves differently depending on whether a `config` is provided or\r\n",
      "                automatically loaded:\r\n",
      "\r\n",
      "                    - If a configuration is provided with `config`, `**kwargs` will be directly passed to the\r\n",
      "                      underlying model's `__init__` method (we assume all relevant updates to the configuration have\r\n",
      "                      already been done)\r\n",
      "                    - If a configuration is not provided, `kwargs` will be first passed to the configuration class\r\n",
      "                      initialization function ([`~PretrainedConfig.from_pretrained`]). Each key of `kwargs` that\r\n",
      "                      corresponds to a configuration attribute will be used to override said attribute with the\r\n",
      "                      supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute\r\n",
      "                      will be passed to the underlying model's `__init__` function.\r\n",
      "\r\n",
      "        Examples:\r\n",
      "\r\n",
      "        ```python\r\n",
      "        >>> from transformers import AutoConfig, BaseAutoModelClass\r\n",
      "\r\n",
      "        >>> # Download model and configuration from huggingface.co and cache.\r\n",
      "        >>> model = BaseAutoModelClass.from_pretrained(\"checkpoint_placeholder\")\r\n",
      "\r\n",
      "        >>> # Update configuration during loading\r\n",
      "        >>> model = BaseAutoModelClass.from_pretrained(\"checkpoint_placeholder\", output_attentions=True)\r\n",
      "        >>> model.config.output_attentions\r\n",
      "        True\r\n",
      "\r\n",
      "        >>> # Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)\r\n",
      "        >>> config = AutoConfig.from_pretrained(\"./pt_model/shortcut_placeholder_pt_model_config.json\")\r\n",
      "        >>> model = BaseAutoModelClass.from_pretrained(\r\n",
      "        ...     \"./pt_model/shortcut_placeholder_pytorch_model.bin\", from_pt=True, config=config\r\n",
      "        ... )\r\n",
      "        ```\r\n",
      "\"\"\"\r\n",
      "\r\n",
      "\r\n",
      "def _get_model_class(config, model_mapping):\r\n",
      "    supported_models = model_mapping[type(config)]\r\n",
      "    if not isinstance(supported_models, (list, tuple)):\r\n",
      "        return supported_models\r\n",
      "\r\n",
      "    name_to_model = {model.__name__: model for model in supported_models}\r\n",
      "    architectures = getattr(config, \"architectures\", [])\r\n",
      "    for arch in architectures:\r\n",
      "        if arch in name_to_model:\r\n",
      "            return name_to_model[arch]\r\n",
      "        elif f\"TF{arch}\" in name_to_model:\r\n",
      "            return name_to_model[f\"TF{arch}\"]\r\n",
      "        elif f\"Flax{arch}\" in name_to_model:\r\n",
      "            return name_to_model[f\"Flax{arch}\"]\r\n",
      "\r\n",
      "    # If not architecture is set in the config or match the supported models, the first element of the tuple is the\r\n",
      "    # defaults.\r\n",
      "    return supported_models[0]\r\n",
      "\r\n",
      "\r\n",
      "class _BaseAutoModelClass:\r\n",
      "    # Base class for auto models.\r\n",
      "    _model_mapping = None\r\n",
      "\r\n",
      "    def __init__(self, *args, **kwargs):\r\n",
      "        raise EnvironmentError(\r\n",
      "            f\"{self.__class__.__name__} is designed to be instantiated \"\r\n",
      "            f\"using the `{self.__class__.__name__}.from_pretrained(pretrained_model_name_or_path)` or \"\r\n",
      "            f\"`{self.__class__.__name__}.from_config(config)` methods.\"\r\n",
      "        )\r\n",
      "\r\n",
      "    @classmethod\r\n",
      "    def from_config(cls, config, **kwargs):\r\n",
      "        trust_remote_code = kwargs.pop(\"trust_remote_code\", None)\r\n",
      "        has_remote_code = hasattr(config, \"auto_map\") and cls.__name__ in config.auto_map\r\n",
      "        has_local_code = type(config) in cls._model_mapping.keys()\r\n",
      "        trust_remote_code = resolve_trust_remote_code(\r\n",
      "            trust_remote_code, config._name_or_path, has_local_code, has_remote_code\r\n",
      "        )\r\n",
      "\r\n",
      "        if has_remote_code and trust_remote_code:\r\n",
      "            class_ref = config.auto_map[cls.__name__]\r\n",
      "            if \"--\" in class_ref:\r\n",
      "                repo_id, class_ref = class_ref.split(\"--\")\r\n",
      "            else:\r\n",
      "                repo_id = config.name_or_path\r\n",
      "            model_class = get_class_from_dynamic_module(class_ref, repo_id, **kwargs)\r\n",
      "            if os.path.isdir(config._name_or_path):\r\n",
      "                model_class.register_for_auto_class(cls.__name__)\r\n",
      "            else:\r\n",
      "                cls.register(config.__class__, model_class, exist_ok=True)\r\n",
      "            _ = kwargs.pop(\"code_revision\", None)\r\n",
      "            return model_class._from_config(config, **kwargs)\r\n",
      "        elif type(config) in cls._model_mapping.keys():\r\n",
      "            model_class = _get_model_class(config, cls._model_mapping)\r\n",
      "            return model_class._from_config(config, **kwargs)\r\n",
      "\r\n",
      "        raise ValueError(\r\n",
      "            f\"Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\r\n",
      "            f\"Model type should be one of {', '.join(c.__name__ for c in cls._model_mapping.keys())}.\"\r\n",
      "        )\r\n",
      "\r\n",
      "    @classmethod\r\n",
      "    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\r\n",
      "        config = kwargs.pop(\"config\", None)\r\n",
      "        trust_remote_code = kwargs.pop(\"trust_remote_code\", None)\r\n",
      "        kwargs[\"_from_auto\"] = True\r\n",
      "        hub_kwargs_names = [\r\n",
      "            \"cache_dir\",\r\n",
      "            \"force_download\",\r\n",
      "            \"local_files_only\",\r\n",
      "            \"proxies\",\r\n",
      "            \"resume_download\",\r\n",
      "            \"revision\",\r\n",
      "            \"subfolder\",\r\n",
      "            \"use_auth_token\",\r\n",
      "            \"token\",\r\n",
      "        ]\r\n",
      "        hub_kwargs = {name: kwargs.pop(name) for name in hub_kwargs_names if name in kwargs}\r\n",
      "        code_revision = kwargs.pop(\"code_revision\", None)\r\n",
      "        commit_hash = kwargs.pop(\"_commit_hash\", None)\r\n",
      "        adapter_kwargs = kwargs.pop(\"adapter_kwargs\", None)\r\n",
      "\r\n",
      "        token = hub_kwargs.pop(\"token\", None)\r\n",
      "        use_auth_token = hub_kwargs.pop(\"use_auth_token\", None)\r\n",
      "        if use_auth_token is not None:\r\n",
      "            warnings.warn(\r\n",
      "                \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\r\n",
      "                FutureWarning,\r\n",
      "            )\r\n",
      "            if token is not None:\r\n",
      "                raise ValueError(\r\n",
      "                    \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\r\n",
      "                )\r\n",
      "            token = use_auth_token\r\n",
      "\r\n",
      "        if token is not None:\r\n",
      "            hub_kwargs[\"token\"] = token\r\n",
      "\r\n",
      "        if commit_hash is None:\r\n",
      "            if not isinstance(config, PretrainedConfig):\r\n",
      "                # We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\r\n",
      "                resolved_config_file = cached_file(\r\n",
      "                    pretrained_model_name_or_path,\r\n",
      "                    CONFIG_NAME,\r\n",
      "                    _raise_exceptions_for_gated_repo=False,\r\n",
      "                    _raise_exceptions_for_missing_entries=False,\r\n",
      "                    _raise_exceptions_for_connection_errors=False,\r\n",
      "                    **hub_kwargs,\r\n",
      "                )\r\n",
      "                commit_hash = extract_commit_hash(resolved_config_file, commit_hash)\r\n",
      "            else:\r\n",
      "                commit_hash = getattr(config, \"_commit_hash\", None)\r\n",
      "\r\n",
      "        if is_peft_available():\r\n",
      "            if adapter_kwargs is None:\r\n",
      "                adapter_kwargs = {}\r\n",
      "                if token is not None:\r\n",
      "                    adapter_kwargs[\"token\"] = token\r\n",
      "\r\n",
      "            maybe_adapter_path = find_adapter_config_file(\r\n",
      "                pretrained_model_name_or_path, _commit_hash=commit_hash, **adapter_kwargs\r\n",
      "            )\r\n",
      "\r\n",
      "            if maybe_adapter_path is not None:\r\n",
      "                with open(maybe_adapter_path, \"r\", encoding=\"utf-8\") as f:\r\n",
      "                    adapter_config = json.load(f)\r\n",
      "\r\n",
      "                    adapter_kwargs[\"_adapter_model_path\"] = pretrained_model_name_or_path\r\n",
      "                    pretrained_model_name_or_path = adapter_config[\"base_model_name_or_path\"]\r\n",
      "\r\n",
      "        if not isinstance(config, PretrainedConfig):\r\n",
      "            kwargs_orig = copy.deepcopy(kwargs)\r\n",
      "            # ensure not to pollute the config object with torch_dtype=\"auto\" - since it's\r\n",
      "            # meaningless in the context of the config object - torch.dtype values are acceptable\r\n",
      "            if kwargs.get(\"torch_dtype\", None) == \"auto\":\r\n",
      "                _ = kwargs.pop(\"torch_dtype\")\r\n",
      "            # to not overwrite the quantization_config if config has a quantization_config\r\n",
      "            if kwargs.get(\"quantization_config\", None) is not None:\r\n",
      "                _ = kwargs.pop(\"quantization_config\")\r\n",
      "\r\n",
      "            config, kwargs = AutoConfig.from_pretrained(\r\n",
      "                pretrained_model_name_or_path,\r\n",
      "                return_unused_kwargs=True,\r\n",
      "                trust_remote_code=trust_remote_code,\r\n",
      "                code_revision=code_revision,\r\n",
      "                _commit_hash=commit_hash,\r\n",
      "                **hub_kwargs,\r\n",
      "                **kwargs,\r\n",
      "            )\r\n",
      "\r\n",
      "            # if torch_dtype=auto was passed here, ensure to pass it on\r\n",
      "            if kwargs_orig.get(\"torch_dtype\", None) == \"auto\":\r\n",
      "                kwargs[\"torch_dtype\"] = \"auto\"\r\n",
      "            if kwargs_orig.get(\"quantization_config\", None) is not None:\r\n",
      "                kwargs[\"quantization_config\"] = kwargs_orig[\"quantization_config\"]\r\n",
      "\r\n",
      "        has_remote_code = hasattr(config, \"auto_map\") and cls.__name__ in config.auto_map\r\n",
      "        has_local_code = type(config) in cls._model_mapping.keys()\r\n",
      "        trust_remote_code = resolve_trust_remote_code(\r\n",
      "            trust_remote_code, pretrained_model_name_or_path, has_local_code, has_remote_code\r\n",
      "        )\r\n",
      "\r\n",
      "        # Set the adapter kwargs\r\n",
      "        kwargs[\"adapter_kwargs\"] = adapter_kwargs\r\n",
      "\r\n",
      "        if has_remote_code and trust_remote_code:\r\n",
      "            class_ref = config.auto_map[cls.__name__]\r\n",
      "            model_class = get_class_from_dynamic_module(\r\n",
      "                class_ref, pretrained_model_name_or_path, code_revision=code_revision, **hub_kwargs, **kwargs\r\n",
      "            )\r\n",
      "            _ = hub_kwargs.pop(\"code_revision\", None)\r\n",
      "            if os.path.isdir(pretrained_model_name_or_path):\r\n",
      "                model_class.register_for_auto_class(cls.__name__)\r\n",
      "            else:\r\n",
      "                cls.register(config.__class__, model_class, exist_ok=True)\r\n",
      "            return model_class.from_pretrained(\r\n",
      "                pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs\r\n",
      "            )\r\n",
      "        elif type(config) in cls._model_mapping.keys():\r\n",
      "            model_class = _get_model_class(config, cls._model_mapping)\r\n",
      "            return model_class.from_pretrained(\r\n",
      "                pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs\r\n",
      "            )\r\n",
      "        raise ValueError(\r\n",
      "            f\"Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\r\n",
      "            f\"Model type should be one of {', '.join(c.__name__ for c in cls._model_mapping.keys())}.\"\r\n",
      "        )\r\n",
      "\r\n",
      "    @classmethod\r\n",
      "    def register(cls, config_class, model_class, exist_ok=False):\r\n",
      "        \"\"\"\r\n",
      "        Register a new model for this class.\r\n",
      "\r\n",
      "        Args:\r\n",
      "            config_class ([`PretrainedConfig`]):\r\n",
      "                The configuration corresponding to the model to register.\r\n",
      "            model_class ([`PreTrainedModel`]):\r\n",
      "                The model to register.\r\n",
      "        \"\"\"\r\n",
      "        if hasattr(model_class, \"config_class\") and str(model_class.config_class) != str(config_class):\r\n",
      "            raise ValueError(\r\n",
      "                \"The model class you are passing has a `config_class` attribute that is not consistent with the \"\r\n",
      "                f\"config class you passed (model has {model_class.config_class} and you passed {config_class}. Fix \"\r\n",
      "                \"one of those so they match!\"\r\n",
      "            )\r\n",
      "        cls._model_mapping.register(config_class, model_class, exist_ok=exist_ok)\r\n",
      "\r\n",
      "\r\n",
      "class _BaseAutoBackboneClass(_BaseAutoModelClass):\r\n",
      "    # Base class for auto backbone models.\r\n",
      "    _model_mapping = None\r\n",
      "\r\n",
      "    @classmethod\r\n",
      "    def _load_timm_backbone_from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\r\n",
      "        requires_backends(cls, [\"vision\", \"timm\"])\r\n",
      "        from ...models.timm_backbone import TimmBackboneConfig\r\n",
      "\r\n",
      "        config = kwargs.pop(\"config\", TimmBackboneConfig())\r\n",
      "\r\n",
      "        if kwargs.get(\"out_features\", None) is not None:\r\n",
      "            raise ValueError(\"Cannot specify `out_features` for timm backbones\")\r\n",
      "\r\n",
      "        if kwargs.get(\"output_loading_info\", False):\r\n",
      "            raise ValueError(\"Cannot specify `output_loading_info=True` when loading from timm\")\r\n",
      "\r\n",
      "        num_channels = kwargs.pop(\"num_channels\", config.num_channels)\r\n",
      "        features_only = kwargs.pop(\"features_only\", config.features_only)\r\n",
      "        use_pretrained_backbone = kwargs.pop(\"use_pretrained_backbone\", config.use_pretrained_backbone)\r\n",
      "        out_indices = kwargs.pop(\"out_indices\", config.out_indices)\r\n",
      "        config = TimmBackboneConfig(\r\n",
      "            backbone=pretrained_model_name_or_path,\r\n",
      "            num_channels=num_channels,\r\n",
      "            features_only=features_only,\r\n",
      "            use_pretrained_backbone=use_pretrained_backbone,\r\n",
      "            out_indices=out_indices,\r\n",
      "        )\r\n",
      "        return super().from_config(config, **kwargs)\r\n",
      "\r\n",
      "    @classmethod\r\n",
      "    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\r\n",
      "        use_timm_backbone = kwargs.pop(\"use_timm_backbone\", False)\r\n",
      "        if use_timm_backbone:\r\n",
      "            return cls._load_timm_backbone_from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\r\n",
      "\r\n",
      "        return super().from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\r\n",
      "\r\n",
      "\r\n",
      "def insert_head_doc(docstring, head_doc=\"\"):\r\n",
      "    if len(head_doc) > 0:\r\n",
      "        return docstring.replace(\r\n",
      "            \"one of the model classes of the library \",\r\n",
      "            f\"one of the model classes of the library (with a {head_doc} head) \",\r\n",
      "        )\r\n",
      "    return docstring.replace(\r\n",
      "        \"one of the model classes of the library \", \"one of the base model classes of the library \"\r\n",
      "    )\r\n",
      "\r\n",
      "\r\n",
      "def auto_class_update(cls, checkpoint_for_example=\"google-bert/bert-base-cased\", head_doc=\"\"):\r\n",
      "    # Create a new class with the right name from the base class\r\n",
      "    model_mapping = cls._model_mapping\r\n",
      "    name = cls.__name__\r\n",
      "    class_docstring = insert_head_doc(CLASS_DOCSTRING, head_doc=head_doc)\r\n",
      "    cls.__doc__ = class_docstring.replace(\"BaseAutoModelClass\", name)\r\n",
      "\r\n",
      "    # Now we need to copy and re-register `from_config` and `from_pretrained` as class methods otherwise we can't\r\n",
      "    # have a specific docstrings for them.\r\n",
      "    from_config = copy_func(_BaseAutoModelClass.from_config)\r\n",
      "    from_config_docstring = insert_head_doc(FROM_CONFIG_DOCSTRING, head_doc=head_doc)\r\n",
      "    from_config_docstring = from_config_docstring.replace(\"BaseAutoModelClass\", name)\r\n",
      "    from_config_docstring = from_config_docstring.replace(\"checkpoint_placeholder\", checkpoint_for_example)\r\n",
      "    from_config.__doc__ = from_config_docstring\r\n",
      "    from_config = replace_list_option_in_docstrings(model_mapping._model_mapping, use_model_types=False)(from_config)\r\n",
      "    cls.from_config = classmethod(from_config)\r\n",
      "\r\n",
      "    if name.startswith(\"TF\"):\r\n",
      "        from_pretrained_docstring = FROM_PRETRAINED_TF_DOCSTRING\r\n",
      "    elif name.startswith(\"Flax\"):\r\n",
      "        from_pretrained_docstring = FROM_PRETRAINED_FLAX_DOCSTRING\r\n",
      "    else:\r\n",
      "        from_pretrained_docstring = FROM_PRETRAINED_TORCH_DOCSTRING\r\n",
      "    from_pretrained = copy_func(_BaseAutoModelClass.from_pretrained)\r\n",
      "    from_pretrained_docstring = insert_head_doc(from_pretrained_docstring, head_doc=head_doc)\r\n",
      "    from_pretrained_docstring = from_pretrained_docstring.replace(\"BaseAutoModelClass\", name)\r\n",
      "    from_pretrained_docstring = from_pretrained_docstring.replace(\"checkpoint_placeholder\", checkpoint_for_example)\r\n",
      "    shortcut = checkpoint_for_example.split(\"/\")[-1].split(\"-\")[0]\r\n",
      "    from_pretrained_docstring = from_pretrained_docstring.replace(\"shortcut_placeholder\", shortcut)\r\n",
      "    from_pretrained.__doc__ = from_pretrained_docstring\r\n",
      "    from_pretrained = replace_list_option_in_docstrings(model_mapping._model_mapping)(from_pretrained)\r\n",
      "    cls.from_pretrained = classmethod(from_pretrained)\r\n",
      "    return cls\r\n",
      "\r\n",
      "\r\n",
      "def get_values(model_mapping):\r\n",
      "    result = []\r\n",
      "    for model in model_mapping.values():\r\n",
      "        if isinstance(model, (list, tuple)):\r\n",
      "            result += list(model)\r\n",
      "        else:\r\n",
      "            result.append(model)\r\n",
      "\r\n",
      "    return result\r\n",
      "\r\n",
      "\r\n",
      "def getattribute_from_module(module, attr):\r\n",
      "    if attr is None:\r\n",
      "        return None\r\n",
      "    if isinstance(attr, tuple):\r\n",
      "        return tuple(getattribute_from_module(module, a) for a in attr)\r\n",
      "    if hasattr(module, attr):\r\n",
      "        return getattr(module, attr)\r\n",
      "    # Some of the mappings have entries model_type -> object of another model type. In that case we try to grab the\r\n",
      "    # object at the top level.\r\n",
      "    transformers_module = importlib.import_module(\"transformers\")\r\n",
      "\r\n",
      "    if module != transformers_module:\r\n",
      "        try:\r\n",
      "            return getattribute_from_module(transformers_module, attr)\r\n",
      "        except ValueError:\r\n",
      "            raise ValueError(f\"Could not find {attr} neither in {module} nor in {transformers_module}!\")\r\n",
      "    else:\r\n",
      "        raise ValueError(f\"Could not find {attr} in {transformers_module}!\")\r\n",
      "\r\n",
      "\r\n",
      "class _LazyAutoMapping(OrderedDict):\r\n",
      "    \"\"\"\r\n",
      "    \" A mapping config to object (model or tokenizer for instance) that will load keys and values when it is accessed.\r\n",
      "\r\n",
      "    Args:\r\n",
      "        - config_mapping: The map model type to config class\r\n",
      "        - model_mapping: The map model type to model (or tokenizer) class\r\n",
      "    \"\"\"\r\n",
      "\r\n",
      "    def __init__(self, config_mapping, model_mapping):\r\n",
      "        self._config_mapping = config_mapping\r\n",
      "        self._reverse_config_mapping = {v: k for k, v in config_mapping.items()}\r\n",
      "        self._model_mapping = model_mapping\r\n",
      "        self._model_mapping._model_mapping = self\r\n",
      "        self._extra_content = {}\r\n",
      "        self._modules = {}\r\n",
      "\r",
      "\r\n",
      "    def __len__(self):\r\n",
      "        common_keys = set(self._config_mapping.keys()).intersection(self._model_mapping.keys())\r\n",
      "        return len(common_keys) + len(self._extra_content)\r\n",
      "\r\n",
      "    def __getitem__(self, key):\r\n",
      "        if key in self._extra_content:\r\n",
      "            return self._extra_content[key]\r\n",
      "        model_type = self._reverse_config_mapping[key.__name__]\r\n",
      "        if model_type in self._model_mapping:\r\n",
      "            model_name = self._model_mapping[model_type]\r\n",
      "            return self._load_attr_from_module(model_type, model_name)\r\n",
      "\r\n",
      "        # Maybe there was several model types associated with this config.\r\n",
      "        model_types = [k for k, v in self._config_mapping.items() if v == key.__name__]\r\n",
      "        for mtype in model_types:\r\n",
      "            if mtype in self._model_mapping:\r\n",
      "                model_name = self._model_mapping[mtype]\r\n",
      "                return self._load_attr_from_module(mtype, model_name)\r\n",
      "        raise KeyError(key)\r\n",
      "\r\n",
      "    def _load_attr_from_module(self, model_type, attr):\r\n",
      "        module_name = model_type_to_module_name(model_type)\r\n",
      "        if module_name not in self._modules:\r\n",
      "            self._modules[module_name] = importlib.import_module(f\".{module_name}\", \"transformers.models\")\r\n",
      "        return getattribute_from_module(self._modules[module_name], attr)\r\n",
      "\r\n",
      "    def keys(self):\r\n",
      "        mapping_keys = [\r\n",
      "            self._load_attr_from_module(key, name)\r\n",
      "            for key, name in self._config_mapping.items()\r\n",
      "            if key in self._model_mapping.keys()\r\n",
      "        ]\r\n",
      "        return mapping_keys + list(self._extra_content.keys())\r\n",
      "\r\n",
      "    def get(self, key, default):\r\n",
      "        try:\r\n",
      "            return self.__getitem__(key)\r\n",
      "        except KeyError:\r\n",
      "            return default\r\n",
      "\r\n",
      "    def __bool__(self):\r\n",
      "        return bool(self.keys())\r\n",
      "\r\n",
      "    def values(self):\r\n",
      "        mapping_values = [\r\n",
      "            self._load_attr_from_module(key, name)\r\n",
      "            for key, name in self._model_mapping.items()\r\n",
      "            if key in self._config_mapping.keys()\r\n",
      "        ]\r\n",
      "        return mapping_values + list(self._extra_content.values())\r\n",
      "\r\n",
      "    def items(self):\r\n",
      "        mapping_items = [\r\n",
      "            (\r\n",
      "                self._load_attr_from_module(key, self._config_mapping[key]),\r\n",
      "                self._load_attr_from_module(key, self._model_mapping[key]),\r\n",
      "            )\r\n",
      "            for key in self._model_mapping.keys()\r\n",
      "            if key in self._config_mapping.keys()\r\n",
      "        ]\r\n",
      "        return mapping_items + list(self._extra_content.items())\r\n",
      "\r\n",
      "    def __iter__(self):\r\n",
      "        return iter(self.keys())\r\n",
      "\r\n",
      "    def __contains__(self, item):\r\n",
      "        if item in self._extra_content:\r\n",
      "            return True\r\n",
      "        if not hasattr(item, \"__name__\") or item.__name__ not in self._reverse_config_mapping:\r\n",
      "            return False\r\n",
      "        model_type = self._reverse_config_mapping[item.__name__]\r\n",
      "        return model_type in self._model_mapping\r\n",
      "\r\n",
      "    def register(self, key, value, exist_ok=False):\r\n",
      "        \"\"\"\r\n",
      "        Register a new model in this mapping.\r\n",
      "        \"\"\"\r\n",
      "        if hasattr(key, \"__name__\") and key.__name__ in self._reverse_config_mapping:\r\n",
      "            model_type = self._reverse_config_mapping[key.__name__]\r\n",
      "            if model_type in self._model_mapping.keys() and not exist_ok:\r\n",
      "                raise ValueError(f\"'{key}' is already used by a Transformers model.\")\r\n",
      "\r\n",
      "        self._extra_content[key] = value\r\n"
     ]
    }
   ],
   "source": [
    "%cat ~/anaconda3/envs/dl/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6ccbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    Main training entry point.\n",
    "            of [`Trainer`]. If present, training will resume from the model/optimizer/scheduler states loaded here.\n",
    "            gathering predictions for evaluation during the training.\n",
    "    self.is_in_train = True\n",
    "    # do_train is not a reliable argument, as it might not be set and .train() still called, so\n",
    "    if (args.fp16_full_eval or args.bf16_full_eval) and not args.do_train:\n",
    "        raise TypeError(f\"train() received got unexpected keyword arguments: {', '.join(list(kwargs.keys()))}.\")\n",
    "    self._train_batch_size = self.args.train_batch_size\n",
    "        # In case of repeating the find_executable_batch_size, set `self._train_batch_size` properly\n",
    "        if state.train_batch_size is not None:\n",
    "            self._train_batch_size = state.train_batch_size\n",
    "    inner_training_loop = find_executable_batch_size(\n",
    "        self._inner_training_loop, self._train_batch_size, args.auto_find_batch_size\n",
    "            return inner_training_loop(\n",
    "        return inner_training_loop(\n",
    "def _inner_training_loop(\n",
    "    self._train_batch_size = batch_size\n",
    "    if self.state.train_batch_size != self._train_batch_size:\n",
    "        # Temporarily unset `self.args.train_batch_size`\n",
    "        original_bs = self.args.per_device_train_batch_size\n",
    "        self.args.per_device_train_batch_size = self._train_batch_size // max(1, self.args.n_gpu)\n",
    "        self.args.per_device_train_batch_size = original_bs\n",
    "    self.state.train_batch_size = self._train_batch_size\n",
    "logger.debug(f\"Currently training with a batch size of: {self._train_batch_size}\")\n",
    "# Data loader and number of training steps\n",
    "train_dataloader = self.get_train_dataloader()\n",
    "    train_dataloader = tpu_spmd_dataloader(train_dataloader)\n",
    "# Setting up training control variables:\n",
    "# number of training epochs: num_train_epochs\n",
    "# number of training steps per epoch: num_update_steps_per_epoch\n",
    "# total number of training steps to execute: max_steps\n",
    "total_train_batch_size = self._train_batch_size * args.gradient_accumulation_steps * args.world_size\n",
    "num_train_tokens = None\n",
    "if has_length(train_dataloader):\n",
    "    len_dataloader = len(train_dataloader)\n",
    "    num_examples = self.num_examples(train_dataloader)\n",
    "        num_train_epochs = args.max_steps // num_update_steps_per_epoch + int(\n",
    "        # May be slightly incorrect if the last batch in the training dataloader has a smaller size but it's\n",
    "        num_train_samples = args.max_steps * total_train_batch_size\n",
    "            num_train_tokens = (\n",
    "                self.num_tokens(train_dataloader, args.max_steps) * args.gradient_accumulation_steps\n",
    "        max_steps = math.ceil(args.num_train_epochs * num_update_steps_per_epoch)\n",
    "        num_train_epochs = math.ceil(args.num_train_epochs)\n",
    "        num_train_samples = self.num_examples(train_dataloader) * args.num_train_epochs\n",
    "            num_train_tokens = self.num_tokens(train_dataloader) * args.num_train_epochs\n",
    "    num_train_epochs = sys.maxsize\n",
    "    num_examples = total_train_batch_size * args.max_steps\n",
    "    num_train_samples = args.max_steps * total_train_batch_size\n",
    "        num_train_tokens = self.num_tokens(train_dataloader, args.max_steps) * args.gradient_accumulation_steps\n",
    "    self.optimizer, self.lr_scheduler = deepspeed_init(self, num_training_steps=max_steps)\n",
    "    self.create_optimizer_and_scheduler(num_training_steps=max_steps)\n",
    "self.state.train_batch_size = self._train_batch_size\n",
    "    self.create_optimizer_and_scheduler(num_training_steps=max_steps)\n",
    "    self.model.train()\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(f\"  Num Epochs = {num_train_epochs:,}\")\n",
    "logger.info(f\"  Instantaneous batch size per device = {self.args.per_device_train_batch_size:,}\")\n",
    "if self.args.per_device_train_batch_size != self._train_batch_size:\n",
    "    logger.info(f\"  Training with DataParallel so batch size has been adjusted to: {self._train_batch_size:,}\")\n",
    "logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_train_batch_size:,}\")\n",
    "logger.info(f\"  Number of trainable parameters = {get_model_param_count(model, trainable_only=True):,}\")\n",
    "epochs_trained = 0\n",
    "steps_trained_in_current_epoch = 0\n",
    "steps_trained_progress_bar = None\n",
    "# Check if continuing training from a checkpoint\n",
    "    self.compare_trainer_and_checkpoint_args(self.args, self.state)\n",
    "    epochs_trained = self.state.global_step // num_update_steps_per_epoch\n",
    "        steps_trained_in_current_epoch = self.state.global_step % (num_update_steps_per_epoch)\n",
    "        steps_trained_in_current_epoch *= args.gradient_accumulation_steps\n",
    "        steps_trained_in_current_epoch = 0\n",
    "    logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n",
    "    logger.info(f\"  Continuing training from epoch {epochs_trained}\")\n",
    "    logger.info(f\"  Continuing training from global step {self.state.global_step}\")\n",
    "            f\"  Will skip the first {epochs_trained} epochs then the first\"\n",
    "            f\" {steps_trained_in_current_epoch} batches in the first epoch.\"\n",
    "self.callback_handler.train_dataloader = train_dataloader\n",
    "# This should be the same if the state has been saved but in case the training arguments changed, it's safer\n",
    "self.state.num_train_epochs = num_train_epochs\n",
    "self.control = self.callback_handler.on_train_begin(args, self.state, self.control)\n",
    "for epoch in range(epochs_trained, num_train_epochs):\n",
    "    epoch_iterator = train_dataloader\n",
    "    if epoch == epochs_trained and resume_from_checkpoint is not None and steps_trained_in_current_epoch == 0:\n",
    "    if steps_trained_in_current_epoch > 0:\n",
    "        epoch_iterator = skip_first_batches(epoch_iterator, steps_trained_in_current_epoch)\n",
    "        steps_skipped = steps_trained_in_current_epoch\n",
    "        steps_trained_in_current_epoch = 0\n",
    "        # Skip past any already trained steps if resuming training\n",
    "        if steps_trained_in_current_epoch > 0:\n",
    "            steps_trained_in_current_epoch -= 1\n",
    "            if steps_trained_progress_bar is not None:\n",
    "                steps_trained_progress_bar.update(1)\n",
    "            if steps_trained_in_current_epoch == 0:\n",
    "        elif steps_trained_progress_bar is not None:\n",
    "            steps_trained_progress_bar.close()\n",
    "            steps_trained_progress_bar = None\n",
    "            tr_loss_step = self.training_step(model, inputs)\n",
    "        if self.control.should_epoch_stop or self.control.should_training_stop:\n",
    "            \"There seems to be not a single sample in your epoch_iterator, stopping training at step\"\n",
    "        self.control.should_training_stop = True\n",
    "                \"configured. Check your training configuration if this is unexpected.\"\n",
    "    if self.control.should_training_stop:\n",
    "    # Clean the state at the end of training\n",
    "train_loss = self._total_loss_scalar / effective_global_step\n",
    "    \"train\",\n",
    "    num_samples=num_train_samples,\n",
    "    num_tokens=num_train_tokens,\n",
    "metrics[\"train_loss\"] = train_loss\n",
    "self.is_in_train = False\n",
    "self.control = self.callback_handler.on_train_end(args, self.state, self.control)\n",
    "# After training we make sure to retrieve back the original forward pass method\n",
    "return TrainOutput(self.state.global_step, train_loss, metrics)\n",
    "\n",
    "            import ray.train\n",
    "            run_id = ray.train.get_context().get_trial_id()\n",
    "        config = PretrainedConfig.from_json_file(config_file)\n",
    "                f\"You are resuming training from a checkpoint trained with {checkpoint_version} of \"\n",
    "        # If train a model using PEFT & LoRA, assume that adapter have been saved properly.\n",
    "                        model.load_adapter(peft_id, subdir_name, is_trainable=(subdir_name == active_adapter))\n",
    "                    model.load_adapter(resume_from_checkpoint, active_adapter, is_trainable=True)\n",
    "                # If train a model using PEFT & LoRA, assume that adapter have been saved properly.\n",
    "            f\"Could not locate the best model at {best_model_path}, if you are running a distributed training \"\n",
    "                f\"The `metric_for_best_model` training argument is set to '{metric_to_check}', which is not found in the evaluation metrics. \"\n",
    "                f\"Didn't find an RNG file for process {process_index}, if you are resuming a training that \"\n",
    "                \"Didn't find an RNG file, if you are resuming a training that was launched in a distributed \"\n",
    "                    \"\\nThis won't yield the same results as if the training had not been interrupted.\"\n",
    "                    \"\\nThis won't yield the same results as if the training had not been interrupted.\"\n",
    "                    \"\\nThis won't yield the same results as if the training had not been interrupted.\"\n",
    "                f\"The `metric_for_best_model` training argument is set to '{metric_to_check}', which is not found in the evaluation metrics. \"\n",
    "    # Save RNG state in non-distributed training\n",
    "                # We use the CPU when training on one GPU to avoid OOM for GPU RAM when training big models.\n",
    "                # In distributed training however, we load directly on each GPU and risk the GPU OOM as it's more\n",
    "            logger.info(\"Continuing training from checkpoint, restoring any callbacks that were passed in\")\n",
    "            [`~trainer_utils.default_hp_space_optuna`] or [`~trainer_utils.default_hp_space_ray`] or\n",
    "            [`~trainer_utils.default_hp_space_sigopt`] depending on your backend.\n",
    "            method. Will default to [`~trainer_utils.default_compute_objective`].\n",
    "        backend (`str` or [`~training_utils.HPSearchBackend`], *optional*):\n",
    "        [`trainer_utils.BestRun` or `List[trainer_utils.BestRun]`]: All the information about the best run or best\n",
    "    Log `logs` on the various objects watching training.\n",
    "            \"The batch received was empty, your model won't be able to train on it. Double-check that your \"\n",
    "            f\"training dataset contains keys expected by the model: {','.join(self._signature_columns)}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2735810a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Trainer class, to easily train a 🤗 Transformers from scratch or finetune it on a new task.\r\n",
      "from .configuration_utils import PretrainedConfig\r\n",
      "from .trainer_callback import (\r\n",
      "from .trainer_pt_utils import (\r\n",
      "from .trainer_utils import (\r\n",
      "from .training_args import OptimizerNames, ParallelMode, TrainingArguments\r\n",
      "    from .trainer_pt_utils import smp_forward_backward, smp_forward_only, smp_gather, smp_nested_concat\r\n",
      "TRAINING_ARGS_NAME = \"training_args.bin\"\r\n",
      "TRAINER_STATE_NAME = \"trainer_state.json\"\r\n",
      "    Trainer is a simple but feature-complete training and eval loop for PyTorch, optimized for 🤗 Transformers.\r\n",
      "            The model to train, evaluate or use for predictions. If not provided, a `model_init` must be passed.\r\n",
      "            The arguments to tweak for training. Will default to a basic instance of [`TrainingArguments`] with the\r\n",
      "            `output_dir` set to a directory named *tmp_trainer* in the current directory if not provided.\r\n",
      "            The function to use to form a batch from a list of elements of `train_dataset` or `eval_dataset`. Will\r\n",
      "        train_dataset (Union[`torch.utils.data.Dataset`, `torch.utils.data.IterableDataset`, `datasets.Dataset`], *optional*):\r\n",
      "            The dataset to use for training. If it is a [`~datasets.Dataset`], columns not accepted by the\r\n",
      "            Note that if it's a `torch.utils.data.IterableDataset` with some randomization and you are training in a\r\n",
      "            interrupted training or reuse the fine-tuned model.\r\n",
      "            A function that instantiates the model to be used. If provided, each call to [`~Trainer.train`] will start\r\n",
      "            A list of callbacks to customize the training loop. Will add those to the list of default callbacks\r\n",
      "        - **is_in_train** -- Whether or not a model is currently running `train` (e.g. when `evaluate` is called while\r\n",
      "          in `train`)\r\n",
      "    from .trainer_pt_utils import _get_learning_rate, log_metrics, metrics_format, save_metrics, save_state\r\n",
      "        train_dataset: Optional[Union[Dataset, IterableDataset, \"datasets.Dataset\"]] = None,\r\n",
      "            output_dir = \"tmp_trainer\"\r\n",
      "        self.is_in_train = False\r\n",
      "                    \" overwrite your model when calling the `train` method. This will become a fatal error in the next\"\r\n",
      "                f\"The model you have picked ({model.__class__.__name__}) cannot be used as is for training: it only \"\r\n",
      "        _quantization_method_supports_training = (\r\n",
      "            getattr(model, \"hf_quantizer\", None) is not None and model.hf_quantizer.is_trainable\r\n",
      "                \"You cannot perform fine-tuning on purely quantized models. Please attach trainable adapters on top of\"\r\n",
      "        elif _is_quantized_and_base_model and not _quantization_method_supports_training:\r\n",
      "                \" but that quantization method do not support training. Please open an issue on GitHub: https://github.com/huggingface/transformers\"\r\n",
      "                f\" to request the support for training support for {model.hf_quantizer.quantization_config.quant_method}\"\r\n",
      "                raise ValueError(\"Using fsdp only works in distributed training.\")\r\n",
      "        #    and we only use deepspeed for training at the moment\r\n",
      "            or ((args.fp16_full_eval or args.bf16_full_eval) and not args.do_train)\r\n",
      "        self.train_dataset = train_dataset\r\n",
      "        if args.max_steps > 0 and args.num_train_epochs > 0:\r\n",
      "            logger.warning(\"max_steps is given, it will override any value given in num_train_epochs\")\r\n",
      "        if train_dataset is not None and not has_length(train_dataset) and args.max_steps <= 0:\r\n",
      "                \"The train_dataset does not implement __len__, max_steps has to be specified. \"\r\n",
      "            train_dataset is not None\r\n",
      "            and isinstance(train_dataset, torch.utils.data.IterableDataset)\r\n",
      "                # When there's mismatch between SMP config and trainer argument, use SMP config as truth\r\n",
      "                        f\"but FP16 provided in trainer argument is {args.fp16}, \"\r\n",
      "                # smp < 1.10 does not support fp16 in trainer.\r\n",
      "                        \"but SageMaker Model Parallelism < 1.10 does not support FP16 in trainer.\"\r\n",
      "        self._train_batch_size = args.train_batch_size\r\n",
      "            raise ValueError(\"Neftune is not activated make sure to call `trainer._activate_neftune()` first\")\r\n",
      "    def _get_train_sampler(self) -> Optional[torch.utils.data.Sampler]:\r\n",
      "        if self.train_dataset is None or not has_length(self.train_dataset):\r\n",
      "            if is_datasets_available() and isinstance(self.train_dataset, datasets.Dataset):\r\n",
      "                    self.train_dataset[self.args.length_column_name]\r\n",
      "                    if self.args.length_column_name in self.train_dataset.column_names\r\n",
      "                self.args.train_batch_size * self.args.gradient_accumulation_steps,\r\n",
      "                dataset=self.train_dataset,\r\n",
      "            return RandomSampler(self.train_dataset)\r\n",
      "    def get_train_dataloader(self) -> DataLoader:\r\n",
      "        Returns the training [`~torch.utils.data.DataLoader`].\r\n",
      "        Will use no sampler if `train_dataset` does not implement `__len__`, a random sampler (adapted to distributed\r\n",
      "        training if necessary) otherwise.\r\n",
      "        if self.train_dataset is None:\r\n",
      "            raise ValueError(\"Trainer: training requires a train_dataset.\")\r\n",
      "        train_dataset = self.train_dataset\r\n",
      "        if is_datasets_available() and isinstance(train_dataset, datasets.Dataset):\r\n",
      "            train_dataset = self._remove_unused_columns(train_dataset, description=\"training\")\r\n",
      "            data_collator = self._get_collator_with_removed_columns(data_collator, description=\"training\")\r\n",
      "            \"batch_size\": self._train_batch_size,\r\n",
      "        if not isinstance(train_dataset, torch.utils.data.IterableDataset):\r\n",
      "            dataloader_params[\"sampler\"] = self._get_train_sampler()\r\n",
      "        return self.accelerator.prepare(DataLoader(train_dataset, **dataloader_params))\r\n",
      "        # don't change during training\r\n",
      "    def create_optimizer_and_scheduler(self, num_training_steps: int):\r\n",
      "        self.create_scheduler(num_training_steps=num_training_steps, optimizer=optimizer)\r\n",
      "    def get_num_trainable_parameters(self):\r\n",
      "        Get the number of trainable parameters.\r\n",
      "        Returns the optimizer class and optimizer parameters based on the training arguments.\r\n",
      "            args (`transformers.training_args.TrainingArguments`):\r\n",
      "                The training arguments for the training session.\r\n",
      "                \"Activated GaLoRE fine-tuning, depending on your model size and hardware, the training might take a while before starting. Please be patient !\"\r\n",
      "    def create_scheduler(self, num_training_steps: int, optimizer: torch.optim.Optimizer = None):\r\n",
      "        Setup the scheduler. The optimizer of the trainer must have been set up either before this method is called or\r\n",
      "            num_training_steps (int): The number of training steps to do.\r\n",
      "                num_warmup_steps=self.args.get_warmup_steps(num_training_steps),\r\n",
      "                num_training_steps=num_training_steps,\r\n",
      "            return len(dataloader) * self.args.per_device_train_batch_size\r\n",
      "    def num_tokens(self, train_dl: DataLoader, max_steps: Optional[int] = None) -> int:\r\n",
      "        train_tokens = 0\r\n",
      "            for step, batch in enumerate(train_dl):\r\n",
      "                train_tokens += tokens\r\n",
      "            return train_tokens\r\n",
      "            return train_tokens\r\n",
      "            # Rebuild the deepspeed config to reflect the updated training parameters\r\n",
      "            self.args.hf_deepspeed_config.trainer_config_process(self.args)\r\n",
      "                    self.callback_handler.on_train_end(self.args, self.state, self.control)\r\n",
      "            import ray.train\r\n",
      "                    checkpoint = ray.train.Checkpoint.from_directory(temp_checkpoint_dir)\r\n",
      "                ray.train.report(metrics, checkpoint=checkpoint)\r\n",
      "    def torch_jit_model_eval(self, model, dataloader, training=False):\r\n",
      "        if not training:\r\n",
      "    def ipex_optimize_model(self, model, training=False, dtype=torch.float32):\r\n",
      "        if not training:\r\n",
      "            dtype = torch.bfloat16 if not self.is_in_train and self.args.bf16_full_eval else dtype\r\n",
      "            model = ipex.optimize(model, dtype=dtype, level=\"O1\", conv_bn_folding=False, inplace=not self.is_in_train)\r\n",
      "            if not model.training:\r\n",
      "                model.train()\r\n",
      "    def compare_trainer_and_checkpoint_args(self, training_args, trainer_state):\r\n",
      "        warning_str = \"Warning: The following arguments do not match the ones in the `trainer_state.json` within the checkpoint directory: \"\r\n",
      "            arg_value = getattr(training_args, arg_attr, None)\r\n",
      "            state_value = getattr(trainer_state, state_attr, None)\r\n",
      "                warning_str += f\"\\n\\t{arg_attr}: {arg_value} (from args) != {state_value} (from trainer_state.json)\"\r\n",
      "        # train bs is special as we need to account for multi-GPU\r\n",
      "        train_bs_args = training_args.per_device_train_batch_size\r\n",
      "        train_bs_state = trainer_state.train_batch_size // max(1, training_args.n_gpu)\r\n",
      "        if train_bs_args != train_bs_state:\r\n",
      "            warning_str += f\"\\n\\tper_device_train_batch_size: {train_bs_args} (from args) != {train_bs_state} (from trainer_state.json)\"\r\n",
      "    def _wrap_model(self, model, training=True, dataloader=None):\r\n",
      "            model = self.ipex_optimize_model(model, training, dtype=dtype)\r\n",
      "        # train/eval could be run multiple-times - if already wrapped, don't re-wrap it again\r\n",
      "        # Mixed precision training with apex (torch < 1.6)\r\n",
      "        if self.use_apex and training:\r\n",
      "        # Multi-gpu training (should be after apex fp16 initialization) / 8bit models does not support DDP\r\n",
      "            model = self.torch_jit_model_eval(model, dataloader, training)\r\n",
      "        if not training:\r\n",
      "        # Distributed training (should be after apex fp16 initialization)\r\n",
      "        # Distributed training using PyTorch FSDP\r\n",
      "    def train(\r\n",
      "        Main training entry point.\r\n",
      "                of [`Trainer`]. If present, training will resume from the model/optimizer/scheduler states loaded here.\r\n",
      "                gathering predictions for evaluation during the training.\r\n",
      "        self.is_in_train = True\r\n",
      "        # do_train is not a reliable argument, as it might not be set and .train() still called, so\r\n",
      "        if (args.fp16_full_eval or args.bf16_full_eval) and not args.do_train:\r\n",
      "            raise TypeError(f\"train() received got unexpected keyword arguments: {', '.join(list(kwargs.keys()))}.\")\r\n",
      "        self._train_batch_size = self.args.train_batch_size\r\n",
      "            # In case of repeating the find_executable_batch_size, set `self._train_batch_size` properly\r\n",
      "            if state.train_batch_size is not None:\r\n",
      "                self._train_batch_size = state.train_batch_size\r\n",
      "        inner_training_loop = find_executable_batch_size(\r\n",
      "            self._inner_training_loop, self._train_batch_size, args.auto_find_batch_size\r\n",
      "                return inner_training_loop(\r\n",
      "            return inner_training_loop(\r\n",
      "    def _inner_training_loop(\r\n",
      "        self._train_batch_size = batch_size\r\n",
      "            if self.state.train_batch_size != self._train_batch_size:\r\n",
      "                    # Temporarily unset `self.args.train_batch_size`\r\n",
      "                    original_bs = self.args.per_device_train_batch_size\r\n",
      "                    self.args.per_device_train_batch_size = self._train_batch_size // max(1, self.args.n_gpu)\r\n",
      "                    self.args.per_device_train_batch_size = original_bs\r\n",
      "            self.state.train_batch_size = self._train_batch_size\r\n",
      "        logger.debug(f\"Currently training with a batch size of: {self._train_batch_size}\")\r\n",
      "        # Data loader and number of training steps\r\n",
      "        train_dataloader = self.get_train_dataloader()\r\n",
      "            train_dataloader = tpu_spmd_dataloader(train_dataloader)\r\n",
      "        # Setting up training control variables:\r\n",
      "        # number of training epochs: num_train_epochs\r\n",
      "        # number of training steps per epoch: num_update_steps_per_epoch\r\n",
      "        # total number of training steps to execute: max_steps\r\n",
      "        total_train_batch_size = self._train_batch_size * args.gradient_accumulation_steps * args.world_size\r\n",
      "        num_train_tokens = None\r\n",
      "        if has_length(train_dataloader):\r\n",
      "            len_dataloader = len(train_dataloader)\r\n",
      "            num_examples = self.num_examples(train_dataloader)\r\n",
      "                num_train_epochs = args.max_steps // num_update_steps_per_epoch + int(\r\n",
      "                # May be slightly incorrect if the last batch in the training dataloader has a smaller size but it's\r\n",
      "                num_train_samples = args.max_steps * total_train_batch_size\r\n",
      "                    num_train_tokens = (\r\n",
      "                        self.num_tokens(train_dataloader, args.max_steps) * args.gradient_accumulation_steps\r\n",
      "                max_steps = math.ceil(args.num_train_epochs * num_update_steps_per_epoch)\r\n",
      "                num_train_epochs = math.ceil(args.num_train_epochs)\r\n",
      "                num_train_samples = self.num_examples(train_dataloader) * args.num_train_epochs\r\n",
      "                    num_train_tokens = self.num_tokens(train_dataloader) * args.num_train_epochs\r\n",
      "            num_train_epochs = sys.maxsize\r\n",
      "            num_examples = total_train_batch_size * args.max_steps\r\n",
      "            num_train_samples = args.max_steps * total_train_batch_size\r\n",
      "                num_train_tokens = self.num_tokens(train_dataloader, args.max_steps) * args.gradient_accumulation_steps\r\n",
      "            self.optimizer, self.lr_scheduler = deepspeed_init(self, num_training_steps=max_steps)\r\n",
      "            self.create_optimizer_and_scheduler(num_training_steps=max_steps)\r\n",
      "        self.state.train_batch_size = self._train_batch_size\r\n",
      "            self.create_optimizer_and_scheduler(num_training_steps=max_steps)\r\n",
      "            self.model.train()\r\n",
      "        logger.info(\"***** Running training *****\")\r\n",
      "        logger.info(f\"  Num Epochs = {num_train_epochs:,}\")\r\n",
      "        logger.info(f\"  Instantaneous batch size per device = {self.args.per_device_train_batch_size:,}\")\r\n",
      "        if self.args.per_device_train_batch_size != self._train_batch_size:\r\n",
      "            logger.info(f\"  Training with DataParallel so batch size has been adjusted to: {self._train_batch_size:,}\")\r\n",
      "        logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_train_batch_size:,}\")\r\n",
      "        logger.info(f\"  Number of trainable parameters = {get_model_param_count(model, trainable_only=True):,}\")\r\n",
      "        epochs_trained = 0\r\n",
      "        steps_trained_in_current_epoch = 0\r\n",
      "        steps_trained_progress_bar = None\r\n",
      "        # Check if continuing training from a checkpoint\r\n",
      "            self.compare_trainer_and_checkpoint_args(self.args, self.state)\r\n",
      "            epochs_trained = self.state.global_step // num_update_steps_per_epoch\r\n",
      "                steps_trained_in_current_epoch = self.state.global_step % (num_update_steps_per_epoch)\r\n",
      "                steps_trained_in_current_epoch *= args.gradient_accumulation_steps\r\n",
      "                steps_trained_in_current_epoch = 0\r\n",
      "            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\r\n",
      "            logger.info(f\"  Continuing training from epoch {epochs_trained}\")\r\n",
      "            logger.info(f\"  Continuing training from global step {self.state.global_step}\")\r\n",
      "                    f\"  Will skip the first {epochs_trained} epochs then the first\"\r\n",
      "                    f\" {steps_trained_in_current_epoch} batches in the first epoch.\"\r\n",
      "        self.callback_handler.train_dataloader = train_dataloader\r\n",
      "        # This should be the same if the state has been saved but in case the training arguments changed, it's safer\r\n",
      "        self.state.num_train_epochs = num_train_epochs\r\n",
      "        self.control = self.callback_handler.on_train_begin(args, self.state, self.control)\r\n",
      "        for epoch in range(epochs_trained, num_train_epochs):\r\n",
      "            epoch_iterator = train_dataloader\r\n",
      "            if epoch == epochs_trained and resume_from_checkpoint is not None and steps_trained_in_current_epoch == 0:\r\n",
      "            if steps_trained_in_current_epoch > 0:\r\n",
      "                epoch_iterator = skip_first_batches(epoch_iterator, steps_trained_in_current_epoch)\r\n",
      "                steps_skipped = steps_trained_in_current_epoch\r\n",
      "                steps_trained_in_current_epoch = 0\r\n",
      "                # Skip past any already trained steps if resuming training\r\n",
      "                if steps_trained_in_current_epoch > 0:\r\n",
      "                    steps_trained_in_current_epoch -= 1\r\n",
      "                    if steps_trained_progress_bar is not None:\r\n",
      "                        steps_trained_progress_bar.update(1)\r\n",
      "                    if steps_trained_in_current_epoch == 0:\r\n",
      "                elif steps_trained_progress_bar is not None:\r\n",
      "                    steps_trained_progress_bar.close()\r\n",
      "                    steps_trained_progress_bar = None\r\n",
      "                    tr_loss_step = self.training_step(model, inputs)\r\n",
      "                if self.control.should_epoch_stop or self.control.should_training_stop:\r\n",
      "                    \"There seems to be not a single sample in your epoch_iterator, stopping training at step\"\r\n",
      "                self.control.should_training_stop = True\r\n",
      "                        \"configured. Check your training configuration if this is unexpected.\"\r\n",
      "            if self.control.should_training_stop:\r\n",
      "            # Clean the state at the end of training\r\n",
      "        train_loss = self._total_loss_scalar / effective_global_step\r\n",
      "            \"train\",\r\n",
      "            num_samples=num_train_samples,\r\n",
      "            num_tokens=num_train_tokens,\r\n",
      "        metrics[\"train_loss\"] = train_loss\r\n",
      "        self.is_in_train = False\r\n",
      "        self.control = self.callback_handler.on_train_end(args, self.state, self.control)\r\n",
      "        # After training we make sure to retrieve back the original forward pass method\r\n",
      "        return TrainOutput(self.state.global_step, train_loss, metrics)\r\n",
      "                import ray.train\r\n",
      "                run_id = ray.train.get_context().get_trial_id()\r\n",
      "            config = PretrainedConfig.from_json_file(config_file)\r\n",
      "                    f\"You are resuming training from a checkpoint trained with {checkpoint_version} of \"\r\n",
      "            # If train a model using PEFT & LoRA, assume that adapter have been saved properly.\r\n",
      "                            model.load_adapter(peft_id, subdir_name, is_trainable=(subdir_name == active_adapter))\r\n",
      "                        model.load_adapter(resume_from_checkpoint, active_adapter, is_trainable=True)\r\n",
      "                    # If train a model using PEFT & LoRA, assume that adapter have been saved properly.\r\n",
      "                f\"Could not locate the best model at {best_model_path}, if you are running a distributed training \"\r\n",
      "                    f\"The `metric_for_best_model` training argument is set to '{metric_to_check}', which is not found in the evaluation metrics. \"\r\n",
      "                    f\"Didn't find an RNG file for process {process_index}, if you are resuming a training that \"\r\n",
      "                    \"Didn't find an RNG file, if you are resuming a training that was launched in a distributed \"\r\n",
      "                        \"\\nThis won't yield the same results as if the training had not been interrupted.\"\r\n",
      "                        \"\\nThis won't yield the same results as if the training had not been interrupted.\"\r\n",
      "                        \"\\nThis won't yield the same results as if the training had not been interrupted.\"\r\n",
      "                    f\"The `metric_for_best_model` training argument is set to '{metric_to_check}', which is not found in the evaluation metrics. \"\r\n",
      "        # Save RNG state in non-distributed training\r\n",
      "                    # We use the CPU when training on one GPU to avoid OOM for GPU RAM when training big models.\r\n",
      "                    # In distributed training however, we load directly on each GPU and risk the GPU OOM as it's more\r\n",
      "                logger.info(\"Continuing training from checkpoint, restoring any callbacks that were passed in\")\r\n",
      "                [`~trainer_utils.default_hp_space_optuna`] or [`~trainer_utils.default_hp_space_ray`] or\r\n",
      "                [`~trainer_utils.default_hp_space_sigopt`] depending on your backend.\r\n",
      "                method. Will default to [`~trainer_utils.default_compute_objective`].\r\n",
      "            backend (`str` or [`~training_utils.HPSearchBackend`], *optional*):\r\n",
      "            [`trainer_utils.BestRun` or `List[trainer_utils.BestRun]`]: All the information about the best run or best\r\n",
      "        Log `logs` on the various objects watching training.\r\n",
      "                \"The batch received was empty, your model won't be able to train on it. Double-check that your \"\r\n",
      "                f\"training dataset contains keys expected by the model: {','.join(self._signature_columns)}.\"\r\n",
      "    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:\r\n",
      "        Perform a training step on a batch of inputs.\r\n",
      "                The model to train.\r\n",
      "            `torch.Tensor`: The tensor with training loss on this batch.\r\n",
      "        model.train()\r\n",
      "            loss = loss.mean()  # mean() to average on multi-gpu parallel training\r\n",
      "        Whether or not this process is the local (e.g., on one machine if training in a distributed fashion on several\r\n",
      "        Whether or not this process is the global main process (when training in a distributed fashion on several\r\n",
      "        Will save the model, so you can reload it using `from_pretrained()`.\r\n",
      "        # Save a trained model and configuration using `save_pretrained()`.\r\n",
      "        # They can then be reloaded using `from_pretrained()`\r\n",
      "                self.accelerator.unwrap_model(model).save_pretrained(\r\n",
      "            model.save_pretrained(\r\n",
      "            self.tokenizer.save_pretrained(output_dir)\r\n",
      "        # Save a trained model and configuration using `save_pretrained()`.\r\n",
      "        # They can then be reloaded using `from_pretrained()`\r\n",
      "                self.accelerator.unwrap_model(self.model).save_pretrained(\r\n",
      "            self.model.save_pretrained(\r\n",
      "            self.tokenizer.save_pretrained(output_dir)\r\n",
      "        # Good practice: save your training arguments together with the trained model\r\n",
      "                separate evaluations on each dataset. This can be useful to monitor how training affects other\r\n",
      "            dictionary also contains the epoch number which comes from the training state.\r\n",
      "        # if eval is called w/o train, handle model prep here\r\n",
      "            _, _ = deepspeed_init(self, num_training_steps=0, inference=True)\r\n",
      "        model = self._wrap_model(self.model, training=False, dataloader=dataloader)\r\n",
      "        # while ``train`` is running, cast it to the right dtype first and then put on device\r\n",
      "        if not self.is_in_train:\r\n",
      "                The license of the model. Will default to the license of the pretrained model used, if the original\r\n",
      "        training_summary = TrainingSummary.from_trainer(\r\n",
      "        model_card = training_summary.to_model_card()\r\n",
      "            self.tokenizer.save_pretrained(output_dir)\r\n",
      "        # Same for the training arguments\r\n",
      "        commit_message: Optional[str] = \"End of training\",\r\n",
      "            commit_message (`str`, *optional*, defaults to `\"End of training\"`):\r\n",
      "        # Needs to be executed on all processes for TPU training, but will only save on the processed determined by\r\n",
      "        # \"tags\" argument to `push_to_hub` so that trainer automatically handles internal tags\r\n",
      "        # if eval is called w/o train, handle model prep here\r\n",
      "            _, _ = deepspeed_init(self, num_training_steps=0, inference=True)\r\n",
      "        model = self._wrap_model(self.model, training=False, dataloader=dataloader)\r\n",
      "        # while ``train`` is running, cast it to the right dtype first and then put on device\r\n",
      "        if not self.is_in_train:\r\n",
      "        # deepspeed and accelerate flags covering both trainer args and accelerate launcher\r\n",
      "                        \"The activation_checkpointing in FSDP config and the gradient_checkpointing in training arg \"\r\n",
      "        ds_plugin.hf_ds_config.trainer_config_process(self.args, auto_find_batch_size)\r\n"
     ]
    }
   ],
   "source": [
    "%cat ~/anaconda3/envs/dl/lib/python3.8/site-packages/transformers/trainer.py | grep 'train'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
